# initialise evotree
function init_evotree(params::Union{EvoTreeRegressor,EvoTreeCount,EvoTreeClassifier,EvoTreeGaussian},
    X::AbstractMatrix{R}, Y::AbstractVector{S}; verbosity=1) where {R<:Real, S}

    seed!(params.seed)

    display("start init")

    K = 1
    levels = ""
    if typeof(params.loss) == Logistic
        Î¼ = fill(logit(mean(Y)), 1)
    elseif typeof(params.loss) == Poisson
        Y = Float64.(Y)
        Î¼ = fill(log(mean(Y)), 1)
    elseif typeof(params.loss) == Softmax
        if typeof(Y) <: AbstractCategoricalVector
            levels = CategoricalArray(CategoricalArrays.levels(Y))
            K = length(levels)
            Î¼ = zeros(K)
            Y = MLJBase.int(Y)
        else
            levels = CategoricalArray(sort(unique(Y)))
            K = length(levels)
            Î¼ = zeros(K)
        end
    elseif typeof(params.loss) == Gaussian
        K = 2
        Î¼ = SVector{2}([mean(Y), log(var(Y))])
    else
        Î¼ = fill(mean(Y), 1)
    end

    # initialize preds
    pred = zeros(SVector{K,Float64}, size(X,1))
    for i in eachindex(pred)
        pred[i] += Î¼
    end

    # bias = Tree([TreeNode(SVector{1, Float64}(Î¼))])
    bias = Tree([TreeNode(SVector{K,Float64}(Î¼))])
    evotree = GBTree([bias], params, Metric(), K, levels)

    X_size = size(X)
    ð‘–_ = collect(1:X_size[1])
    ð‘—_ = collect(1:X_size[2])

    display("initialize gradients")
    # initialize gradients and weights
    Î´, Î´Â² = zeros(SVector{evotree.K, Float64}, X_size[1]), zeros(SVector{evotree.K, Float64}, X_size[1])
    # Î´, Î´Â² = zeros(SVector{1, Float64}, X_size[1]), zeros(SVector{1, Float64}, X_size[1])
    ð‘¤ = zeros(SVector{1, Float64}, X_size[1]) .+ one(Float64)
    display("gradients initialized")

    # binarize data into quantiles
    edges = get_edges(X, params.nbins)
    X_bin = binarize(X, edges)

    # initialize train nodes
    train_nodes = Vector{TrainNode{evotree.K, Float64, Int64}}(undef, 2^params.max_depth-1)
    for node in 1:2^params.max_depth-1
        train_nodes[node] = TrainNode(0, SVector{evotree.K, Float64}(fill(-Inf, evotree.K)), SVector{evotree.K, Float64}(fill(-Inf, evotree.K)), SVector{1, Float64}(fill(-Inf, 1)), -Inf, [0], [0])
    end

    # initializde node splits info and tracks - colsample size (ð‘—)
    splits = Vector{SplitInfo{evotree.K, Float64, Int64}}(undef, X_size[2])
    hist_Î´ = Vector{Vector{SVector{evotree.K, Float64}}}(undef, X_size[2])
    hist_Î´Â² = Vector{Vector{SVector{evotree.K, Float64}}}(undef, X_size[2])
    hist_ð‘¤ = Vector{Vector{SVector{1, Float64}}}(undef, X_size[2])
    for feat in ð‘—_
        splits[feat] = SplitInfo{evotree.K, Float64, Int}(-Inf, SVector{evotree.K, Float64}(zeros(evotree.K)), SVector{evotree.K, Float64}(zeros(evotree.K)), SVector{1, Float64}(zeros(1)), SVector{evotree.K, Float64}(zeros(evotree.K)), SVector{evotree.K, Float64}(zeros(evotree.K)), SVector{1, Float64}(zeros(1)), -Inf, -Inf, 0, feat, 0.0)
        hist_Î´[feat] = zeros(SVector{evotree.K, Float64}, length(edges[feat]))
        hist_Î´Â²[feat] = zeros(SVector{evotree.K, Float64}, length(edges[feat]))
        hist_ð‘¤[feat] = zeros(SVector{1, Float64}, length(edges[feat]))
    end

    display("before cache")

    cache = (params=deepcopy(params),
        X=X, Y=Y, pred=pred,
        ð‘–_=ð‘–_, ð‘—_=ð‘—_, Î´=Î´, Î´Â²=Î´Â², ð‘¤=ð‘¤,
        edges=edges, X_bin=X_bin,
        train_nodes=train_nodes, splits=splits,
        hist_Î´=hist_Î´, hist_Î´Â²=hist_Î´Â², hist_ð‘¤=hist_ð‘¤)

    cache.params.nrounds = 0

    return evotree, cache
end


function grow_evotree!(evotree::GBTree, cache; verbosity=1)

    # initialize from cache
    params = evotree.params
    train_nodes = cache.train_nodes
    splits = cache.splits
    X_size = size(cache.X_bin)
    Î´nrounds = params.nrounds - cache.params.nrounds

    # loop over nrounds
    for i in 1:Î´nrounds

        # select random rows and cols
        ð‘– = cache.ð‘–_[sample(cache.ð‘–_, ceil(Int, params.rowsample * X_size[1]), replace=false, ordered=true)]
        ð‘— = cache.ð‘—_[sample(cache.ð‘—_, ceil(Int, params.colsample * X_size[2]), replace=false, ordered=true)]
        # reset gain to -Inf
        for feat in cache.ð‘—_
            splits[feat].gain = -Inf
        end

        # build a new tree
        update_grads!(params.loss, params.Î±, cache.pred, cache.Y, cache.Î´, cache.Î´Â², cache.ð‘¤)
        âˆ‘Î´, âˆ‘Î´Â², âˆ‘ð‘¤ = sum(cache.Î´[ð‘–]), sum(cache.Î´Â²[ð‘–]), sum(cache.ð‘¤[ð‘–])
        gain = get_gain(params.loss, âˆ‘Î´, âˆ‘Î´Â², âˆ‘ð‘¤, params.Î»)
        # assign a root and grow tree
        train_nodes[1] = TrainNode(1, âˆ‘Î´, âˆ‘Î´Â², âˆ‘ð‘¤, gain, ð‘–, ð‘—)
        tree = grow_tree(cache.Î´, cache.Î´Â², cache.ð‘¤, cache.hist_Î´, cache.hist_Î´Â², cache.hist_ð‘¤, params, train_nodes, splits, cache.edges, cache.X_bin)
        push!(evotree.trees, tree)
        predict!(cache.pred, tree, cache.X)

    end #end of nrounds

    cache.params.nrounds = params.nrounds
    # cache = (deepcopy(params), X, Y, pred, ð‘–_, ð‘—_, Î´, Î´Â², ð‘¤, edges, X_bin, train_nodes, splits, hist_Î´, hist_Î´Â², hist_ð‘¤)
    # return model, cache
    return evotree
end

# grow a single tree
function grow_tree(Î´, Î´Â², ð‘¤,
    hist_Î´, hist_Î´Â², hist_ð‘¤,
    params::Union{EvoTreeRegressor,EvoTreeCount,EvoTreeClassifier,EvoTreeGaussian},
    train_nodes::Vector{TrainNode{L,T,S}},
    splits::Vector{SplitInfo{L,T,Int}},
    edges, X_bin) where {R<:Real, T<:AbstractFloat, S<:Int, L}

    active_id = ones(Int, 1)
    leaf_count = one(Int)
    tree_depth = one(Int)
    tree = Tree(Vector{TreeNode{L, T, Int, Bool}}())

    # grow while there are remaining active nodes
    while size(active_id, 1) > 0 && tree_depth <= params.max_depth
        next_active_id = ones(Int, 0)
        # grow nodes
        for id in active_id
            node = train_nodes[id]
            if tree_depth == params.max_depth || node.âˆ‘ð‘¤[1] <= params.min_weight
                push!(tree.nodes, TreeNode(pred_leaf(params.loss, node, params, Î´Â²)))
            else
                @threads for feat in node.ð‘—
                    splits[feat].gain = node.gain
                    find_split_static!(hist_Î´[feat], hist_Î´Â²[feat], hist_ð‘¤[feat], view(X_bin,:,feat), Î´, Î´Â², ð‘¤, node.âˆ‘Î´, node.âˆ‘Î´Â², node.âˆ‘ð‘¤, params, splits[feat], edges[feat], node.ð‘–)
                    # update_hist!(hist_Î´, hist_Î´Â², hist_ð‘¤, X_bin, Î´, Î´Â², ð‘¤, set, feat)
                    # find_split!(hist_Î´[feat], hist_Î´Â²[feat], hist_ð‘¤[feat], node.âˆ‘Î´, node.âˆ‘Î´Â², node.âˆ‘ð‘¤, params, splits[feat], edges[feat], feat)
                end
                # assign best split
                best = get_max_gain(splits)
                # grow node if best split improve gain
                if best.gain > node.gain + params.Î³
                    left, right = update_set(node.ð‘–, best.ð‘–, view(X_bin,:,best.feat))
                    train_nodes[leaf_count + 1] = TrainNode(node.depth + 1, best.âˆ‘Î´L, best.âˆ‘Î´Â²L, best.âˆ‘ð‘¤L, best.gainL, left, node.ð‘—)
                    train_nodes[leaf_count + 2] = TrainNode(node.depth + 1, best.âˆ‘Î´R, best.âˆ‘Î´Â²R, best.âˆ‘ð‘¤R, best.gainR, right, node.ð‘—)
                    # push split Node
                    push!(tree.nodes, TreeNode(leaf_count + 1, leaf_count + 2, best.feat, best.cond, L))
                    push!(next_active_id, leaf_count + 1)
                    push!(next_active_id, leaf_count + 2)
                    leaf_count += 2
                else
                    push!(tree.nodes, TreeNode(pred_leaf(params.loss, node, params, Î´Â²)))
                end # end of single node split search
            end
        end # end of loop over active ids for a given depth
        active_id = next_active_id
        tree_depth += 1
    end # end of tree growth
    return tree
end

# extract the gain value from the vector of best splits and return the split info associated with best split
function get_max_gain(splits::Vector{SplitInfo{L,T,S}}) where {L,T,S}
    gains = (x -> x.gain).(splits)
    feat = findmax(gains)[2]
    best = splits[feat]
    return best
end

function fit_evotree(params, X_train, Y_train;
    X_eval=nothing, Y_eval=nothing,
    early_stopping_rounds=9999,
    eval_every_n=1,
    print_every_n=9999,
    verbosity=1)

    # initialize metric
    iter_since_best = 0
    if params.metric != :none
        metric_track = Metric()
        metric_best = Metric()
    end

    nrounds_max = params.nrounds
    params.nrounds = 0
    model, cache = init_evotree(params, X_train, Y_train)
    iter = 1

    if params.metric != :none && !isnothing(X_eval)
        pred_eval = predict(model.trees[1], X_eval, model.K)
    end

    while model.params.nrounds < nrounds_max && iter_since_best < early_stopping_rounds
        model.params.nrounds += 1
        grow_evotree!(model, cache)
        # callback function
        if params.metric != :none
            if !isnothing(X_eval)
                predict!(pred_eval, model.trees[model.params.nrounds+1], X_eval)
                metric_track.metric = eval_metric(Val{params.metric}(), pred_eval, Y_eval, params.Î±)
            else
                metric_track.metric = eval_metric(Val{params.metric}(), cache.pred, Y_train, params.Î±)
            end
            if metric_track.metric < metric_best.metric
                metric_best.metric = metric_track.metric
                metric_best.iter =  model.params.nrounds
                iter_since_best = 0
            else
                iter_since_best += 1
            end
            if mod(model.params.nrounds, print_every_n) == 0 && verbosity > 0
                display(string("iter:", model.params.nrounds, ", eval: ", metric_track.metric))
            end
        end # end of callback
    end
    if params.metric != :none
        model.metric.iter = metric_best.iter
        model.metric.metric = metric_best.metric
    end
    params.nrounds = nrounds_max
    return model
end
