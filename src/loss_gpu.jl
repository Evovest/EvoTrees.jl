#####################
# linear
#####################
function kernel_linear_Î´!(Î´::CuDeviceMatrix{T}, p::CuDeviceMatrix{T}, t::CuDeviceVector{T}, ğ‘¤::CuDeviceVector{T}) where {T<:AbstractFloat}
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    if i <= length(t)
        @inbounds Î´[i] = 2 * (p[i] - t[i]) * ğ‘¤[i]
    end
    return
end

function kernel_linear_Î´Â²!(Î´Â²::CuDeviceMatrix{T}, p::CuDeviceMatrix{T}, t::CuDeviceVector{T}, ğ‘¤::CuDeviceVector{T}) where {T<:AbstractFloat}
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    if i <= length(t)
        @inbounds Î´Â²[i] = 2 * ğ‘¤[i]
    end
    return
end

# base approach - block built along the cols first, the rows (limit collisions)
function update_grads_gpu!(loss::S, Î´::CuMatrix{T}, Î´Â²::CuMatrix{T}, p::CuMatrix{T}, t::CuVector{T}, ğ‘¤::CuVector{T}; MAX_THREADS=1024) where {S <: GradientRegression, T<:AbstractFloat}
    thread_i = min(MAX_THREADS, length(t))
    threads = (thread_i)
    blocks = ceil.(Int, (length(t)) ./ threads)
    @cuda blocks=blocks threads=threads kernel_linear_Î´!(Î´, p, t, ğ‘¤)
    @cuda blocks=blocks threads=threads kernel_linear_Î´Â²!(Î´Â², p, t, ğ‘¤)
    return
end


# Gradient regression
function get_gain(loss::S, âˆ‘Î´::AbstractVector{T}, âˆ‘Î´Â²::AbstractVector{T}, âˆ‘ğ‘¤::T, Î»::T) where {S <: GradientRegression, T <: AbstractFloat}
    gain = sum((âˆ‘Î´ .^ 2 ./ (âˆ‘Î´Â² .+ Î» .* âˆ‘ğ‘¤)) ./ 2)
    return gain
end


# Gaussian - http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html
# pred[i][1] = Î¼
# pred[i][2] = log(Ïƒ)
function kernel_gauss_Î´!(Î´::CuDeviceMatrix{T}, p::CuDeviceMatrix{T}, t::CuDeviceVector{T}, ğ‘¤::CuDeviceVector{T}) where {T<:AbstractFloat}
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    if i <= length(t)
        Î´[i,1] = (p[i,1] - t[i]) / max(Cfloat(1e-8), exp(2f0 * p[i,2])) * ğ‘¤[i]
        Î´[i,2] = (1f0 - (p[i,1] - t[i])^2f0 / max(Cfloat(1e-8), exp(2f0 * p[i,2]))) * ğ‘¤[i]
    end
    return
end

function kernel_gauss_Î´Â²!(Î´Â²::CuDeviceMatrix{T}, p::CuDeviceMatrix{T}, t::CuDeviceVector{T}, ğ‘¤::CuDeviceVector{T}) where {T<:AbstractFloat}
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    if i <= length(t)
        Î´Â²[i,1] = ğ‘¤[i] / max(Cfloat(1e-8), exp(2 * p[i,2]))
        Î´Â²[i,2] = 2 * ğ‘¤[i] / max(Cfloat(1e-8), exp(2 * p[i,2])) * (p[i,1] - target[i])^2
    end
end

# base approach - block built along the cols first, the rows (limit collisions)
function update_grads_gpu!(loss::S, Î´::CuMatrix{T}, Î´Â²::CuMatrix{T}, p::CuMatrix{T}, t::CuVector{T}, ğ‘¤::CuVector{T}; MAX_THREADS=1024) where {S <: GaussianRegression, T<:AbstractFloat}
    thread_i = min(MAX_THREADS, length(t))
    threads = (thread_i)
    blocks = ceil.(Int, (length(t)) ./ threads)
    @cuda blocks=blocks threads=threads kernel_linear_Î´!(Î´, p, t, ğ‘¤)
    @cuda blocks=blocks threads=threads kernel_linear_Î´Â²!(Î´Â², p, t, ğ‘¤)
    return
end

# GaussianRegression
function get_gain(loss::S, âˆ‘Î´::T, âˆ‘Î´Â²::T, âˆ‘ğ‘¤::T, Î»::T) where {S <: GaussianRegression, T <: AbstractFloat}
    gain = sum((âˆ‘Î´ .^ 2 ./ (âˆ‘Î´Â² .+ Î» .* âˆ‘ğ‘¤)) ./ 2)
    return gain
end
