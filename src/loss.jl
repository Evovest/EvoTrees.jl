# compute the gradient and hessian given target and predict
# linear
function update_grads!(loss::Linear, Î±::T, pred::AbstractArray{T, 1}, target::AbstractArray{T, 1}, Î´::AbstractArray{T, 1}, Î´Â²::AbstractArray{T, 1}, ğ‘¤::AbstractArray{T, 1}) where T <: AbstractFloat
    @. Î´ = 2 * (pred - target) * ğ‘¤
    @. Î´Â² = 2 * ğ‘¤
end

# compute the gradient and hessian given target and predict
# logistic - on linear predictor
function update_grads!(loss::Logistic, Î±::T, pred::AbstractArray{T, 1}, target::AbstractArray{T, 1}, Î´::AbstractArray{T, 1}, Î´Â²::AbstractArray{T, 1}, ğ‘¤::AbstractArray{T, 1}) where T <: AbstractFloat
    @. Î´ = (sigmoid(pred) * (1 - target) - (1 - sigmoid(pred)) * target) * ğ‘¤
    @. Î´Â² = sigmoid(pred) * (1 - sigmoid(pred)) * ğ‘¤
end

# compute the gradient and hessian given target and predict
# poisson
# Reference: https://isaacchanghau.github.io/post/loss_functions/
function update_grads!(loss::Poisson, Î±::T, pred::AbstractArray{T, 1}, target::AbstractArray{T, 1}, Î´::AbstractArray{T, 1}, Î´Â²::AbstractArray{T, 1}, ğ‘¤::AbstractArray{T, 1}) where T <: AbstractFloat
    @. Î´ = (exp(pred) - target) * ğ‘¤
    @. Î´Â² = exp(pred) * ğ‘¤
end

# compute the gradient and hessian given target and predict
# L1
function update_grads!(loss::L1, Î±::T, pred::AbstractArray{T, 1}, target::AbstractArray{T, 1}, Î´::AbstractArray{T, 1}, Î´Â²::AbstractArray{T, 1}, ğ‘¤::AbstractArray{T, 1}) where T <: AbstractFloat
    @. Î´ =  (Î± * max(target - pred, 0) - (1-Î±) * max(pred - target, 0)) * ğ‘¤
end

# compute the gradient and hessian given target and predict
# Quantile
function quantile_grads(pred, target, Î±)
    if target > pred; Î±
    elseif target < pred; Î± - 1
    end
end
function update_grads!(loss::Quantile, Î±::T, pred::AbstractArray{T, 1}, target::AbstractArray{T, 1}, Î´::AbstractArray{T, 1}, Î´Â²::AbstractArray{T, 1}, ğ‘¤::AbstractArray{T, 1}) where T <: AbstractFloat
    @. Î´ =  quantile_grads(pred, target, Î±) * ğ‘¤
    @. Î´Â² =  (target - pred) # No weighting on Î´Â² as it would be applied on the quantile calculation
end


function logit(x::AbstractArray{T, 1}) where T <: AbstractFloat
    @. x = log(x / (1 - x))
    return x
end

function logit(x::T) where T <: AbstractFloat
    x = log(x / (1 - x))
    return x
end

function sigmoid(x::AbstractArray{T, 1}) where T <: AbstractFloat
    @. x = 1 / (1 + exp(-x))
    return x
end

function sigmoid(x::T) where T <: AbstractFloat
    x = 1 / (1 + exp(-x))
    return x
end

# update the performance tracker - GradientRegression
function update_track!(loss::S, track::SplitTrack{T}, Î»::T) where {S <: GradientRegression, T <: AbstractFloat}
    track.gainL = (track.âˆ‘Î´L ^ 2 / (track.âˆ‘Î´Â²L + Î» .* track.âˆ‘ğ‘¤L)) / 2
    track.gainR = (track.âˆ‘Î´R ^ 2 / (track.âˆ‘Î´Â²R + Î» .* track.âˆ‘ğ‘¤R)) / 2
    track.gain = track.gainL + track.gainR
end

# update the performance tracker - L1Regression
function update_track!(loss::S, track::SplitTrack{T}, Î»::T) where {S <: L1Regression, T <: AbstractFloat}
    track.gainL = abs(track.âˆ‘Î´L)
    track.gainR = abs(track.âˆ‘Î´R)
    track.gain = track.gainL + track.gainR
end

# update the performance tracker - QuantileRegression
function update_track!(loss::S, track::SplitTrack{T}, Î»::T) where {S <: QuantileRegression, T <: AbstractFloat}
    track.gainL = abs(track.âˆ‘Î´L) / (1 + Î»)
    track.gainR = abs(track.âˆ‘Î´R) / (1 + Î»)
    track.gain = track.gainL + track.gainR
end

# Calculate the gain for a given split - GradientRegression
function get_gain(loss::S, âˆ‘Î´::T, âˆ‘Î´Â²::T, âˆ‘ğ‘¤::T, Î»::T) where {S <: GradientRegression, T <: AbstractFloat}
    gain = (âˆ‘Î´ ^ 2 / (âˆ‘Î´Â² + Î» * âˆ‘ğ‘¤)) / 2
    return gain
end

# Calculate the gain for a given split - L1Regression
function get_gain(loss::S, âˆ‘Î´::T, âˆ‘Î´Â²::T, âˆ‘ğ‘¤::T, Î»::T) where {S <: L1Regression, T <: AbstractFloat}
    gain = abs(âˆ‘Î´)
    return gain
end

# Calculate the gain for a given split - QuantileRegression
function get_gain(loss::S, âˆ‘Î´::T, âˆ‘Î´Â²::T, âˆ‘ğ‘¤::T, Î»::T) where {S <: QuantileRegression, T <: AbstractFloat}
    # gain = (âˆ‘Î´ ^ 2 / (Î» * âˆ‘ğ‘¤)) / 2
    gain = abs(âˆ‘Î´) / (1 + Î»)
    return gain
end
