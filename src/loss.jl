# linear
function update_grads!(loss::Linear, Î±::T, pred::Vector{SVector{L,T}}, target::AbstractVector{T}, Î´::Vector{SVector{L,T}}, Î´Â²::Vector{SVector{L,T}}, ğ‘¤::Vector{SVector{1,T}}) where {T <: AbstractFloat, L, M}
    @inbounds for i in eachindex(Î´)
        Î´[i] = 2 .* (pred[i] .- target[i]) .* ğ‘¤[i]
        Î´Â²[i] = 2 .* ğ‘¤[i]
    end
end

# logistic - on linear predictor
function update_grads!(loss::Logistic, Î±::T, pred::Vector{SVector{L,T}}, target::AbstractVector{T}, Î´::Vector{SVector{L,T}}, Î´Â²::Vector{SVector{L,T}}, ğ‘¤::Vector{SVector{1,T}}) where {T <: AbstractFloat, L, M}
    @inbounds for i in eachindex(Î´)
        # Î´[i] = (sigmoid.(pred[i]) .* (1 .- target[i]) .- (1 .- sigmoid.(pred[i])) .* target[i]) .* ğ‘¤[i]
        # Î´Â²[i] = sigmoid.(pred[i]) .* (1 .- sigmoid.(pred[i])) .* ğ‘¤[i]
        Î´[i] = (sigmoid(pred[i][1]) * (1 - target[i]) - (1 - sigmoid(pred[i][1])) * target[i][1]) * ğ‘¤[i]
        Î´Â²[i] = sigmoid(pred[i][1]) * (1 - sigmoid(pred[i][1])) * ğ‘¤[i]
    end
end

# Poisson
function update_grads!(loss::Poisson, Î±::T, pred::Vector{SVector{L,T}}, target::AbstractVector{T}, Î´::Vector{SVector{L,T}}, Î´Â²::Vector{SVector{L,T}}, ğ‘¤::Vector{SVector{1,T}}) where {T <: AbstractFloat, L, M}
    @inbounds for i in eachindex(Î´)
        Î´[i] = (exp.(pred[i]) .- target[i]) .* ğ‘¤[i]
        Î´Â²[i] = exp.(pred[i]) .* ğ‘¤[i]
    end
end

# L1
function update_grads!(loss::L1, Î±::T, pred::Vector{SVector{L,T}}, target::AbstractArray{T, 1}, Î´::Vector{SVector{L,T}}, Î´Â²::Vector{SVector{L,T}}, ğ‘¤::Vector{SVector{1,T}}) where {T <: AbstractFloat, L, M}
    @inbounds for i in eachindex(Î´)
        Î´[i] =  (Î± * max(target[i] - pred[i][1], 0) - (1-Î±) * max(pred[i][1] - target[i], 0)) * ğ‘¤[i]
    end
end

# Softmax
function update_grads!(loss::Softmax, Î±::T, pred::Vector{SVector{L,T}}, target::AbstractVector{Int}, Î´::Vector{SVector{L,T}}, Î´Â²::Vector{SVector{L,T}}, ğ‘¤::Vector{SVector{1,T}}) where {T <: AbstractFloat, L, M}
    pred = pred - maximum.(pred)
    # sums = sum(exp.(pred), dims=2)
    @inbounds for i in 1:size(pred,1)
        sums = sum(exp.(pred[i]))
        Î´[i] = (exp.(pred[i]) ./ sums - (onehot(target[i], 1:L))) * ğ‘¤[i][1]
        Î´Â²[i] =  1 / sums * (1 - exp.(pred[i]) ./ sums) * ğ‘¤[i][1]
    end
end

# Quantile
function update_grads!(loss::Quantile, Î±::T, pred::Vector{SVector{L,T}}, target::AbstractVector{T}, Î´::Vector{SVector{L,T}}, Î´Â²::Vector{SVector{L,T}}, ğ‘¤::Vector{SVector{1,T}}) where {T <: AbstractFloat, L, M}
    @inbounds for i in eachindex(Î´)
        Î´[i] = target[i] > pred[i][1] ? Î± * ğ‘¤[i] : (Î± - 1) * ğ‘¤[i]
        Î´Â²[i] = target[i] - pred[i] # Î´Â² serves to calculate the quantile value - hence no weighting on Î´Â²
    end
end

# Gaussian - http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html
function update_grads!(loss::Gaussian, Î±, pred::Vector{SVector{L,T}}, target::AbstractArray{T, 1}, Î´::Vector{SVector{L,T}}, Î´Â²::Vector{SVector{L,T}}, ğ‘¤::Vector{SVector{1,T}}) where {T <: AbstractFloat, L, M}
    @inbounds @threads for i in eachindex(Î´)
        Î´[i] = SVector((pred[i][1] - target[i]) / exp(pred[i][2]) * ğ‘¤[i][1], ğ‘¤[i][1] / 2 * (1 - (pred[i][1] - target[i])^2 / exp(pred[i][2])))
        Î´Â²[i] = SVector(ğ‘¤[i][1] / exp(pred[i][2]), ğ‘¤[i][1] / exp(pred[i][2]) * (pred[i][1] - target[i])^2)
    end
end

# utility functions
function logit(x::AbstractArray{T, 1}) where T <: AbstractFloat
    @. x = log(x / (1 - x))
    return x
end

function logit(x::T) where T <: AbstractFloat
    x = log(x / (1 - x))
    return x
end

function sigmoid(x::AbstractArray{T, 1}) where T <: AbstractFloat
    @. x = 1 / (1 + exp(-x))
    return x
end

function sigmoid(x::T) where T <: AbstractFloat
    x = 1 / (1 + exp(-x))
    return x
end

function softmax(x::AbstractVector{T}) where T <: AbstractFloat
    x .-= maximum(x)
    x = exp.(x) ./ sum(exp.(x))
    return x
end


##############################
# get the gain metric
##############################
# GradientRegression
function get_gain(loss::S, âˆ‘Î´::SVector{L,T}, âˆ‘Î´Â²::SVector{L,T}, âˆ‘ğ‘¤::SVector{1,T}, Î»::T) where {S <: GradientRegression, T <: AbstractFloat, L}
    gain = sum((âˆ‘Î´ .^ 2 ./ (âˆ‘Î´Â² .+ Î» .* âˆ‘ğ‘¤)) ./ 2)
    return gain
end

# MultiClassRegression
function get_gain(loss::S, âˆ‘Î´::SVector{L,T}, âˆ‘Î´Â²::SVector{L,T}, âˆ‘ğ‘¤::SVector{1,T}, Î»::T) where {S <: MultiClassRegression, T <: AbstractFloat, L}
    gain = sum((âˆ‘Î´ .^ 2 ./ (âˆ‘Î´Â² .+ Î» .* âˆ‘ğ‘¤)) ./ 2)
    return gain
end

# L1 Regression
function get_gain(loss::S, âˆ‘Î´::SVector{L,T}, âˆ‘Î´Â²::SVector{L,T}, âˆ‘ğ‘¤::SVector{1,T}, Î»::T) where {S <: L1Regression, T <: AbstractFloat, L}
    gain = sum(abs.(âˆ‘Î´))
    return gain
end

# QuantileRegression
function get_gain(loss::S, âˆ‘Î´::SVector{L,T}, âˆ‘Î´Â²::SVector{L,T}, âˆ‘ğ‘¤::SVector{1,T}, Î»::T) where {S <: QuantileRegression, T <: AbstractFloat, L}
    gain = sum(abs.(âˆ‘Î´) ./ (1 .+ Î»))
    return gain
end

# GaussianRegression
function get_gain(loss::S, âˆ‘Î´::SVector{L,T}, âˆ‘Î´Â²::SVector{L,T}, âˆ‘ğ‘¤::SVector{1,T}, Î»::T) where {S <: GaussianRegression, T <: AbstractFloat, L}
    gain = sum((âˆ‘Î´ .^ 2 ./ (âˆ‘Î´Â² .+ Î» .* âˆ‘ğ‘¤)) ./ 2)
    return gain
end
