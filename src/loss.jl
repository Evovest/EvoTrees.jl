# compute the gradient and hessian given target and predict
# linear
function update_grads!(loss::Linear, Î±::T, pred::AbstractMatrix{T}, target::AbstractVector{T}, Î´::Vector{SVector{L,T}}, Î´Â²::Vector{SVector{L,T}}, ğ‘¤::Vector{SVector{1,T}}) where {T <: AbstractFloat, L, M}
    for i in eachindex(Î´)
        Î´[i] = 2 * (pred[i] - target[i]) * ğ‘¤[i]
        Î´Â²[i] = 2 * ğ‘¤[i]
    end
end

# compute the gradient and hessian given target and predict
# logistic - on linear predictor
function update_grads!(loss::Logistic, Î±::T, pred::AbstractMatrix{T}, target::AbstractVector{T}, Î´::Vector{SVector{L,T}}, Î´Â²::Vector{SVector{L,T}}, ğ‘¤::Vector{SVector{1,T}}) where {T <: AbstractFloat, L, M}
    for i in eachindex(Î´)
        Î´[i] = (sigmoid(pred[i]) * (1 - target[i]) - (1 - sigmoid(pred[i])) * target[i]) * ğ‘¤[i]
        Î´Â²[i] = sigmoid(pred[i]) * (1 - sigmoid(pred[i])) * ğ‘¤[i]
    end
end

# compute the gradient and hessian given target and predict
# poisson
# Reference: https://isaacchanghau.github.io/post/loss_functions/
function update_grads!(loss::Poisson, Î±::T, pred::AbstractMatrix{T}, target::AbstractVector{T}, Î´::Vector{SVector{L,T}}, Î´Â²::Vector{SVector{L,T}}, ğ‘¤::Vector{SVector{1,T}}) where {T <: AbstractFloat, L, M}
    for i in eachindex(Î´)
        Î´[i] = (exp(pred[i]) - target[i]) * ğ‘¤[i]
        Î´Â²[i] = exp(pred[i]) * ğ‘¤[i]
    end
end

# compute the gradient and hessian given target and predict
# L1
function update_grads!(loss::L1, Î±::T, pred::AbstractMatrix{T}, target::AbstractArray{T, 1}, Î´::Vector{SVector{L,T}}, Î´Â²::Vector{SVector{L,T}}, ğ‘¤::Vector{SVector{1,T}}) where {T <: AbstractFloat, L, M}
    for i in eachindex(Î´)
        Î´[i] =  (Î± * max(target[i] - pred[i], 0) - (1-Î±) * max(pred[i] - target[i], 0)) * ğ‘¤[i]
    end
end

# compute the gradient and hessian given target and predict
# poisson
# Reference: https://isaacchanghau.github.io/post/loss_functions/
function update_grads!(loss::Softmax, Î±::T, pred::AbstractMatrix{T}, target::AbstractVector{Int}, Î´::Vector{SVector{L,T}}, Î´Â²::Vector{SVector{L,T}}, ğ‘¤::Vector{SVector{1,T}}) where {T <: AbstractFloat, L, M}
    # max = maximum(pred, dims=2)
    pred = pred .- maximum(pred, dims=2)
    sums = sum(exp.(pred), dims=2)
    for i in 1:size(pred,1)
        for j in 1:size(pred,2)
            if target[i] == j
                Î´[i,j] = (exp(pred[i,j]) / sums[i] - 1) * ğ‘¤[i]
                Î´Â²[i,j] =  1 / sums[i] * (1 - exp(pred[i,j]) / sums[i]) * ğ‘¤[i]
            else
                Î´[i,j] = exp(pred[i,j]) / sums[i] * ğ‘¤[i]
                Î´Â²[i,j] =  1 / sums[i] * (1 - exp(pred[i,j]) / sums[i]) * ğ‘¤[i]
            end
        end
    end
end

# compute the gradient and hessian given target and predict
# Quantile
function quantile_grads(pred, target, Î±)
    if target > pred; Î±
    elseif target < pred; Î± - 1
    end
end
function update_grads!(loss::Quantile, Î±::T, pred::AbstractMatrix{T}, target::AbstractVector{T}, Î´::AbstractMatrix{T}, Î´Â²::AbstractMatrix{T}, ğ‘¤::AbstractVector{T}) where T <: AbstractFloat
    @. Î´ =  quantile_grads(pred, target, Î±) * ğ‘¤
    @. Î´Â² =  (target - pred) # No weighting on Î´Â² as it would be applied on the quantile calculation
end

function logit(x::AbstractArray{T, 1}) where T <: AbstractFloat
    @. x = log(x / (1 - x))
    return x
end

function logit(x::T) where T <: AbstractFloat
    x = log(x / (1 - x))
    return x
end

function sigmoid(x::AbstractArray{T, 1}) where T <: AbstractFloat
    @. x = 1 / (1 + exp(-x))
    return x
end

function sigmoid(x::T) where T <: AbstractFloat
    x = 1 / (1 + exp(-x))
    return x
end

function softmax(x::AbstractVector{T}) where T <: AbstractFloat
    x .-= maximum(x)
    x = exp.(x) ./ sum(exp.(x))
    return x
end


##############################
# get the gain metric
##############################
function get_gain(loss::S, âˆ‘Î´::SVector{L,T}, âˆ‘Î´Â²::SVector{L,T}, âˆ‘ğ‘¤::SVector{1,T}, Î»::T) where {S <: GradientRegression, T <: AbstractFloat, L}
    gain = sum((âˆ‘Î´ .^ 2 ./ (âˆ‘Î´Â² .+ Î» .* âˆ‘ğ‘¤)) ./ 2)
    return gain
end

# Calculate the gain for a given split - MultiClassRegression
function get_gain(loss::S, âˆ‘Î´::SVector{L,T}, âˆ‘Î´Â²::SVector{L,T}, âˆ‘ğ‘¤::SVector{1,T}, Î»::T) where {S <: MultiClassRegression, T <: AbstractFloat, L}
    gain = sum((âˆ‘Î´ .^ 2 ./ (âˆ‘Î´Â² .+ Î» .* âˆ‘ğ‘¤)) ./ 2)
    return gain
end

# Calculate the gain for a given split - L1Regression
function get_gain(loss::S, âˆ‘Î´::SVector{L,T}, âˆ‘Î´Â²::SVector{L,T}, âˆ‘ğ‘¤::SVector{1,T}, Î»::T) where {S <: L1Regression, T <: AbstractFloat, L}
    gain = sum(abs.(âˆ‘Î´))
    return gain
end

# Calculate the gain for a given split - QuantileRegression
function get_gain(loss::S, âˆ‘Î´::SVector{L,T}, âˆ‘Î´Â²::SVector{L,T}, âˆ‘ğ‘¤::SVector{1,T}, Î»::T) where {S <: QuantileRegression, T <: AbstractFloat, L}
    # gain = (âˆ‘Î´ ^ 2 / (Î» * âˆ‘ğ‘¤)) / 2
    gain = abs(âˆ‘Î´) / (1 + Î»)
    return gain
end
