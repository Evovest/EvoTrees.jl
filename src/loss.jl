
# compute the gradient and hessian given target and predict
# linear
function update_grads!(::Val{:linear}, pred::AbstractArray{T, 1}, target::AbstractArray{T, 1}, Î´::AbstractArray{T, 1}, Î´Â²::AbstractArray{T, 1}, ğ‘¤::AbstractArray{T, 1}) where T <: AbstractFloat
    @. Î´ = 2 * (pred - target) * ğ‘¤
    @. Î´Â² = 2 * ğ‘¤
end

# compute the gradient and hessian given target and predict
# logistic - on linear predictor
function update_grads!(::Val{:logistic}, pred::AbstractArray{T, 1}, target::AbstractArray{T, 1}, Î´::AbstractArray{T, 1}, Î´Â²::AbstractArray{T, 1}, ğ‘¤::AbstractArray{T, 1}) where T <: AbstractFloat
    @. Î´ = (sigmoid(pred) * (1 - target) - (1 - sigmoid(pred)) * target) * ğ‘¤
    @. Î´Â² = sigmoid(pred) * (1 - sigmoid(pred)) * ğ‘¤
end

# compute the gradient and hessian given target and predict
# poisson
# Reference: https://isaacchanghau.github.io/post/loss_functions/
function update_grads!(::Val{:poisson}, pred::AbstractArray{T, 1}, target::AbstractArray{T, 1}, Î´::AbstractArray{T, 1}, Î´Â²::AbstractArray{T, 1}, ğ‘¤::AbstractArray{T, 1}) where T <: AbstractFloat
    @. Î´ = (exp(pred) - target) * ğ‘¤
    @. Î´Â² = exp(pred) * ğ‘¤
end

function logit(x::AbstractArray{T, 1}) where T <: AbstractFloat
    @. x = x / (1 - x)
    return Î´, Î´Â²
end

function sigmoid(x::AbstractArray{T, 1}) where T <: AbstractFloat
    @. x = 1 / (1 + exp(-x))
    return x
end

function sigmoid(x::T) where T <: AbstractFloat
    x = 1 / (1 + exp(-x))
    return x
end

# update the performance tracker
function update_track!(track::SplitTrack{T}, Î»::T) where T <: AbstractFloat
    track.gainL = (track.âˆ‘Î´L ^ 2 / (track.âˆ‘Î´Â²L + Î» .* track.âˆ‘ğ‘¤L)) / 2
    track.gainR = (track.âˆ‘Î´R ^ 2 / (track.âˆ‘Î´Â²R + Î» .* track.âˆ‘ğ‘¤R)) / 2
    track.gain = track.gainL + track.gainR
end

# Calculate the gain for a given split
function get_gain(âˆ‘Î´::T, âˆ‘Î´Â²::T, âˆ‘ğ‘¤::T, Î»::T) where T <: AbstractFloat
    gain = (âˆ‘Î´ ^ 2 / (âˆ‘Î´Â² + Î» * âˆ‘ğ‘¤)) / 2
    return gain
end
