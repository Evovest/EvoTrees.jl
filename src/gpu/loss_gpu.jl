# Gradient regression
function get_gain_gpu(::Type{L}, âˆ‘::AbstractVector{T}, lambda::T) where {L<:GradientRegression,T<:AbstractFloat}
    gain = âˆ‘[1]^2 / (âˆ‘[2] + lambda * âˆ‘[3]) / 2
    return gain
end

# Gaussian regression
function get_gain_gpu(::Type{L}, âˆ‘::AbstractVector{T}, lambda::T) where {L<:GaussianRegression,T<:AbstractFloat}
    gain = âˆ‘[1]^2 / (âˆ‘[3] + lambda * âˆ‘[5]) / 2 + âˆ‘[2]^2 / (âˆ‘[4] + lambda * âˆ‘[5]) / 2
    return gain
end

#####################
# linear
#####################
function kernel_linear_Î´ð‘¤!(Î´ð‘¤::CuDeviceMatrix, p::CuDeviceMatrix, y::CuDeviceVector)
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    if i <= length(y)
        @inbounds Î´ð‘¤[1, i] = 2 * (p[i] - y[i]) * Î´ð‘¤[3, i]
        @inbounds Î´ð‘¤[2, i] = 2 * Î´ð‘¤[3, i]
    end
    return
end
function update_grads_gpu!(::Type{Linear}, Î´ð‘¤::CuMatrix, p::CuMatrix, y::CuVector; MAX_THREADS=1024)
    threads = min(MAX_THREADS, length(y))
    blocks = ceil(Int, (length(y)) / threads)
    @cuda blocks = blocks threads = threads kernel_linear_Î´ð‘¤!(Î´ð‘¤, p, y)
    CUDA.synchronize()
    return
end

#####################
# Logistic
#####################
function kernel_logistic_Î´ð‘¤!(Î´ð‘¤::CuDeviceMatrix, p::CuDeviceMatrix, y::CuDeviceVector)
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    if i <= length(y)
        @inbounds pred = sigmoid(p[1, i])
        @inbounds Î´ð‘¤[1, i] = (pred - y[i]) * Î´ð‘¤[3, i]
        @inbounds Î´ð‘¤[2, i] = pred * (1 - pred) * Î´ð‘¤[3, i]
    end
    return
end
function update_grads_gpu!(::Type{Logistic}, Î´ð‘¤::CuMatrix, p::CuMatrix, y::CuVector; MAX_THREADS=1024)
    threads = min(MAX_THREADS, length(y))
    blocks = ceil(Int, (length(y)) / threads)
    @cuda blocks = blocks threads = threads kernel_logistic_Î´ð‘¤!(Î´ð‘¤, p, y)
    CUDA.synchronize()
    return
end

#####################
# Poisson
#####################
function kernel_poisson_Î´ð‘¤!(Î´ð‘¤::CuDeviceMatrix, p::CuDeviceMatrix, y::CuDeviceVector)
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    if i <= length(y)
        @inbounds pred = exp(p[1, i])
        @inbounds Î´ð‘¤[1, i] = (pred - y[i]) * Î´ð‘¤[3, i]
        @inbounds Î´ð‘¤[2, i] = pred * Î´ð‘¤[3, i]
    end
    return
end
function update_grads_gpu!(::Type{Poisson}, Î´ð‘¤::CuMatrix, p::CuMatrix, y::CuVector; MAX_THREADS=1024)
    threads = min(MAX_THREADS, length(y))
    blocks = ceil(Int, (length(y)) / threads)
    @cuda blocks = blocks threads = threads kernel_poisson_Î´ð‘¤!(Î´ð‘¤, p, y)
    CUDA.synchronize()
    return
end

#####################
# Gamma
#####################
function kernel_gamma_Î´ð‘¤!(Î´ð‘¤::CuDeviceMatrix, p::CuDeviceMatrix, y::CuDeviceVector)
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    if i <= length(y)
        pred = exp(p[1, i])
        @inbounds Î´ð‘¤[1, i] = 2 * (1 - y[i] / pred) * Î´ð‘¤[3, i]
        @inbounds Î´ð‘¤[2, i] = 2 * y[i] / pred * Î´ð‘¤[3, i]
    end
    return
end
function update_grads_gpu!(::Type{Gamma}, Î´ð‘¤::CuMatrix, p::CuMatrix, y::CuVector; MAX_THREADS=1024)
    threads = min(MAX_THREADS, length(y))
    blocks = ceil(Int, (length(y)) / threads)
    @cuda blocks = blocks threads = threads kernel_gamma_Î´ð‘¤!(Î´ð‘¤, p, y)
    CUDA.synchronize()
    return
end

#####################
# Tweedie
#####################
function kernel_tweedie_Î´ð‘¤!(Î´ð‘¤::CuDeviceMatrix, p::CuDeviceMatrix, y::CuDeviceVector)
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    rho = eltype(p)(1.5)
    if i <= length(y)
        @inbounds pred = exp(p[1, i])
        @inbounds Î´ð‘¤[1, i] = 2 * (pred^(2 - rho) - y[i] * pred^(1 - rho)) * Î´ð‘¤[3, i]
        @inbounds Î´ð‘¤[2, i] = 2 * ((2 - rho) * pred^(2 - rho) - (1 - rho) * y[i] * pred^(1 - rho)) * Î´ð‘¤[3, i]
    end
    return
end
function update_grads_gpu!(::Type{Tweedie}, Î´ð‘¤::CuMatrix, p::CuMatrix, y::CuVector; MAX_THREADS=1024)
    threads = min(MAX_THREADS, length(y))
    blocks = ceil(Int, (length(y)) / threads)
    @cuda blocks = blocks threads = threads kernel_tweedie_Î´ð‘¤!(Î´ð‘¤, p, y)
    CUDA.synchronize()
    return
end


################################################################################
# Gaussian - http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html
# pred[i][1] = Î¼
# pred[i][2] = log(Ïƒ)
################################################################################
function kernel_gauss_Î´ð‘¤!(Î´ð‘¤::CuDeviceMatrix, p::CuDeviceMatrix, y::CuDeviceVector)
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    @inbounds if i <= length(y)
        # first order gradients
        Î´ð‘¤[1, i] = (p[1, i] - y[i]) / exp(2 * p[2, i]) * Î´ð‘¤[5, i]
        Î´ð‘¤[2, i] = (1 - (p[1, i] - y[i])^2 / exp(2 * p[2, i])) * Î´ð‘¤[5, i]
        # second order gradients
        Î´ð‘¤[3, i] = Î´ð‘¤[5, i] / exp(2 * p[2, i])
        Î´ð‘¤[4, i] = 2 * Î´ð‘¤[5, i] / exp(2 * p[2, i]) * (p[1, i] - y[i])^2
    end
    return
end

function update_grads_gpu!(::Type{Gaussian}, Î´ð‘¤::CuMatrix, p::CuMatrix, y::CuVector; MAX_THREADS=1024)
    threads = min(MAX_THREADS, length(y))
    blocks = ceil(Int, (length(y)) / threads)
    @cuda blocks = blocks threads = threads kernel_gauss_Î´ð‘¤!(Î´ð‘¤, p, y)
    CUDA.synchronize()
    return
end


function update_childs_âˆ‘_gpu!(::Type{L}, nodes, n, bin, feat) where {L}
    nodes[n<<1].âˆ‘ .= nodes[n].hL[:, bin, feat]
    nodes[n<<1+1].âˆ‘ .= nodes[n].hR[:, bin, feat]
    return nothing
end