"""
    build a single histogram containing all grads and weight information
"""
function hist_kernel!(h::CuDeviceArray{T,3}, Î´::CuDeviceMatrix{T}, xid::CuDeviceMatrix{S}, ğ‘–, ğ‘—) where {T,S}
    
    nbins = size(h, 2)

    it = threadIdx().x
    id, jd = blockDim().x, blockDim().y
    ib, j, k = blockIdx().x, blockIdx().y, blockIdx().z
    ig, jg = gridDim().x, gridDim().y
    # j = 1 + (jb - 1) * jd
    
    shared = @cuDynamicSharedMem(T, nbins)
    fill!(shared, 0)
    sync_threads()

    i_tot = length(ğ‘–)
    iter = 0
    while iter * id * ig < i_tot
        i = it + id * (ib - 1) + iter * id * ig
        if i <= length(ğ‘–) && j <= length(ğ‘—)
            # depends on shared to be assigned to a single feature
            @inbounds i_idx = ğ‘–[i]
            @inbounds CUDA.atomic_add!(pointer(shared, xid[i_idx, ğ‘—[j]]), Î´[i_idx, k])   
        end
        iter += 1
    end
    sync_threads()
    # loop to cover cases where nbins > nthreads
    for iter in 1:(nbins - 1) Ã· id + 1
        bin_id = it + id * (iter - 1)
        # bin_id = it
        if bin_id <= nbins
            @inbounds Î´id = Base._to_linear_index(h, k, bin_id, ğ‘—[j])
            @inbounds CUDA.atomic_add!(pointer(h, Î´id), shared[bin_id])
        end
    end
    # sync_threads()
    return nothing
end

# base approach - block built along the cols first, the rows (limit collisions)
function update_hist_gpu!(
    h::CuArray{T,3}, 
    Î´::CuMatrix{T}, 
    X_bin::CuMatrix{UInt8}, 
    ğ‘–::CuVector{S}, 
    ğ‘—::CuVector{S}, 
    K; MAX_THREADS=128) where {T,S}
    
    fill!(h, 0.0)
    thread_i = min(MAX_THREADS, length(ğ‘–))
    # thread_j = 1
    threads = (thread_i,)
    blocks = (1, length(ğ‘—), 2 * K + 1)

    @cuda blocks = blocks threads = threads shmem = sizeof(T) * size(h, 2) hist_kernel!(h, Î´, X_bin, ğ‘–, ğ‘—)
    return
end

# update the vector of length ğ‘– pointing to associated node id
function update_set_kernel!(mask, set, best, x_bin)
    it = threadIdx().x
    ibd = blockDim().x
    ibi = blockIdx().x
    i = it + ibd * (ibi - 1)
    @inbounds if i <= length(set)
        @inbounds mask[i] = x_bin[set[i]] <= best
    end
    return nothing
end

function update_set_gpu(set, best, x_bin; MAX_THREADS=1024)
    mask = CUDA.zeros(Bool, length(set))
    thread_i = min(MAX_THREADS, length(set))
    threads = (thread_i,)
    blocks = (length(set) Ã· thread_i + 1,)
    @cuda blocks = blocks threads = threads update_set_kernel!(mask, set, best, x_bin)
    left, right = set[mask], set[.!mask]
    return left, right
end


"""
find_split_gpu! : V1
    Direct translation of the cpu approach.
"""
# function find_split_gpu!(hist_Î´::AbstractMatrix{T}, hist_Î´Â²::AbstractMatrix{T}, hist_ğ‘¤::AbstractVector{T},
#     params::EvoTypes, node::TrainNodeGPU{T,S}, info::SplitInfoGPU{T,S}, edges::Vector{T}) where {T,S}

#     # initialize tracking
#     âˆ‘Î´L = copy(node.âˆ‘Î´) .* 0
#     âˆ‘Î´Â²L = copy(node.âˆ‘Î´Â²) .* 0
#     âˆ‘ğ‘¤L = node.âˆ‘ğ‘¤ * 0
#     âˆ‘Î´R = copy(node.âˆ‘Î´)
#     âˆ‘Î´Â²R = copy(node.âˆ‘Î´Â²)
#     âˆ‘ğ‘¤R = node.âˆ‘ğ‘¤

#     # println("âˆ‘Î´Â²L: ", âˆ‘Î´Â²L, " âˆ‘Î´Â²R:", âˆ‘Î´Â²R)
#     # println("find_split_gpu! hist_ğ‘¤: ", hist_ğ‘¤)
#     # println("âˆ‘ğ‘¤L: ", âˆ‘ğ‘¤L, " âˆ‘ğ‘¤R: ", âˆ‘ğ‘¤R)

#     @inbounds for bin in 1:(length(hist_Î´) - 1)
#         @views âˆ‘Î´L .+= hist_Î´[:, bin]
#         @views âˆ‘Î´Â²L .+= hist_Î´Â²[:, bin]
#         âˆ‘ğ‘¤L += hist_ğ‘¤[bin]
#         @views âˆ‘Î´R .-= hist_Î´[:, bin]
#         @views âˆ‘Î´Â²R .-= hist_Î´Â²[:, bin]
#         âˆ‘ğ‘¤R -= hist_ğ‘¤[bin]

#         # println("âˆ‘Î´Â²L: ", âˆ‘Î´Â²L, " | âˆ‘Î´Â²R:", âˆ‘Î´Â²R, " | hist_Î´Â²[bin,:]: ", hist_Î´Â²[bin,:])

#         gainL, gainR = get_gain(params.loss, âˆ‘Î´L, âˆ‘Î´Â²L, âˆ‘ğ‘¤L, params.Î»), get_gain(params.loss, âˆ‘Î´R, âˆ‘Î´Â²R, âˆ‘ğ‘¤R, params.Î»)
#         gain = gainL + gainR

#         # println("âˆ‘ğ‘¤L: ", âˆ‘ğ‘¤L, " âˆ‘ğ‘¤R: ", âˆ‘ğ‘¤R)
#         # println("âˆ‘Î´L: ", âˆ‘Î´L, " âˆ‘Î´R: ", âˆ‘Î´R)
#         # println("info.gain: ", info.gain, " gain: ", gain)

#         if gain > info.gain && âˆ‘ğ‘¤L >= params.min_weight + 0.1 && âˆ‘ğ‘¤R >= params.min_weight + 0.1
#             # println("there's a gain on bin: ", bin)
#             info.gain = gain
#             info.gainL = gainL
#             info.gainR = gainR
#             @views info.âˆ‘Î´L .= âˆ‘Î´L
#             @views info.âˆ‘Î´Â²L .= âˆ‘Î´Â²L
#             info.âˆ‘ğ‘¤L = âˆ‘ğ‘¤L
#             @views info.âˆ‘Î´R .= âˆ‘Î´R
#             @views info.âˆ‘Î´Â²R .= âˆ‘Î´Â²R
#             info.âˆ‘ğ‘¤R = âˆ‘ğ‘¤R
#             info.cond = edges[bin]
#             info.ğ‘– = bin
#         end # info update if gain
#     end # loop on bins
# end



"""
find_split_gpu! : V1
    Direct translation of the cpu approach onto the gpu.
"""
function find_split_gpu_v1!(
    hist::CuArray{T,3},
    params::EvoTypes, 
    node::TrainNodeGPU,
    splits::SplitInfoGPU, 
    edges::Vector{Vector{T}},
    ğ‘—, K) where {T}

    fill!(splits.gains, node.gain)

    @cuda blocks = length(ğ‘—) threads = 1 kernel_find_split_gpu!(
        hist, node.âˆ‘,
        splits.gains, splits.gainsL, splits.gainsR, 
        splits.âˆ‘Ls, splits.âˆ‘Rs, 
        splits.bins, 
        params.Î», ğ‘—, K)

    best = findmax(info.gains)
    # best_cond = edges[bin]
    return best
end


function kernel_find_split_gpu!(hist::CuDeviceArray{T,3}, âˆ‘::CuDeviceVector{T},
    gains::CuDeviceVector{T}, gainsL::CuDeviceVector{T}, gainsR::CuDeviceVector{T},
    âˆ‘Ls::CuDeviceMatrix{T}, âˆ‘Rs::CuDeviceMatrix{T},
    bins::CuDeviceVector{S}, 
    Î», ğ‘—, K) where {T,S}

    j = blockIdx().x

    best_bin = UInt8(0)
    gain_L = 0f0
    gain_R = 0f0

    nbins = size(hist, 2)
    ğ‘¤_prev = 0f0

    âˆ‘L = @cuDynamicSharedMem(T, K)
    âˆ‘R = @cuDynamicSharedMem(T, K)
    
    âˆ‘L .= fill!(âˆ‘, 0)
    âˆ‘R .= node.âˆ‘
    sync_threads()

    @inbounds for bin in 1:(nbins - 1)

        @inbounds for i in 1:(2 * K + 1)
            âˆ‘L[k, bin, j] .+= hist[k, bin, j]
            âˆ‘R[k, bin, j] -= hist[k, bin, j]
        end

        # apply test if current bin added weight
        if âˆ‘L[2 * K + 1] > ğ‘¤_prev + 0.001f0
            ğ‘¤_prev = âˆ‘L[2 * K + 1]
            
            @inbounds for k in 1:K
                gain_L = (âˆ‘L[k]^2 / (âˆ‘L[2 * K + k - 1] + Î» * âˆ‘L[2 * K + 1])) / 2
                gain_R = (âˆ‘R[k]^2 / (âˆ‘R[2 * K + k - 1] + Î» * âˆ‘R[2 * K + 1])) / 2
            end
            gain = gain_L + gain_R

            if gain > gains[ğ‘—[j]] # && âˆ‘ğ‘¤L >= params.min_weight + 0.1 && âˆ‘ğ‘¤R >= params.min_weight + 0.1

                gains[ğ‘—[j]] = gain
                gainsL[ğ‘—[j]] = gain_L
                gainsR[ğ‘—[j]] = gain_R
                bins[ğ‘—[j]] = bin

                @inbounds for k in 1:K
                    âˆ‘Ls[k, ğ‘—[j]] = âˆ‘L[k]
                    âˆ‘Rs[k, ğ‘—[j]] = âˆ‘R[k]
                end
            
            end # info update if gain
        end # loop on bins
    end
end


# operate on hist_gpu
"""
find_split_gpu! - V2
    Find best split over gpu histograms
    ! Check for behavior when sme histograms are empty / near zero observations
"""
    function find_split_gpu_v2!(hist::AbstractArray{T,3}, edges::Vector{Vector{T}}, params::EvoTypes) where {T}

        hist_cum_L = cumsum(hist, dims=2)
        # hist_cum_R = sum(hist, dims=2) .- hist_cum_L
        hist_cum_R = hist_cum_L[:,end:end,:] .- hist_cum_L
    
        gains_L = get_hist_gains_gpu(hist_cum_L, params.Î»)
        gains_R = get_hist_gains_gpu(hist_cum_R, params.Î»)
        gains = gains_L + gains_R

        best = findmax(gains)
        gain, bin, feat = best[1], best[2][1], UInt32(best[2][2])
        cond = edges[feat][bin]
        gainL, gainR = gains_L[bin, feat], gains_R[bin, feat]

        âˆ‘L = hist_cum_L[:, bin, feat]
        âˆ‘R = hist_cum_R[:, bin, feat]
        # âˆ‘L = Array(hist_cum_L[:, bin, feat])
        # âˆ‘R = Array(hist_cum_R[:, bin, feat])

        return (gain = gain, bin = bin, feat = feat, cond = cond,
        gainL = gainL, gainR = gainR,
        âˆ‘L = âˆ‘L, âˆ‘R = âˆ‘R)
    end


    function hist_gains_gpu!(gains::CuDeviceMatrix{T}, h::CuDeviceArray{T,3}, Î»::T) where {T}
    
        i, j = threadIdx().x, blockIdx().y
        K = (size(h, 1) - 1) Ã· 2

        @inbounds ğ‘¤ = h[2 * K + 1, i, j] 
    
        @inbounds if ğ‘¤ > 1e-5
            @inbounds for k in 1:K
                @inbounds gains[i, j] += (h[k, i, j]^2 / (h[2 * K + k - 1, i, j] + Î» * ğ‘¤)) / 2
            end
        end

        return nothing
    end

    function get_hist_gains_gpu(h::CuArray{T,3}, Î»::T; MAX_THREADS=1024) where {T}
    
        gains = CUDA.zeros(T, size(h, 1) - 1, size(h, 2))

        thread_i = min(size(gains, 1), MAX_THREADS)
        thread_j = 1
        threads = (thread_i, thread_j)
        blocks = (1, size(gains, 2))

        @cuda blocks = blocks threads = threads hist_gains_gpu!(gains, h, Î»)
        return gains
    end
