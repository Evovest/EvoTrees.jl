# # GPU - apply along the features axis
# function hist_kernel!(h::CuDeviceArray{T,3}, x::CuDeviceMatrix{T}, id, ğ‘–, ğ‘—, K) where {T <: AbstractFloat}
#     i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
#     j = threadIdx().y + (blockIdx().y - 1) * blockDim().y
#     if i <= length(ğ‘–) && j <= length(ğ‘—)
#         for k in 1:K
#             @inbounds pt = Base._to_linear_index(h, id[ğ‘–[i], ğ‘—[j]], k, ğ‘—[j])
#             @inbounds CUDA.atomic_add!(pointer(h, pt), x[ğ‘–[i],k])
#         end
#     end
#     return
# end

# # for 2D input: ğ‘¤
# function hist_kernel!(h::CuDeviceMatrix{T}, x::CuDeviceVector{T}, id, ğ‘–, ğ‘—) where {T <: AbstractFloat}
#     i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
#     j = threadIdx().y + (blockIdx().y - 1) * blockDim().y
#     if i <= length(ğ‘–) && j <= length(ğ‘—)
#         @inbounds pt = Base._to_linear_index(h, id[ğ‘–[i], ğ‘—[j]], ğ‘—[j])
#         @inbounds CUDA.atomic_add!(pointer(h, pt), x[ğ‘–[i]])
#     end
#     return
# end

# base approach - block built along the cols first, the rows (limit collisions)
# function update_hist_gpu!(hÎ´::CuArray{T,3}, hÎ´Â²::CuArray{T,3}, hğ‘¤::CuMatrix{T},
#     Î´::CuMatrix{T}, Î´Â²::CuMatrix{T}, ğ‘¤::CuVector{T},
#     X_bin::CuMatrix{Int}, ğ‘–::CuVector{Int}, ğ‘—::CuVector{Int}, K; MAX_THREADS=1024) where {T <: AbstractFloat}

#     hÎ´ .= T(0.0)
#     hÎ´Â² .= T(0.0)
#     hğ‘¤ .= T(0.0)

#     thread_j = min(MAX_THREADS, length(ğ‘—))
#     thread_i = min(MAX_THREADS Ã· thread_j, length(ğ‘–))
#     threads = (thread_i, thread_j)
#     blocks = ceil.(Int, (length(ğ‘–), length(ğ‘—)) ./ threads)
#     @cuda blocks = blocks threads = threads hist_kernel!(hÎ´, Î´, X_bin, ğ‘–, ğ‘—, K)
#     @cuda blocks = blocks threads = threads hist_kernel!(hÎ´Â², Î´Â², X_bin, ğ‘–, ğ‘—, K)
#     @cuda blocks = blocks threads = threads hist_kernel!(hğ‘¤, ğ‘¤, X_bin, ğ‘–, ğ‘—)

#     return
# end

function hist_kernel!(hÎ´1::CuDeviceArray{T,3}, hÎ´2::CuDeviceArray{T,3}, hğ‘¤::CuDeviceMatrix{T}, Î´1::CuDeviceMatrix{T}, Î´2::CuDeviceMatrix{T}, ğ‘¤::CuDeviceVector{T}, xid::CuDeviceMatrix{S}, ğ‘–, ğ‘—) where {T,S}
    
    K = size(hÎ´1, 1)
    Ks = 2 * K + 1
    nbins = size(hğ‘¤, 1)

    it, jt = threadIdx().x, threadIdx().y
    ib, jb = blockIdx().x, blockIdx().y
    id, jd = blockDim().x, blockDim().y
    ig, jg = gridDim().x, gridDim().y
    j = jt + (jb - 1) * jd
    
    shared = @cuDynamicSharedMem(T, Ks * nbins)
    fill!(shared, 0)
    sync_threads()

    i_tot = length(ğ‘–)
    iter = 0
    while iter * id * ig < i_tot
        i = it + id * (ib - 1) + iter * id * ig
        if i <= length(ğ‘–) && j <= length(ğ‘—)
            # depends on shared to be assigned to a single feature
            @inbounds i_idx = ğ‘–[i]
            @inbounds k0 = Ks * (xid[i_idx, ğ‘—[j]] - 1) # pointer to shared mem position - 1
            for k in 1:K
                @inbounds CUDA.atomic_add!(pointer(shared, k0 + k), Î´1[i_idx, k])
                @inbounds CUDA.atomic_add!(pointer(shared, k0 + 2 * K + k - 1), Î´2[i_idx, k])
            end
            @inbounds CUDA.atomic_add!(pointer(shared, k0 + Ks), ğ‘¤[i_idx])
        
        end
        iter += 1
    end
    sync_threads()
    # loop to cover cases where nbins > nthreads
    for iter in 1:(nbins - 1) Ã· id + 1
        bin_id = it + id * (iter - 1)
        if bin_id <= nbins
            @inbounds kÎ´0 = Base._to_linear_index(hÎ´1, 1, bin_id, ğ‘—[j])
            @inbounds kğ‘¤0 = Base._to_linear_index(hğ‘¤, bin_id, ğ‘—[j])
            for k in 1:K
                @inbounds CUDA.atomic_add!(pointer(hÎ´1, kÎ´0 + k - 1), shared[Ks * (bin_id - 1) + k])
                @inbounds CUDA.atomic_add!(pointer(hÎ´2, kÎ´0 + k - 1), shared[Ks * (bin_id - 1) + 2 * K + k - 1])
            end
            @inbounds CUDA.atomic_add!(pointer(hğ‘¤, kğ‘¤0), shared[Ks * bin_id])
        end
    end
    # sync_threads()
    return nothing
end

# base approach - block built along the cols first, the rows (limit collisions)
function update_hist_gpu!(hÎ´::CuArray{T,3}, hÎ´Â²::CuArray{T,3}, hğ‘¤::CuMatrix{T},
        Î´::CuMatrix{T}, Î´Â²::CuMatrix{T}, ğ‘¤::CuVector{T},
        X_bin::CuMatrix{UInt8}, ğ‘–::CuVector{S}, ğ‘—::CuVector{S}, K; MAX_THREADS=128) where {T,S}
    
    fill!(hÎ´, 0.0)
    fill!(hÎ´Â², 0.0)
    fill!(hğ‘¤, 0.0)

    thread_i = min(MAX_THREADS, length(ğ‘–))
    thread_j = 1
    threads = (thread_i, thread_j)
    blocks = (8, length(ğ‘—))

    @cuda blocks = blocks threads = threads shmem = sizeof(T) * size(hğ‘¤, 1) * (2 * K + 1) hist_kernel!(hÎ´, hÎ´Â², hğ‘¤, Î´, Î´Â², ğ‘¤, X_bin, ğ‘–, ğ‘—)
    return
end

# base approach - block built along the cols first, the rows (limit collisions)
# function update_hist_gpu!(hÎ´::CuArray{T,3}, hÎ´Â²::CuArray{T,3}, hğ‘¤::CuMatrix{T},
#     Î´::CuMatrix{T}, Î´Â²::CuMatrix{T}, ğ‘¤::CuVector{T},
#     X_bin::CuMatrix{Int}, ğ‘–::CuVector{Int}, ğ‘—::CuVector{Int}, K; MAX_THREADS=1024) where {T <: AbstractFloat}

#     hÎ´ .= T(0.0)
#     hÎ´Â² .= T(0.0)
#     hğ‘¤ .= T(0.0)

#     thread_j = min(MAX_THREADS, length(ğ‘—))
#     thread_i = min(MAX_THREADS Ã· thread_j, length(ğ‘–))
#     threads = (thread_i, thread_j)
#     blocks = ceil.(Int, (length(ğ‘–), length(ğ‘—)) ./ threads)
#     @cuda blocks = blocks threads = threads hist_kernel!(hÎ´, Î´, X_bin, ğ‘–, ğ‘—, K)
#     @cuda blocks = blocks threads = threads hist_kernel!(hÎ´Â², Î´Â², X_bin, ğ‘–, ğ‘—, K)
#     @cuda blocks = blocks threads = threads hist_kernel!(hğ‘¤, ğ‘¤, X_bin, ğ‘–, ğ‘—)

#     return
# end

function find_split_gpu!(hist_Î´::AbstractMatrix{T}, hist_Î´Â²::AbstractMatrix{T}, hist_ğ‘¤::AbstractVector{T},
    params::EvoTypes, node::TrainNode_gpu{T,S}, info::SplitInfo_gpu{T,S}, edges::Vector{T}) where {T,S}

    # initialize tracking
    âˆ‘Î´L = copy(node.âˆ‘Î´) .* 0
    âˆ‘Î´Â²L = copy(node.âˆ‘Î´Â²) .* 0
    âˆ‘ğ‘¤L = node.âˆ‘ğ‘¤ * 0
    âˆ‘Î´R = copy(node.âˆ‘Î´)
    âˆ‘Î´Â²R = copy(node.âˆ‘Î´Â²)
    âˆ‘ğ‘¤R = node.âˆ‘ğ‘¤

    # println("âˆ‘Î´Â²L: ", âˆ‘Î´Â²L, " âˆ‘Î´Â²R:", âˆ‘Î´Â²R)
    # println("find_split_gpu! hist_ğ‘¤: ", hist_ğ‘¤)
    # println("âˆ‘ğ‘¤L: ", âˆ‘ğ‘¤L, " âˆ‘ğ‘¤R: ", âˆ‘ğ‘¤R)

    @inbounds for bin in 1:(length(hist_Î´) - 1)
        @views âˆ‘Î´L .+= hist_Î´[:, bin]
        @views âˆ‘Î´Â²L .+= hist_Î´Â²[:, bin]
        âˆ‘ğ‘¤L += hist_ğ‘¤[bin]
        @views âˆ‘Î´R .-= hist_Î´[:, bin]
        @views âˆ‘Î´Â²R .-= hist_Î´Â²[:, bin]
        âˆ‘ğ‘¤R -= hist_ğ‘¤[bin]

        # println("âˆ‘Î´Â²L: ", âˆ‘Î´Â²L, " | âˆ‘Î´Â²R:", âˆ‘Î´Â²R, " | hist_Î´Â²[bin,:]: ", hist_Î´Â²[bin,:])

        gainL, gainR = get_gain(params.loss, âˆ‘Î´L, âˆ‘Î´Â²L, âˆ‘ğ‘¤L, params.Î»), get_gain(params.loss, âˆ‘Î´R, âˆ‘Î´Â²R, âˆ‘ğ‘¤R, params.Î»)
        gain = gainL + gainR

        # println("âˆ‘ğ‘¤L: ", âˆ‘ğ‘¤L, " âˆ‘ğ‘¤R: ", âˆ‘ğ‘¤R)
        # println("âˆ‘Î´L: ", âˆ‘Î´L, " âˆ‘Î´R: ", âˆ‘Î´R)
        # println("info.gain: ", info.gain, " gain: ", gain)

        if gain > info.gain && âˆ‘ğ‘¤L >= params.min_weight + 0.1 && âˆ‘ğ‘¤R >= params.min_weight + 0.1
            # println("there's a gain on bin: ", bin)
            info.gain = gain
            info.gainL = gainL
            info.gainR = gainR
            @views info.âˆ‘Î´L .= âˆ‘Î´L
            @views info.âˆ‘Î´Â²L .= âˆ‘Î´Â²L
            info.âˆ‘ğ‘¤L = âˆ‘ğ‘¤L
            @views info.âˆ‘Î´R .= âˆ‘Î´R
            @views info.âˆ‘Î´Â²R .= âˆ‘Î´Â²R
            info.âˆ‘ğ‘¤R = âˆ‘ğ‘¤R
            info.cond = edges[bin]
            info.ğ‘– = bin
        end # info update if gain
    end # loop on bins
end


# operate on hist_gpu
function find_split_gpu2!(hist_Î´::AbstractArray{T,3}, hist_Î´Â²::AbstractArray{T,3}, hist_ğ‘¤::AbstractMatrix{T}, edges::Vector{Vector{T}}, params::EvoTypes) where {T}

    hist_Î´_cum_L = cumsum(hist_Î´, dims=2)
    hist_Î´Â²_cum_L = cumsum(hist_Î´Â², dims=2)
    hist_ğ‘¤_cum_L = cumsum(hist_ğ‘¤, dims=1)

    hist_Î´_cum_R = sum(hist_Î´, dims=2) .- hist_Î´_cum_L
    hist_Î´Â²_cum_R = sum(hist_Î´Â², dims=2) .- hist_Î´Â²_cum_L
    hist_ğ‘¤_cum_R = sum(hist_ğ‘¤, dims=1) .- hist_ğ‘¤_cum_L

    gains_L = get_gain_gpu(hist_Î´_cum_L, hist_Î´Â²_cum_L, hist_ğ‘¤_cum_L, params.Î»)
    gains_R = get_gain_gpu(hist_Î´_cum_R, hist_Î´Â²_cum_R, hist_ğ‘¤_cum_R, params.Î»)
    gains = gains_L + gains_R

    best = findmax(gains)
    gain, bin, feat = best[1], best[2][1], UInt32(best[2][2])
    cond = edges[feat][bin]
    gainL, gainR = Array(gains_L)[bin, feat], Array(gains_R)[bin, feat]

    âˆ‘Î´L, âˆ‘Î´Â²L, âˆ‘ğ‘¤L = Array(hist_Î´_cum_L[:, bin, feat]), Array(hist_Î´Â²_cum_L[:, bin, feat]), Array(hist_ğ‘¤_cum_L)[bin, feat]
    âˆ‘Î´R, âˆ‘Î´Â²R, âˆ‘ğ‘¤R = Array(hist_Î´_cum_R[:, bin, feat]), Array(hist_Î´Â²_cum_R[:, bin, feat]), Array(hist_ğ‘¤_cum_R)[bin, feat]

    return (gain = gain, bin = bin, feat = feat, cond = cond,
        gainL = gainL, gainR = gainR,
        âˆ‘Î´L = âˆ‘Î´L, âˆ‘Î´Â²L = âˆ‘Î´Â²L, âˆ‘ğ‘¤L = âˆ‘ğ‘¤L,
        âˆ‘Î´R = âˆ‘Î´R, âˆ‘Î´Â²R = âˆ‘Î´Â²R, âˆ‘ğ‘¤R = âˆ‘ğ‘¤R)
end


function gain_kernel!(gains::CuDeviceMatrix{T}, hÎ´1::CuDeviceArray{T,3}, hÎ´2::CuDeviceArray{T,3}, hğ‘¤::CuDeviceMatrix{T}, Î»::T) where {T}
    
    nbins = size(gains, 1)
    i, jt = threadIdx().x, threadIdx().y
    ib, j = blockIdx().x, blockIdx().y
    
    ğ‘¤ = hğ‘¤[i, j] 
    @inbounds if ğ‘¤ > 1e-8
        @inbounds for k in 1:size(hÎ´1, 1)
            @inbounds gains[i, j] += (hÎ´1[k, i, j]^2 / (hÎ´2[k, i, j] + Î» * ğ‘¤)) / 2
        end
    end

    return nothing
end

# base approach - block built along the cols first, the rows (limit collisions)
function get_gain_gpu(hÎ´1::CuArray{T,3}, hÎ´2::CuArray{T,3}, hğ‘¤::CuMatrix{T},
        Î»::T; MAX_THREADS=1024) where {T}
    
    gains = CUDA.zeros(T, size(hğ‘¤, 1) - 1, size(hğ‘¤, 2))

    thread_i = min(size(gains, 1), MAX_THREADS)
    thread_j = 1
    threads = (thread_i, thread_j)
    blocks = (1, size(gains, 2))

    @cuda blocks = blocks threads = threads gain_kernel!(gains, hÎ´1, hÎ´2, hğ‘¤, Î»)
    return gains
end
