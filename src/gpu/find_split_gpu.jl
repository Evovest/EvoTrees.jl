"""
    build a single histogram containing all grads and weight information
"""
function hist_kernel!(h::CuDeviceArray{T,3}, Î´::CuDeviceMatrix{T}, xid::CuDeviceMatrix{S}, ğ‘–, ğ‘—) where {T,S}
    
    nbins = size(h, 2)

    it = threadIdx().x
    id, jd = blockDim().x, blockDim().y
    ib, j, k = blockIdx().x, blockIdx().y, blockIdx().z
    ig, jg = gridDim().x, gridDim().y
    # j = 1 + (jb - 1) * jd
    
    shared = @cuDynamicSharedMem(T, nbins)
    fill!(shared, 0)
    sync_threads()

    i_tot = length(ğ‘–)
    iter = 0
    while iter * id * ig < i_tot
        i = it + id * (ib - 1) + iter * id * ig
        if i <= length(ğ‘–) && j <= length(ğ‘—)
            # depends on shared to be assigned to a single feature
            @inbounds i_idx = ğ‘–[i]
            @inbounds CUDA.atomic_add!(pointer(shared, xid[i_idx, ğ‘—[j]]), Î´[i_idx, k])   
        end
        iter += 1
    end
    sync_threads()
    # loop to cover cases where nbins > nthreads
    for iter in 1:(nbins - 1) Ã· id + 1
        bin_id = it + id * (iter - 1)
        # bin_id = it
        if bin_id <= nbins
            @inbounds Î´id = Base._to_linear_index(h, k, bin_id, ğ‘—[j])
            @inbounds CUDA.atomic_add!(pointer(h, Î´id), shared[bin_id])
        end
    end
    # sync_threads()
    return nothing
end

# base approach - block built along the cols first, the rows (limit collisions)
function update_hist_gpu!(
    h::CuArray{T,3}, 
    Î´::CuMatrix{T}, 
    X_bin::CuMatrix{UInt8}, 
    ğ‘–::CuVector{S}, 
    ğ‘—::CuVector{S}, 
    K; MAX_THREADS=128) where {T,S}
    
    fill!(h, 0.0)
    thread_i = min(MAX_THREADS, length(ğ‘–))
    # thread_j = 1
    threads = (thread_i,)
    blocks = (1, length(ğ‘—), 2 * K + 1)

    @cuda blocks = blocks threads = threads shmem = sizeof(T) * size(h, 2) hist_kernel!(h, Î´, X_bin, ğ‘–, ğ‘—)
    return
end

# update the vector of length ğ‘– pointing to associated node id
function update_set_kernel!(mask, set, best, x_bin)
    it = threadIdx().x
    ibd = blockDim().x
    ibi = blockIdx().x
    i = it + ibd * (ibi - 1)
    @inbounds if i <= length(set)
        @inbounds mask[i] = x_bin[set[i]] <= best
    end
    return nothing
end

<<<<<<< HEAD
function update_set_gpu(set, best, x_bin; MAX_THREADS=1024)
    mask = CUDA.zeros(Bool, length(set))
    thread_i = min(MAX_THREADS, length(set))
    threads = (thread_i,)
    blocks = (length(set) Ã· thread_i + 1,)
    @cuda blocks = blocks threads = threads update_set_kernel!(mask, set, best, x_bin)
    left, right = set[mask], set[.!mask]
    return left, right
end
=======
# base approach - block built along the cols first, the rows (limit collisions)
function update_hist_gpu!(hÎ´::CuArray{T,3}, hÎ´Â²::CuArray{T,3}, hğ‘¤::CuMatrix{T},
    Î´::CuMatrix{T}, Î´Â²::CuMatrix{T}, ğ‘¤::CuVector{T},
    X_bin::CuMatrix{UInt8}, ğ‘–::CuVector{Int}, ğ‘—::CuVector{Int}, K; MAX_THREADS=1024) where {T <: AbstractFloat}

    hÎ´ .= T(0.0)
    hÎ´Â² .= T(0.0)
    hğ‘¤ .= T(0.0)

    thread_j = min(MAX_THREADS, length(ğ‘—))
    thread_i = min(MAX_THREADS Ã· thread_j, length(ğ‘–))
    threads = (thread_i, thread_j)
    blocks = ceil.(Int, (length(ğ‘–), length(ğ‘—)) ./ threads)
    @cuda blocks = blocks threads = threads hist_kernel!(hÎ´, Î´, X_bin, ğ‘–, ğ‘—, K)
    @cuda blocks = blocks threads = threads hist_kernel!(hÎ´Â², Î´Â², X_bin, ğ‘–, ğ‘—, K)
    @cuda blocks = blocks threads = threads hist_kernel!(hğ‘¤, ğ‘¤, X_bin, ğ‘–, ğ‘—)
>>>>>>> dev


# operate on hist_gpu
"""
find_split_gpu!
    Find best split over gpu histograms
"""

function find_split_gpu!(hist::AbstractArray{T,3}, edges::Vector{Vector{T}}, params::EvoTypes) where {T}

    hist_cum_L = cumsum(hist, dims=2)
    # hist_cum_R = sum(hist, dims=2) .- hist_cum_L
    hist_cum_R = hist_cum_L[:,end:end,:] .- hist_cum_L
    
    gains_L = get_hist_gains_gpu(hist_cum_L[:,1:(end - 1),:], params.Î»)
    gains_R = get_hist_gains_gpu(hist_cum_R[:,1:(end - 1),:], params.Î»)
    gains = gains_L + gains_R

    best = findmax(gains)
    gain, bin, feat = best[1], best[2][1], UInt32(best[2][2])
    cond = edges[feat][bin]
    # gainL, gainR = gains_L[bin, feat], gains_R[bin, feat]
    gainL, gainR = Array(gains_L)[bin, feat], Array(gains_R)[bin, feat]

    # âˆ‘L = hist_cum_L[:, bin, feat]
    # âˆ‘R = hist_cum_R[:, bin, feat]
    âˆ‘L = Array(hist_cum_L[:, bin, feat])
    âˆ‘R = Array(hist_cum_R[:, bin, feat])

    return (gain = gain, bin = bin, feat = feat, cond = cond,
        gainL = gainL, gainR = gainR,
        âˆ‘L = âˆ‘L, âˆ‘R = âˆ‘R)
end


function hist_gains_gpu!(gains::CuDeviceMatrix{T}, h::CuDeviceArray{T,3}, Î»::T) where {T}
    
    i, j = threadIdx().x, blockIdx().x
    K = (size(h, 1) - 1) Ã· 2

    @inbounds ğ‘¤ = h[2 * K + 1, i, j]     
    if ğ‘¤ > 1e-5
        @inbounds for k in 1:K
            if k == 1
                gains[i, j] = (h[k, i, j]^2 / (h[2 * K + k - 1, i, j] + Î» * ğ‘¤)) / 2
            else
                gains[i, j] += (h[k, i, j]^2 / (h[2 * K + k - 1, i, j] + Î» * ğ‘¤)) / 2
            end
        end
    end

    return nothing
end

function get_hist_gains_gpu(h::CuArray{T,3}, Î»::T; MAX_THREADS=1024) where {T}
    
    # gains = CUDA.zeros(T, size(h, 2) - 1, size(h, 3))
    gains = CUDA.fill(T(-Inf), size(h, 2) - 1, size(h, 3))
    
    thread_i = min(size(gains, 1), MAX_THREADS)
    threads = thread_i
    blocks = size(gains, 2)

    @cuda blocks = blocks threads = threads hist_gains_gpu!(gains, h, Î»)
    return gains
end
