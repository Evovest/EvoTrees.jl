# initialise evotree
function init_evotree_gpu(params::EvoTypes{T,U,S},
    X::AbstractMatrix, Y::AbstractVector; verbosity=1) where {T,U,S}

    K = 1
    levels = ""
    X = convert(Matrix{T}, X)
    
    if typeof(params.loss) == Logistic
        Y_cpu = T.(Y)
        Y = CuArray(Y_cpu)
        Î¼ = [logit(mean(Y))]
    elseif typeof(params.loss) == Poisson
        Y_cpu = T.(Y)
        Y = CuArray(Y_cpu)
        Î¼ = fill(log(mean(Y)), 1)
    elseif typeof(params.loss) == Softmax
        if typeof(Y) <: AbstractCategoricalVector
            levels = CategoricalArray(CategoricalArrays.levels(Y))
            K = length(levels)
            Î¼ = zeros(T, K)
            Y = MLJModelInterface.int.(Y)
        else
            levels = CategoricalArray(sort(unique(Y)))
            K = length(levels)
            Î¼ = zeros(T, K)
            Y = UInt32.(Y)
        end
    elseif typeof(params.loss) == Gaussian
        K = 2
        Y_cpu = T.(Y)
        Y = CuArray(Y_cpu)
        Î¼ = [mean(Y), log(std(Y))]
    else
        Y_cpu = T.(Y)
        Y = CuArray(Y_cpu)
        Î¼ = [mean(Y)]
    end

    # initialize preds
    X_size = size(X)
    pred_cpu = zeros(T, X_size[1], K)
    pred_gpu = CUDA.zeros(T, X_size[1], K)
    pred_cpu .= Î¼'
    pred_gpu .= CuArray(Î¼)'

    bias = TreeGPU(CuArray(Î¼))
    evotree = GBTreeGPU([bias], params, Metric(), UInt32(K), levels)

    ð‘–_ = UInt32.(collect(1:X_size[1]))
    ð‘—_ = UInt32.(collect(1:X_size[2]))
    ð‘– = zeros(eltype(ð‘–_), ceil(Int, params.rowsample * X_size[1]))
    ð‘— = zeros(eltype(ð‘—_), ceil(Int, params.colsample * X_size[2]))
    ð‘› = CUDA.ones(eltype(ð‘–_), length(ð‘–_))

    # initialize gradients and weights
    Î´ = CUDA.ones(T, X_size[1], 2 * K + 1)

    # binarize data into quantiles
    edges = get_edges(X, params.nbins)
    X_bin = CuArray(binarize(X, edges))

    # initializde histograms
    hist = CUDA.zeros(T, 2 * K + 1, params.nbins, X_size[2], 2^params.max_depth - 1)
    histL = CUDA.zeros(T, 2 * K + 1, params.nbins, X_size[2], 2^params.max_depth - 1)
    histR = CUDA.zeros(T, 2 * K + 1, params.nbins, X_size[2], 2^params.max_depth - 1)
    gains = CUDA.fill(T(-Inf), params.nbins, X_size[2], 2^params.max_depth - 1)

    # store cache
    cache = (params = deepcopy(params),
        X = X, Y = Y, Y_cpu = Y_cpu, K = K,
        pred_gpu = pred_gpu, pred_cpu = pred_cpu,
        ð‘–_ = ð‘–_, ð‘—_ = ð‘—_, ð‘– = ð‘–, ð‘— = ð‘—, ð‘› = ð‘›,
        Î´ = Î´,
        edges = edges, 
        X_bin = X_bin,
        gains = gains,
        hist = hist, histL = histL, histR = histR)

    cache.params.nrounds = 0

    return evotree, cache
end


function grow_evotree!(evotree::GBTreeGPU{T,S}, cache; verbosity=1) where {T,S}

    # initialize from cache
    params = evotree.params
    X_size = size(cache.X_bin)
    Î´nrounds = params.nrounds - cache.params.nrounds

    # loop over nrounds
    for i in 1:Î´nrounds
        # select random rows and cols
        sample!(params.rng, cache.ð‘–_, cache.ð‘–, replace=false, ordered=true)
        sample!(params.rng, cache.ð‘—_, cache.ð‘—, replace=false, ordered=true)
        # build a new tree
        update_grads_gpu!(params.loss, cache.Î´, cache.pred_gpu, cache.Y)
        # # assign a root and grow tree
        tree = TreeGPU(UInt32(params.max_depth), evotree.K, params.Î»)
        grow_tree_gpu!(tree, params, cache.Î´, cache.hist, cache.histL, cache.histR, cache.gains, cache.edges, CuVector(cache.ð‘–), CuVector(cache.ð‘—), cache.ð‘›, cache.X_bin);
        push!(evotree.trees, tree)
        # bad GPU usage - to be improved!
        predict_gpu!(cache.pred_gpu, tree, cache.X_bin)
    end # end of nrounds
    cache.params.nrounds = params.nrounds
    # return model, cache
    return evotree
end

# grow a single tree - grow through all depth
function grow_tree_gpu!(
    tree::TreeGPU{T},
    params::EvoTypes{T,U,S},
    Î´::AbstractMatrix{T},
    hist::AbstractArray{T,4}, histL::AbstractArray{T,4}, histR::AbstractArray{T,4},
    gains::AbstractArray{T,3},
    edges,
    ð‘–, ð‘—, ð‘›,
    X_bin::AbstractMatrix) where {T,U,S}

    # reset
    # bval, bidx = [zero(T)], [(0,0)]
    hist .= 0
    histL .= 0
    histR .= 0
    gains .= -Inf
    ð‘› .= 1

    # grow while there are remaining active nodes
    for depth in 1:(params.max_depth - 1)
        nid = 2^(depth - 1):2^(depth) - 1
        # println("sum hist: ", sum(hist[3,:,:,1]))
        update_hist_gpu!(hist, Î´, X_bin, ð‘–, ð‘—, ð‘›, depth, MAX_THREADS=512)
        update_gains_gpu!(gains, hist, histL, histR, ð‘—, params, nid, depth)
        @inbounds for n in nid
            best = findmax(view(gains, :, :, n))
            # println("best: ", best)
            if best[2][1] != params.nbins && best[1] > -Inf
                tree.gain[n] = best[1]
                tree.feat[n] = best[2][2]
                tree.cond_bin[n] = best[2][1]
                tree.cond_float[n] = edges[tree.feat[n]][tree.cond_bin[n]]
            end
            tree.split[n] = tree.cond_bin[n] != 0
            if !tree.split[n]
                tree.pred[1, n] = pred_leaf_gpu(params, histL, ð‘—[1], n)
            end
        end
        update_set_gpu!(ð‘›, ð‘–, X_bin, tree.feat, tree.cond_bin, params.nbins)
    end # end of loop over active ids for a given depth

    # loop on final depth to assign preds
    for n in 2^(params.max_depth-1):2^params.max_depth-1
        # check that parent is a split node
        pid = n >> 1 # parent id
        if tree.split[pid]
            if n % 2 == 0
                tree.pred[1, n] = pred_leaf_gpu(params, histL, tree.feat[pid], pid, tree.cond_bin[pid])
            else
                tree.pred[1, n] = pred_leaf_gpu(params, histR, tree.feat[pid], pid, tree.cond_bin[pid])
            end
        end
    end
    return nothing
end
