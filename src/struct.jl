# define an abstrat tree node type - concrete types are TreeSplit and TreeLeaf
abstract type Node{T<:AbstractFloat} end

abstract type ModelType end
abstract type GradientRegression <: ModelType end
abstract type L1Regression <: ModelType end
abstract type QuantileRegression <: ModelType end
struct Linear <: GradientRegression end
struct Poisson <: GradientRegression end
struct Logistic <: GradientRegression end
struct L1 <: L1Regression end
struct Quantile <: QuantileRegression end

# compact alternative to ModeLData - not used for now
# To Do: how to exploit pre-sorting and binning
struct TrainData{T<:AbstractFloat}
    X::Matrix{T}
    X_permsort::Matrix{T}
    Y::Matrix{T}
    Î´::Vector{T}
    Î´Â²::Vector{T}
    ð‘¤::Vector{T}
end

mutable struct SplitInfo{T<:AbstractFloat, S<:Int}
    gain::T
    âˆ‘Î´L::T
    âˆ‘Î´Â²L::T
    âˆ‘ð‘¤L::T
    âˆ‘Î´R::T
    âˆ‘Î´Â²R::T
    âˆ‘ð‘¤R::T
    gainL::T
    gainR::T
    ð‘–::S
    feat::S
    cond::T
end

mutable struct SplitTrack{T<:AbstractFloat}
    âˆ‘Î´L::T
    âˆ‘Î´Â²L::T
    âˆ‘ð‘¤L::T
    âˆ‘Î´R::T
    âˆ‘Î´Â²R::T
    âˆ‘ð‘¤R::T
    gainL::T
    gainR::T
    gain::T
end

struct TreeNode{T<:AbstractFloat, S<:Int, B<:Bool}
    left::S
    right::S
    feat::S
    cond::T
    pred::T
    split::B
end

TreeNode(left::S, right::S, feat::S, cond::T) where {T<:AbstractFloat, S<:Int} = TreeNode{T,S,Bool}(left, right, feat, cond, 0.0, true)
TreeNode(pred::T) where {T<:AbstractFloat} = TreeNode{T,Int,Bool}(0, 0, 0, 0.0, pred, false)

mutable struct EvoTreeRegressor{T<:AbstractFloat, U<:ModelType, S<:Int} <: MLJBase.Deterministic
    loss::U
    nrounds::S
    Î»::T
    Î³::T
    Î·::T
    max_depth::S
    min_weight::T # real minimum number of observations, different from xgboost (but same for linear)
    rowsample::T # subsample
    colsample::T
    nbins::S
    Î±::T
    metric::Symbol
    seed::S
end

function EvoTreeRegressor(;
    loss=:linear,
    nrounds=10,
    Î»=0.0, #
    Î³=0.0, # gamma: min gain to split
    Î·=0.1, # eta: learning rate
    max_depth=5,
    min_weight=1.0, # minimal weight, different from xgboost (but same for linear)
    rowsample=1.0,
    colsample=1.0,
    nbins=64,
    Î±=0.5,
    metric=:mse,
    seed=444)

    if loss == :linear model_type = Linear()
    elseif loss == :logistic model_type = Logistic()
    elseif loss == :poisson model_type = Poisson()
    elseif loss == :L1 model_type = L1()
    elseif loss == :quantile model_type = Quantile()
    end

    model = EvoTreeRegressor(model_type, nrounds, Î», Î³, Î·, max_depth, min_weight, rowsample, colsample, nbins, Î±, metric, seed)
    # message = MLJBase.clean!(model)
    # isempty(message) || @warn message
    return model
end

# For R-package
function EvoTreeRegressorR(
    loss,
    nrounds,
    Î»,
    Î³,
    Î·,
    max_depth,
    min_weight,
    rowsample,
    colsample,
    nbins,
    Î±,
    metric,
    seed)

    if loss == :linear model_type = Linear()
    elseif loss == :logistic model_type = Logistic()
    elseif loss == :poisson model_type = Poisson()
    elseif loss == :L1 model_type = L1()
    elseif loss == :quantile model_type = Quantile()
    end

    model = EvoTreeRegressor(model_type, nrounds, Î», Î³, Î·, max_depth, min_weight, rowsample, colsample, nbins, Î±, metric, seed)
    # message = MLJBase.clean!(model)
    # isempty(message) || @warn message
    return model
end

# single tree is made of a root node that containes nested nodes and leafs
struct TrainNode{T<:AbstractFloat, I<:BitSet, J<:AbstractArray{Int, 1}, S<:Int}
    depth::S
    âˆ‘Î´::T
    âˆ‘Î´Â²::T
    âˆ‘ð‘¤::T
    gain::T
    ð‘–::I
    ð‘—::J
end

# single tree is made of a root node that containes nested nodes and leafs
struct Tree{T<:AbstractFloat, S<:Int}
    nodes::Vector{TreeNode{T,S,Bool}}
end

# eval metric tracking
struct Metric
    iter::Vector{Int}
    metric::Vector{Float64}
end
Metric() = Metric([0], [Inf])

# gradient-boosted tree is formed by a vector of trees
struct GBTree{T<:AbstractFloat, S<:Int}
    trees::Vector{Tree{T,S}}
    params::EvoTreeRegressor
    metric::Metric
end
