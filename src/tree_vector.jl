# initialize train_nodes
function grow_tree(X::AbstractArray{T, 2}, Î´::AbstractArray{Float64, 1}, Î´Â²::AbstractArray{Float64, 1}, params::Params, perm_ini::AbstractArray{Int}, train_nodes::Vector{TrainNode}) where T<:Real

    active_id = [1]
    leaf_count = 1
    tree_depth = 1

    tree = Tree(Vector{TreeNode}())

    splits = Vector{SplitInfo}(undef, size(X, 2))
    for feat in 1:size(X, 2)
        splits[feat] = SplitInfo(-Inf, 0.0, 0.0, 0.0, 0.0, -Inf, -Inf, 0, 0, 0.0)
    end

    tracks = Vector{SplitTrack}(undef, size(X, 2))
    for feat in 1:size(X, 2)
        tracks[feat] = SplitTrack(0.0, 0.0, 0.0, 0.0, -Inf, -Inf, -Inf)
    end

    # grow while there are remaining active nodes
    while size(active_id, 1) > 0 && tree_depth <= params.max_depth
        next_active_id = Int[]

        # grow nodes
        for id in active_id

            node = train_nodes[id]

            if tree_depth == params.max_depth
                push!(tree.nodes, TreeNode(- node.âˆ‘Î´ / (node.âˆ‘Î´Â² + params.Î»)))
            else
                node_size = size(node.ğ‘–, 1)

                @threads for feat in node.ğ‘—
                # for feat in node.ğ‘—
                    sortperm!(view(perm_ini, 1:node_size, feat), view(X, node.ğ‘–, feat), alg = QuickSort, initialized = false)
                    find_split!(view(X, view(node.ğ‘–, view(perm_ini, 1:node_size, feat)), feat), view(Î´, view(node.ğ‘–, view(perm_ini, 1:node_size, feat))) , view(Î´Â², view(node.ğ‘–, view(perm_ini, 1:node_size, feat))), node.âˆ‘Î´, node.âˆ‘Î´Â², params.Î», splits[feat], tracks[feat])
                    # find_split!(view(X, node.ğ‘–[perm_ini[1:node_size, feat]], feat), view(Î´, node.ğ‘–[perm_ini[1:node_size, feat]]) , view(Î´Â², node.ğ‘–[perm_ini[1:node_size, feat]]), node.âˆ‘Î´, node.âˆ‘Î´Â², params.Î», splits[feat], tracks[feat])
                    splits[feat].feat = feat
                end

                # assign best split
                best = get_max_gain(splits)
                # println(best)

                # grow node if best split improve gain
                if best.gain > node.gain + params.Î³

                    # Node: depth, âˆ‘Î´, âˆ‘Î´Â², gain, ğ‘–, ğ‘—
                    # train_nodes[leaf_count + 1] = TrainNode(node.depth + 1, best.âˆ‘Î´L, best.âˆ‘Î´Â²L, best.gainL, view(node.ğ‘–, view(perm_ini, 1:best.ğ‘–, best.feat)), view(node.ğ‘—, :))
                    # train_nodes[leaf_count + 2] = TrainNode(node.depth + 1, best.âˆ‘Î´R, best.âˆ‘Î´Â²R, best.gainR, view(node.ğ‘–, view(perm_ini, best.ğ‘–+1:node_size, best.feat)), view(node.ğ‘—, :))
                    train_nodes[leaf_count + 1] = TrainNode(node.depth + 1, best.âˆ‘Î´L, best.âˆ‘Î´Â²L, best.gainL, node.ğ‘–[perm_ini[1:best.ğ‘–, best.feat]], node.ğ‘—[:])
                    train_nodes[leaf_count + 2] = TrainNode(node.depth + 1, best.âˆ‘Î´R, best.âˆ‘Î´Â²R, best.gainR, node.ğ‘–[perm_ini[best.ğ‘–+1:node_size, best.feat]], node.ğ‘—[:])

                    # push split Node
                    # push!(tree.nodes, SplitNode(leaf_count + 1, leaf_count + 2, best.feat, best.cond))
                    push!(tree.nodes, TreeNode(leaf_count + 1, leaf_count + 2, best.feat, best.cond))

                    push!(next_active_id, leaf_count + 1)
                    push!(next_active_id, leaf_count + 2)

                    leaf_count += 2
                else
                    push!(tree.nodes, TreeNode(- node.âˆ‘Î´ / (node.âˆ‘Î´Â² + params.Î»)))
                end # end of single node split search
            end
            # node.ğ‘– = [0]
        end # end of loop over active ids for a given depth
        active_id = next_active_id
        tree_depth += 1
    end # end of tree growth
    return tree
end


# extract the gain value from the vector of best splits and return the split info associated with best split
function get_max_gain(splits)
    gains = (x -> x.gain).(splits)
    feat = findmax(gains)[2]
    best = splits[feat]
    # best.feat = feat
    return best
end


function grow_gbtree(X::AbstractArray{T, 2}, Y::AbstractArray{<:AbstractFloat, 1}, params::Params; X_eval::AbstractArray{T, 2} = Array{T, 2}(undef, (0,0)), Y_eval::AbstractArray{<:AbstractFloat, 1} = Array{Float64, 1}(undef, 0))  where T<:Real
    Î¼ = mean(Y)
    # pred = ones(size(Y, 1)) .* Î¼
    @fastmath pred = ones(size(Y, 1)) .* Î¼

    Î´, Î´Â² = zeros(Float64, size(Y, 1)), zeros(Float64, size(Y, 1))
    update_grads!(Val{params.loss}(), pred, Y, Î´, Î´Â²)

    # eval init
    if size(Y_eval, 1) > 0
        # pred_eval = ones(size(Y_eval, 1)) .* Î¼
        @fastmath pred_eval = ones(size(Y_eval, 1)) .* Î¼
    end

    âˆ‘Î´, âˆ‘Î´Â² = sum(Î´), sum(Î´Â²)
    gain = get_gain(âˆ‘Î´, âˆ‘Î´Â², params.Î»)

    bias = TreeNode(Î¼)
    bias = Tree([bias])
    gbtree = GBTree([bias], params)

    # sort perm id placeholder
    perm_ini = zeros(Int, size(X))

    X_size = size(X)
    ğ‘–_ = collect(1:X_size[1])
    ğ‘—_ = collect(1:X_size[2])

    train_nodes = Vector{TrainNode}(undef, 2^params.max_depth-1)
    for feat in 1:2^params.max_depth-1
        train_nodes[feat] = TrainNode(0, -Inf, -Inf, -Inf, [0], [0])
    end

    for i in 1:params.nrounds
        # select random rows and cols
        # ğ‘– = view(ğ‘–_, sample(ğ‘–_, floor(Int, params.rowsample * X_size[1]), replace = false))
        # ğ‘— = view(ğ‘—_, sample(ğ‘—_, floor(Int, params.colsample * X_size[2]), replace = false))
        ğ‘– = ğ‘–_[sample(ğ‘–_, floor(Int, params.rowsample * X_size[1]), replace = false)]
        ğ‘— = ğ‘—_[sample(ğ‘—_, floor(Int, params.colsample * X_size[2]), replace = false)]
        # ğ‘– = ğ‘–_
        # ğ‘— = ğ‘—_

        # get gradients
        update_grads!(Val{params.loss}(), pred, Y, Î´, Î´Â²)
        âˆ‘Î´, âˆ‘Î´Â² = sum(Î´), sum(Î´Â²)
        gain = get_gain(âˆ‘Î´, âˆ‘Î´Â², params.Î»)

        # assign a root and grow tree
        train_nodes[1] = TrainNode(1, âˆ‘Î´, âˆ‘Î´Â², gain, ğ‘–, ğ‘—)
        tree = grow_tree(X, Î´, Î´Â², params, perm_ini, train_nodes)

        # get update predictions
        predict!(pred, tree, X)
        # eval predictions
        if size(Y_eval, 1) > 0
            predict!(pred_eval, tree, X_eval)
        end
        # update push tree to model
        push!(gbtree.trees, tree)

        # callback function
        if mod(i, 10) == 0
            if size(Y_eval, 1) > 0
                println("iter:", i, ", train:", sqrt(mean((pred .- Y) .^ 2)), ", eval: ", sqrt(mean((pred_eval .- Y_eval) .^ 2)))
            else
                println("iter:", i, ", train:", sqrt(mean((pred .- Y) .^ 2)))
            end
        end # end of callback

    end #end of nrounds
    return gbtree
end


function find_split!(x::AbstractArray{T, 1}, Î´::AbstractArray{Float64, 1}, Î´Â²::AbstractArray{Float64, 1}, âˆ‘Î´, âˆ‘Î´Â², Î», info::SplitInfo, track::SplitTrack) where T<:Real

    # info.gain = (âˆ‘Î´ ^ 2 / (âˆ‘Î´Â² + Î»)) / 2.0
    @fastmath info.gain = (âˆ‘Î´ ^ 2 / (âˆ‘Î´Â² + Î»)) / 2.0

    track.âˆ‘Î´L = 0.0
    track.âˆ‘Î´Â²L = 0.0
    track.âˆ‘Î´R = âˆ‘Î´
    track.âˆ‘Î´Â²R = âˆ‘Î´Â²

    @fastmath @inbounds for i in 1:(size(x, 1) - 1)
    # @inbounds for i in 1:(size(x, 1) - 1)

        track.âˆ‘Î´L += Î´[i]
        track.âˆ‘Î´Â²L += Î´Â²[i]
        track.âˆ‘Î´R -= Î´[i]
        track.âˆ‘Î´Â²R -= Î´Â²[i]

        @fastmath @inbounds if x[i] < x[i+1] # check gain only if there's a change in value
        # @inbounds if x[i] < x[i+1] # check gain only if there's a change in value

            update_track!(track, Î»)
            if track.gain > info.gain
                info.gain = track.gain
                info.gainL = track.gainL
                info.gainR = track.gainR
                info.âˆ‘Î´L = track.âˆ‘Î´L
                info.âˆ‘Î´Â²L = track.âˆ‘Î´Â²L
                info.âˆ‘Î´R = track.âˆ‘Î´R
                info.âˆ‘Î´Â²R = track.âˆ‘Î´Â²R
                info.cond = x[i]
                info.ğ‘– = i
            end
        end
    end
end
