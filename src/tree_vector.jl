# initialize train_nodes
function grow_tree(X::AbstractArray{R, 2}, Œ¥::AbstractArray{T, 1}, Œ¥¬≤::AbstractArray{T, 1}, ùë§::AbstractArray{T, 1}, params::EvoTreeRegressor, perm_ini::AbstractArray{Int}, train_nodes::Vector{TrainNode{T, I, J, S}}, splits::Vector{SplitInfo{Float64, Int}}, tracks::Vector{SplitTrack{Float64}}, X_edges) where {R<:Real, T<:AbstractFloat, I<:AbstractArray{Int, 1}, J<:AbstractArray{Int, 1}, S<:Int}

    active_id = ones(Int, 1)
    leaf_count = 1::Int
    tree_depth = 1::Int
    tree = Tree(Vector{TreeNode{Float64, Int, Bool}}())

    # grow while there are remaining active nodes
    while size(active_id, 1) > 0 && tree_depth <= params.max_depth
        next_active_id = ones(Int, 0)
        # grow nodes
        for id in active_id
            node = train_nodes[id]
            if tree_depth == params.max_depth || node.‚àëùë§ <= params.min_weight
                push!(tree.nodes, TreeNode(pred_leaf(params.loss, node, params, view(Œ¥¬≤, node.ùëñ))))
            else
                node_size = size(node.ùëñ, 1)
                @threads for feat in node.ùëó
                    sortperm!(view(perm_ini, 1:node_size, feat), view(X, node.ùëñ, feat), alg = QuickSort, initialized = false)
                    find_split_bitset!(view(X, view(node.ùëñ, view(perm_ini, 1:node_size, feat)), feat), view(Œ¥, view(node.ùëñ, view(perm_ini, 1:node_size, feat))) , view(Œ¥¬≤, view(node.ùëñ, view(perm_ini, 1:node_size, feat))), view(ùë§, view(node.ùëñ, view(perm_ini, 1:node_size, feat))), node.‚àëŒ¥, node.‚àëŒ¥¬≤, node.‚àëùë§, params, splits[feat], tracks[feat], X_edges[feat])
                end
                # assign best split
                best = get_max_gain(splits)
                # grow node if best split improve gain
                if best.gain > node.gain + params.Œ≥
                    train_nodes[leaf_count + 1] = TrainNode(node.depth + 1, best.‚àëŒ¥L, best.‚àëŒ¥¬≤L, best.‚àëùë§L, best.gainL, node.ùëñ[perm_ini[1:best.ùëñ, best.feat]], node.ùëó)
                    train_nodes[leaf_count + 2] = TrainNode(node.depth + 1, best.‚àëŒ¥R, best.‚àëŒ¥¬≤R, best.‚àëùë§R, best.gainR, node.ùëñ[perm_ini[best.ùëñ+1:node_size, best.feat]], node.ùëó)
                    push!(tree.nodes, TreeNode(leaf_count + 1, leaf_count + 2, best.feat, best.cond))
                    push!(next_active_id, leaf_count + 1)
                    push!(next_active_id, leaf_count + 2)
                    leaf_count += 2
                else
                    push!(tree.nodes, TreeNode(pred_leaf(params.loss, node, params, view(Œ¥¬≤, node.ùëñ))))
                end # end of single node split search
            end
        end # end of loop over active ids for a given depth
        active_id = next_active_id
        tree_depth += 1
    end # end of tree growth
    return tree
end

# extract the gain value from the vector of best splits and return the split info associated with best split
function get_max_gain(splits::Vector{SplitInfo{Float64,Int}})
    gains = (x -> x.gain).(splits)
    feat = findmax(gains)[2]
    best = splits[feat]
    # best.feat = feat
    return best
end

function get_edges(X, nbins=250)
    edges = Vector{Vector}(undef, size(X,2))
    @threads for i in 1:size(X, 2)
        edges[i] = unique(quantile(view(X, :,i), (0:nbins)/nbins))[2:(end-1)]
        if length(edges[i]) == 0
            edges[i] = [minimum(view(X, :,i))]
        end
    end
    return edges
end

function binarize(X, edges)
    X_bin = zeros(UInt8, size(X))
    @threads for i in 1:size(X, 2)
        X_bin[:,i] = searchsortedlast.(Ref(edges[i]), view(X,:,i)) .+ 1
    end
    X_bin
end

# grow_gbtree
function grow_gbtree(X::AbstractArray{R, 2}, Y::AbstractArray{T, 1}, params::EvoTreeRegressor;
    X_eval::AbstractArray{R, 2} = Array{R, 2}(undef, (0,0)), Y_eval::AbstractArray{T, 1} = Array{Float64, 1}(undef, 0),
    early_stopping_rounds=Int(1e5), print_every_n=100, verbosity=1) where {R<:Real, T<:AbstractFloat}

    X_edges = get_edges(X, params.nbins)
    X_bin = binarize(X, X_edges)

    Œº = mean(Y)
    if typeof(params.loss) == Logistic
        Œº = logit(Œº)
    elseif params.loss == Poisson
        Œº = log(Œº)
    end
    pred = ones(size(Y, 1)) .* Œº

    # initialize gradients and weights
    Œ¥, Œ¥¬≤ = zeros(Float64, size(Y, 1)), zeros(Float64, size(Y, 1))
    ùë§ = ones(Float64, size(Y, 1))

    # eval init
    if size(Y_eval, 1) > 0
        pred_eval = ones(size(Y_eval, 1)) .* Œº
    end

    bias = Tree([TreeNode(Œº)])
    gbtree = GBTree([bias], params, Metric())

    # sort perm id placeholder
    perm_ini = zeros(Int, size(X_bin))

    X_size = size(X)
    ùëñ_ = collect(1:X_size[1])
    ùëó_ = collect(1:X_size[2])

    # initialize train nodes
    train_nodes = Vector{TrainNode{Float64, Array{Int64,1}, Array{Int64, 1}, Int64}}(undef, 2^params.max_depth-1)
    for feat in 1:2^params.max_depth-1
        train_nodes[feat] = TrainNode(0, -Inf, -Inf, -Inf, -Inf, [0], [0])
    end

    # initialize metric
    if params.metric != :none
        metric_track = Metric()
        metric_best = Metric()
        iter_since_best = 0
    end

    # loop over nrounds
    for i in 1:params.nrounds
        # select random rows and cols
        ùëñ = ùëñ_[sample(ùëñ_, ceil(Int, params.rowsample * X_size[1]), replace = false)]
        ùëó = ùëó_[sample(ùëó_, ceil(Int, params.colsample * X_size[2]), replace = false)]

        # get gradients
        update_grads!(params.loss, params.Œ±, pred, Y, Œ¥, Œ¥¬≤, ùë§)
        ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§ = sum(Œ¥[ùëñ]), sum(Œ¥¬≤[ùëñ]), sum(ùë§[ùëñ])
        gain = get_gain(params.loss, ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§, params.Œª)

        # initializde node splits info and tracks - colsample size (ùëó)
        splits = Vector{SplitInfo{Float64, Int64}}(undef, X_size[2])
        for feat in ùëó_
            splits[feat] = SplitInfo{Float64, Int64}(-Inf, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -Inf, -Inf, 0, feat, 0.0)
        end
        tracks = Vector{SplitTrack{Float64}}(undef, X_size[2])
        for feat in ùëó_
            tracks[feat] = SplitTrack{Float64}(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -Inf, -Inf, -Inf)
        end

        # assign a root and grow tree
        train_nodes[1] = TrainNode(1, ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§, gain, ùëñ, ùëó)
        tree = grow_tree(X_bin, Œ¥, Œ¥¬≤, ùë§, params, perm_ini, train_nodes, splits, tracks, X_edges)
        # update push tree to model
        push!(gbtree.trees, tree)

        # get update predictions
        predict!(pred, tree, X)
        # eval predictions
        if size(Y_eval, 1) > 0
            predict!(pred_eval, tree, X_eval)
        end

        # callback function
        if params.metric != :none

            if size(Y_eval, 1) > 0
                metric_track.metric .= eval_metric(Val{params.metric}(), pred_eval, Y_eval, params.Œ±)
            else
                metric_track.metric .= eval_metric(Val{params.metric}(), pred, Y, params.Œ±)
            end

            if metric_track.metric < metric_best.metric
                metric_best.metric .=  metric_track.metric
                metric_best.iter .=  i
            else
                iter_since_best += 1
            end

            if mod(i, print_every_n) == 0 && verbosity > 0
                display(string("iter:", i, ", eval: ", metric_track.metric))
            end
            iter_since_best >= early_stopping_rounds ? break : nothing
        end
    end #end of nrounds

    if params.metric != :none
        gbtree.metric.iter .= metric_best.iter
        gbtree.metric.metric .= metric_best.metric
    end
    return gbtree
end

# grow_gbtree
function grow_gbtree!(model::GBTree, X::AbstractArray{R, 2}, Y::AbstractArray{T, 1};
    X_eval::AbstractArray{R, 2} = Array{R, 2}(undef, (0,0)), Y_eval::AbstractArray{T, 1} = Array{Float64, 1}(undef, 0),
    early_stopping_rounds=Int(1e5), print_every_n=100, verbosity=1) where {R<:Real, T<:AbstractFloat}

    params = model.params

    X_edges = get_edges(X, params.nbins)
    X_bin = binarize(X, X_edges)

    # initialize gradients and weights
    Œ¥, Œ¥¬≤ = zeros(Float64, size(Y, 1)), zeros(Float64, size(Y, 1))
    ùë§ = ones(Float64, size(Y, 1))

    pred = predict(model, X)
    # eval init
    if size(Y_eval, 1) > 0
        pred_eval = predict(model, X_eval)
    end

    # sort perm id placeholder
    perm_ini = zeros(Int, size(X_bin))

    X_size = size(X)
    ùëñ_ = collect(1:X_size[1])
    ùëó_ = collect(1:X_size[2])

    # initialize train nodes
    train_nodes = Vector{TrainNode{Float64, Array{Int64,1}, Array{Int64, 1}, Int64}}(undef, 2^params.max_depth-1)
    for feat in 1:2^params.max_depth-1
        train_nodes[feat] = TrainNode(0, -Inf, -Inf, -Inf, -Inf, [0], [0])
    end

    # initialize metric
    if params.metric != :none
        metric_track = model.metric
        metric_best = model.metric
        iter_since_best = 0
    end

    # loop over nrounds
    for i in 1:params.nrounds
        # select random rows and cols
        ùëñ = ùëñ_[sample(ùëñ_, ceil(Int, params.rowsample * X_size[1]), replace = false)]
        ùëó = ùëó_[sample(ùëó_, ceil(Int, params.colsample * X_size[2]), replace = false)]

        # get gradients
        update_grads!(params.loss, params.Œ±, pred, Y, Œ¥, Œ¥¬≤, ùë§)
        ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§ = sum(Œ¥[ùëñ]), sum(Œ¥¬≤[ùëñ]), sum(ùë§[ùëñ])
        gain = get_gain(params.loss, ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§, params.Œª)

        # initializde node splits info and tracks - colsample size (ùëó)
        splits = Vector{SplitInfo{Float64, Int64}}(undef, X_size[2])
        for feat in ùëó_
            splits[feat] = SplitInfo{Float64, Int64}(-Inf, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -Inf, -Inf, 0, feat, 0.0)
        end
        tracks = Vector{SplitTrack{Float64}}(undef, X_size[2])
        for feat in ùëó_
            tracks[feat] = SplitTrack{Float64}(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -Inf, -Inf, -Inf)
        end

        # assign a root and grow tree
        train_nodes[1] = TrainNode(1, ‚àëŒ¥, ‚àëŒ¥¬≤, ‚àëùë§, gain, ùëñ, ùëó)
        tree = grow_tree(X_bin, Œ¥, Œ¥¬≤, ùë§, params, perm_ini, train_nodes, splits, tracks, X_edges)
        # update push tree to model
        push!(model.trees, tree)

        # get update predictions
        predict!(pred, tree, X)
        # eval predictions
        if size(Y_eval, 1) > 0
            predict!(pred_eval, tree, X_eval)
        end

        # callback function
        if params.metric != :none

            if size(Y_eval, 1) > 0
                metric_track.metric .= eval_metric(Val{params.metric}(), pred_eval, Y_eval, params.Œ±)
            else
                metric_track.metric .= eval_metric(Val{params.metric}(), pred, Y, params.Œ±)
            end

            if metric_track.metric < metric_best.metric
                metric_best.metric .=  metric_track.metric
                metric_best.iter .=  i
            else
                iter_since_best += 1
            end

            if mod(i, print_every_n) == 0 && verbosity > 0
                display(string("iter:", i, ", eval: ", metric_track.metric))
            end
            iter_since_best >= early_stopping_rounds ? break : nothing
        end
    end #end of nrounds

    if params.metric != :none
        model.metric.iter .= metric_best.iter
        model.metric.metric .= metric_best.metric
    end
    return model
end
