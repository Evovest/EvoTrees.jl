var documenterSearchIndex = {"docs":
[{"location":"api/#fit_evotree","page":"API","title":"fit_evotree","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"fit_evotree","category":"page"},{"location":"api/#EvoTrees.fit_evotree","page":"API","title":"EvoTrees.fit_evotree","text":"fit_evotree(params;\n    x_train, y_train, w_train=nothing, offset_train=nothing;\n    x_eval=nothing, y_eval=nothing, w_eval=nothing, offset_eval=nothing,\n    early_stopping_rounds=9999,\n    print_every_n=9999,\n    verbosity=1)\n\nMain training function. Performs model fitting given configuration params, x_train, y_train input data. \n\nArguments\n\nparams::EvoTypes: configuration info providing hyper-paramters. EvoTypes comprises EvoTreeRegressor, EvoTreeClassifier, EvoTreeCount and EvoTreeMLE.\n\nKeyword arguments\n\nx_train::Matrix: training data of size [#observations, #features]. \ny_train::Vector: vector of train targets of length #observations.\nw_train::Vector: vector of train weights of length #observations. If nothing, a vector of ones is assumed.\noffset_train::VecOrMat: offset for the training data. Should match the size of the predictions.\nx_eval::Matrix: evaluation data of size [#observations, #features]. \ny_eval::Vector: vector of evaluation targets of length #observations.\nw_eval::Vector: vector of evaluation weights of length #observations. Defaults to nothing (assumes a vector of 1s).\noffset_eval::VecOrMat: evaluation data offset. Should match the size of the predictions.\nmetric: The evaluation metric that wil be tracked on x_eval, y_eval and optionally w_eval / offset_eval data.    Supported metrics are: \n:mse: mean-squared error. Adapted for general regression models.\n:rmse: root-mean-squared error (CPU only). Adapted for general regression models.\n:mae: mean absolute error. Adapted for general regression models.\n:logloss: Adapted for :logistic regression models.\n:mlogloss: Multi-class cross entropy. Adapted to EvoTreeClassifier classification models. \n:poisson: Poisson deviance. Adapted to EvoTreeCount count models.\n:gamma: Gamma deviance. Adapted to regression problem on Gamma like, positively distributed targets.\n:tweedie: Tweedie deviance. Adapted to regression problem on Tweedie like, positively distributed targets with probability mass at y == 0.\n:gaussian_mle: Gaussian log-likelihood. Adapted to MLE when using EvoTreeMLE with loss = :gaussian_mle. \n:logistic_mle: Logistic log-likelihood. Adapted to MLE when using EvoTreeMLE with loss = :logistic_mle. \nearly_stopping_rounds::Integer: number of consecutive rounds without metric improvement after which fitting in stopped. \nprint_every_n: sets at which frequency logging info should be printed. \nverbosity: set to 1 to print logging info during training.\nfnames: the names of the x_train features. If provided, should be a vector of string with length(fnames) = size(x_train, 2).\nreturn_logger::Bool = false: if set to true (default), fit_evotree return a tuple (m, logger) where logger is a dict containing various tracking information.\n\n\n\n\n\n","category":"function"},{"location":"api/#Predict","page":"API","title":"Predict","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"EvoTrees.predict","category":"page"},{"location":"api/#MLJModelInterface.predict","page":"API","title":"MLJModelInterface.predict","text":"predict(tree::Tree{L,K,T}, X::AbstractMatrix)\n\nPrediction from a single tree - assign each observation to its final leaf.\n\n\n\n\n\npredict(model::EvoTree, X::AbstractMatrix; ntree_limit = length(model.trees))\n\nPredictions from an EvoTree model - sums the predictions from all trees composing the model. Use ntree_limit=N to only predict with the first N trees.\n\n\n\n\n\n","category":"function"},{"location":"api/#Features-Importance","page":"API","title":"Features Importance","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"importance","category":"page"},{"location":"api/#EvoTrees.importance","page":"API","title":"EvoTrees.importance","text":"importance(model::Union{EvoTree,EvoTreeGPU})\n\nSorted normalized feature importance based on loss function gain. Feature names associated to the model are stored in model.info[:fnames] as a string Vector and can be updated at any time. Eg: model.info[:fnames] = new_fnames_vec.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/logistic-regression-titanic/#Logistic-Regression-on-Titanic-Dataset","page":"Logistic Regression - Titanic","title":"Logistic Regression on Titanic Dataset","text":"","category":"section"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"We will use the Titanic dataset, which is included in the MLDatasets package. It describes the survival status of individual passengers on the Titanic. The model will be approached as a logistic regression problem, although a Classifier model could also have been used (see the Classification - Iris tutorial). ","category":"page"},{"location":"tutorials/logistic-regression-titanic/#Getting-started","page":"Logistic Regression - Titanic","title":"Getting started","text":"","category":"section"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"To begin, we will load the required packages and the dataset:","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"using EvoTrees\nusing MLDatasets\nusing DataFrames\nusing Statistics: mean\nusing CategoricalArrays\nusing Random\n\ndf = MLDatasets.Titanic().dataframe","category":"page"},{"location":"tutorials/logistic-regression-titanic/#Preprocessing","page":"Logistic Regression - Titanic","title":"Preprocessing","text":"","category":"section"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"Before we can train our model, we need to preprocess the dataset. We will split our data according to train and eval indices, and separate features from the target variable.","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"Random.seed!(123)\n\ntrain_ratio = 0.8\ntrain_indices = randperm(nrow(df))[1:Int(round(train_ratio * nrow(df)))]\n\n# remove unneeded variables\ndf = df[:, Not([:PassengerId, :Name, :Embarked, :Cabin, :Ticket])]\n\n# treat string feature and missing values\ntransform!(df, :Sex => ByRow(x -> x == \"male\" ? 0 : 1) => :Sex)\ntransform!(df, :Age => ByRow(x -> ismissing(x) ? mean(skipmissing(df.Age)) : x) => :Age)\n\ntrain_data = df[train_indices, :]\neval_data = df[setdiff(1:nrow(df), train_indices), :]\n\nx_train, y_train = Matrix(train_data[:, Not(:Survived)]), train_data[:, :Survived]\nx_eval, y_eval = Matrix(eval_data[:, Not(:Survived)]), eval_data[:, :Survived]","category":"page"},{"location":"tutorials/logistic-regression-titanic/#Training","page":"Logistic Regression - Titanic","title":"Training","text":"","category":"section"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"Now we are ready to train our model. We will first define a model configuration using the EvoTreeClassifier model constructor.  Then, we'll use fit_evotree to train a boosted tree model. We'll pass optional x_eval and y_eval arguments, which enable the usage of early stopping. ","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"config = EvoTreeRegressor(loss = :logistic, nrounds=200, eta=0.1, max_depth=5, rowsample = 0.6, colsample = 0.9)\nmodel = fit_evotree(config;\n    x_train, y_train,\n    x_eval, y_eval,\n    metric = :logloss,\n    early_stopping_rounds=10,\n    print_every_n=10)","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"Finally, we can get predictions by passing training and testing data to our model. We can then evaluate the accuracy of our model, which should be near 100% for this simple classification problem. ","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"pred_train = model(x_train)\npred_eval = model(x_eval)","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"julia> mean((pred_train .> 0.5) .== y_train)\n0.8625525946704067\n\njulia> mean((pred_eval .> 0.5) .== y_eval)\n0.8258426966292135","category":"page"},{"location":"tutorials/regression-boston/#Regression-on-Boston-Housing-Dataset","page":"Regression - Boston","title":"Regression on Boston Housing Dataset","text":"","category":"section"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"We will use the Boston Housing dataset, which is included in the MLDatasets package. It's derived from information collected by the U.S. Census Service concerning housing in the area of Boston. Target variable represents the median housing value.","category":"page"},{"location":"tutorials/regression-boston/#Getting-started","page":"Regression - Boston","title":"Getting started","text":"","category":"section"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"To begin, we will load the required packages and the dataset:","category":"page"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"using EvoTrees\nusing MLDatasets\nusing DataFrames\nusing Statistics: mean\nusing CategoricalArrays\nusing Random\n\ndf = MLDatasets.BostonHousing().dataframe","category":"page"},{"location":"tutorials/regression-boston/#Preprocessing","page":"Regression - Boston","title":"Preprocessing","text":"","category":"section"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"Before we can train our model, we need to preprocess the dataset. We will split our data according to train and eval indices, and separate features from the target variable.","category":"page"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"Random.seed!(123)\n\ntrain_ratio = 0.8\ntrain_indices = randperm(nrow(df))[1:Int(round(train_ratio * nrow(df)))]\n\ntrain_data = df[train_indices, :]\neval_data = df[setdiff(1:nrow(df), train_indices), :]\n\nx_train, y_train = Matrix(train_data[:, Not(:MEDV)]), train_data[:, :MEDV]\nx_eval, y_eval = Matrix(eval_data[:, Not(:MEDV)]), eval_data[:, :MEDV]","category":"page"},{"location":"tutorials/regression-boston/#Training","page":"Regression - Boston","title":"Training","text":"","category":"section"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"Now we are ready to train our model. We will first define a model configuration using the EvoTreeRegressor model constructor.  Then, we'll use fit_evotree to train a boosted tree model. We'll pass optional x_eval and y_eval arguments, which enable the usage of early stopping. ","category":"page"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"config = EvoTreeRegressor(nrounds=200, eta=0.1, max_depth=4, lambda=0.1, rowsample = 0.9, colsample = 0.9)\nmodel = fit_evotree(config;\n    x_train, y_train,\n    x_eval, y_eval,\n    metric = :mse,\n    early_stopping_rounds=10,\n    print_every_n=10)","category":"page"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"Finally, we can get predictions by passing training and testing data to our model. We can then apply various evaluation metric, such as the MAE (mean absolute error):  ","category":"page"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"pred_train = model(x_train)\npred_eval = model(x_eval)","category":"page"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"julia> mean(abs.(pred_train .- y_train))\n1.121228068080949\n\njulia> mean(abs.(pred_eval .- y_eval))\n2.399116769167456","category":"page"},{"location":"models/#EvoTreeRegressor","page":"Models","title":"EvoTreeRegressor","text":"","category":"section"},{"location":"models/","page":"Models","title":"Models","text":"EvoTreeRegressor","category":"page"},{"location":"models/#EvoTrees.EvoTreeRegressor","page":"Models","title":"EvoTrees.EvoTreeRegressor","text":"EvoTreeRegressor(;kwargs...)\n\nA model type for constructing a EvoTreeRegressor, based on EvoTrees.jl, and implementing both an internal API and the MLJ model interface.\n\nHyper-parameters\n\nloss=:linear:         Loss to be be minimized during training. One of:\n:linear\n:logistic\n:gamma\n:tweedie\n:quantile\n:L1\nnrounds=10:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions.  A lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.   \nlambda::T=0.0:        L2 regularization term on weights. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:         Minimum gain improvement needed to perform a node split. Higher gamma can result in a more robust model.\nalpha::T=0.5:         Loss specific parameter in the [0, 1] range:                           - :quantile: target quantile for the regression.                           - :L1: weighting parameters to positive vs negative residuals.                                 - Positive residual weights = alpha                                 - Negative residual weights = (1 - alpha)\nmax_depth=5:          Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=0.0:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector.\nrowsample=1.0:        Proportion of rows that are sampled at each iteration to build the tree. Should be in ]0, 1].\ncolsample=1.0:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be in ]0, 1].\nnbins=32:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins.\nmonotone_constraints=Dict{Int, Int}(): Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  Only :linear, :logistic, :gamma and tweedie losses are supported at the moment.\nT=Float32:            The float precision on which the model will be trained. One of Float32 or Float64.\nrng=123:              Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=\"cpu\":         Hardware device to use for computations. Can be either \"cpu\" or \"gpu\". Only :linear, :logistic, :gamma and tweedie losses are supported on GPU.\n\nInternal API\n\nDo config = EvoTreeRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeRegressor(loss=...).\n\nTraining model\n\nA model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n\nInference\n\nPredictions are obtained using predict which returns a Matrix of size [nobs, 1]:\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)\n\nMLJ Interface\n\nFrom MLJ, the type can be imported using:\n\nEvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\n\nDo model = EvoTreeRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeRegressor(loss=...).\n\nTraining model\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y) where\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are deterministic.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.\n\nReport\n\nThe fields of report(mach) are:\n\n:features: The names of the features encountered in training.\n\nExamples\n\n# Internal API\nusing EvoTrees\nconfig = EvoTreeRegressor(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\n# MLJ Interface\nusing MLJ\nEvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\nmodel = EvoTreeRegressor(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_boston\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"models/#EvoTreeClassifier","page":"Models","title":"EvoTreeClassifier","text":"","category":"section"},{"location":"models/","page":"Models","title":"Models","text":"EvoTreeClassifier","category":"page"},{"location":"models/#EvoTrees.EvoTreeClassifier","page":"Models","title":"EvoTrees.EvoTreeClassifier","text":"EvoTreeClassifier(;kwargs...)\n\nA model type for constructing a EvoTreeClassifier, based on EvoTrees.jl, and implementing both an internal API and the MLJ model interface. EvoTreeClassifier is used to perform multi-class classification, using cross-entropy loss.\n\nHyper-parameters\n\nnrounds=10:                 Number of rounds. It corresponds to the number of trees that will be sequentially stacked.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions.  A lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.  \nlambda::T=0.0:              L2 regularization term on weights. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:               Minimum gain improvement needed to perform a node split. Higher gamma can result in a more robust model.\nmax_depth=5:                Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=0.0:             Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector.\nrowsample=1.0:              Proportion of rows that are sampled at each iteration to build the tree. Should be in ]0, 1].\ncolsample=1.0:              Proportion of columns / features that are sampled at each iteration to build the tree. Should be in ]0, 1].\nnbins=32:                   Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins.\nT=Float32:                  The float precision on which the model will be trained. One of Float32 or Float64.\nrng=123:                    Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=\"cpu\":               Hardware device to use for computations. Can be either \"cpu\" or \"gpu\".\n\nInternal API\n\nDo config = EvoTreeClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeClassifier(max_depth=...).\n\nTraining model\n\nA model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n\nInference\n\nPredictions are obtained using predict which returns a Matrix of size [nobs, K] where K is the number of classes:\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)\n\nMLJ\n\nFrom MLJ, the type can be imported using:\n\nEvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees\n\nDo model = EvoTreeClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeClassifier(loss=...).\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Multiclas or <:OrderedFactor; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic.\npredict_mode(mach, Xnew): returns the mode of each of the prediction above.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.\n\nReport\n\nThe fields of report(mach) are:\n\n:features: The names of the features encountered in training.\n\nExamples\n\n# Internal API\nusing EvoTrees\nconfig = EvoTreeClassifier(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(1:3, nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\n# MLJ Interface\nusing MLJ\nEvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees\nmodel = EvoTreeClassifier(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_iris\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mode(mach, X)\n\nSee also EvoTrees.jl.\n\n\n\n\n\n","category":"type"},{"location":"models/#EvoTreeCount","page":"Models","title":"EvoTreeCount","text":"","category":"section"},{"location":"models/","page":"Models","title":"Models","text":"EvoTreeCount","category":"page"},{"location":"models/#EvoTrees.EvoTreeCount","page":"Models","title":"EvoTrees.EvoTreeCount","text":"EvoTreeCount(;kwargs...)\n\nA model type for constructing a EvoTreeCount, based on EvoTrees.jl, and implementing both an internal API the MLJ model interface. EvoTreeCount is used to perform Poisson probabilistic regression on count target.\n\nHyper-parameters\n\nnrounds=10:                 Number of rounds. It corresponds to the number of trees that will be sequentially stacked.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions.  A lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.  \nlambda::T=0.0:              L2 regularization term on weights. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:               Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model.\nmax_depth=5:                Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=0.0:             Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector.\nrowsample=1.0:              Proportion of rows that are sampled at each iteration to build the tree. Should be ]0, 1].\ncolsample=1.0:              Proportion of columns / features that are sampled at each iteration to build the tree. Should be ]0, 1].\nnbins=32:                   Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins.\nmonotone_constraints=Dict{Int, Int}(): Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).\nT=Float32:                  The float precision on which the model will be trained. One of Float32 or Float64.\nrng=123:                    Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=\"cpu\":               Hardware device to use for computations. Can be either \"cpu\" or \"gpu\".\n\nInternal API\n\nDo config = EvoTreeCount() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeCount(max_depth=...).\n\nTraining model\n\nA model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n\nInference\n\nPredictions are obtained using predict which returns a Matrix of size [nobs, 1]:\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)\n\nMLJ\n\nFrom MLJ, the type can be imported using:\n\nEvoTreeCount = @load EvoTreeCount pkg=EvoTrees\n\nDo model = EvoTreeCount() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeCount(loss=...).\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y) where\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Count; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nOperations\n\npredict(mach, Xnew): returns a vector of Poisson distributions given features Xnew having the same scitype as X above. Predictions are probabilistic.\n\nSpecific metrics can also be predicted using:\n\npredict_mean(mach, Xnew)\npredict_mode(mach, Xnew)\npredict_median(mach, Xnew)\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.\n\nReport\n\nThe fields of report(mach) are:\n\n:features: The names of the features encountered in training.\n\nExamples\n\n# Internal API\nusing EvoTrees\nconfig = EvoTreeCount(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(0:2, nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\nusing MLJ\nEvoTreeCount = @load EvoTreeCount pkg=EvoTrees\nmodel = EvoTreeCount(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nX, y = randn(nobs, nfeats), rand(0:2, nobs)\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mean(mach, X)\npreds = predict_mode(mach, X)\npreds = predict_median(mach, X)\n\n\nSee also EvoTrees.jl.\n\n\n\n\n\n","category":"type"},{"location":"models/#EvoTreeMLE","page":"Models","title":"EvoTreeMLE","text":"","category":"section"},{"location":"models/","page":"Models","title":"Models","text":"EvoTreeMLE","category":"page"},{"location":"models/#EvoTrees.EvoTreeMLE","page":"Models","title":"EvoTrees.EvoTreeMLE","text":"EvoTreeMLE(;kwargs...)\n\nA model type for constructing a EvoTreeMLE, based on EvoTrees.jl, and implementing both an internal API the MLJ model interface. EvoTreeMLE performs maximum likelihood estimation. Assumed distribution is specified through loss kwargs. Both Gaussian and Logistic distributions are supported.\n\nHyper-parameters\n\nloss=:gaussian:         Loss to be be minimized during training. One of:\n\n:gaussian / :gaussian_mle\n:logistic / :logistic_mle\nnrounds=10:                 Number of rounds. It corresponds to the number of trees that will be sequentially stacked.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions. \n\nA lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.  \n\nlambda::T=0.0:              L2 regularization term on weights. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:               Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model.\nmax_depth=5:                Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=0.0:             Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector.\nrowsample=1.0:              Proportion of rows that are sampled at each iteration to build the tree. Should be in ]0, 1].\ncolsample=1.0:              Proportion of columns / features that are sampled at each iteration to build the tree. Should be in ]0, 1].\nnbins=32:                   Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins.\nmonotone_constraints=Dict{Int, Int}(): Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  !Experimental feature: note that for MLE regression, constraints may not be enforced systematically.\nT=Float64:                  The float precision on which the model will be trained. One of Float32 or Float64.\nrng=123:                    Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=\"cpu\":               Hardware device to use for computations. Can be either \"cpu\" or \"gpu\".\n\nInternal API\n\nDo config = EvoTreeMLE() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeMLE(max_depth=...).\n\nTraining model\n\nA model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n\nInference\n\nPredictions are obtained using predict which returns a Matrix of size [nobs, nparams] where the second dimensions refer to μ & σ for Normal/Gaussian and μ & s for Logistic.\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)\n\nMLJ\n\nFrom MLJ, the type can be imported using:\n\nEvoTreeMLE = @load EvoTreeMLE pkg=EvoTrees\n\nDo model = EvoTreeMLE() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeMLE(loss=...).\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nOperations\n\npredict(mach, Xnew): returns a vector of Gaussian or Logistic distributions (according to provided loss) given features Xnew having the same scitype as X above.\n\nPredictions are probabilistic.\n\nSpecific metrics can also be predicted using:\n\npredict_mean(mach, Xnew)\npredict_mode(mach, Xnew)\npredict_median(mach, Xnew)\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.\n\nReport\n\nThe fields of report(mach) are:\n\n:features: The names of the features encountered in training.\n\nExamples\n\n# Internal API\nusing EvoTrees\nconfig = EvoTreeMLE(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\n# MLJ Interface\nusing MLJ\nEvoTreeMLE = @load EvoTreeMLE pkg=EvoTrees\nmodel = EvoTreeMLE(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_boston\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mean(mach, X)\npreds = predict_mode(mach, X)\npreds = predict_median(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"models/#EvoTreeGaussian","page":"Models","title":"EvoTreeGaussian","text":"","category":"section"},{"location":"models/","page":"Models","title":"Models","text":"EvoTreeGaussian is to be deprecated. Please use EvoTreeMLE with loss = :gaussian_mle. ","category":"page"},{"location":"models/","page":"Models","title":"Models","text":"EvoTreeGaussian","category":"page"},{"location":"models/#EvoTrees.EvoTreeGaussian","page":"Models","title":"EvoTrees.EvoTreeGaussian","text":"EvoTreeGaussian(;kwargs...)\n\nA model type for constructing a EvoTreeGaussian, based on EvoTrees.jl, and implementing both an internal API the MLJ model interface. EvoTreeGaussian is used to perform Gaussian probabilistic regression, fitting μ and σ parameters to maximize likelihood.\n\nHyper-parameters\n\nnrounds=10:                 Number of rounds. It corresponds to the number of trees that will be sequentially stacked.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions.  A lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.  \nlambda::T=0.0:              L2 regularization term on weights. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:               Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model.\nmax_depth=5:                Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=0.0:             Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector.\nrowsample=1.0:              Proportion of rows that are sampled at each iteration to build the tree. Should be in ]0, 1].\ncolsample=1.0:              Proportion of columns / features that are sampled at each iteration to build the tree. Should be in ]0, 1].\nnbins=32:                   Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins.\nmonotone_constraints=Dict{Int, Int}(): Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  !Experimental feature: note that for Gaussian regression, constraints may not be enforce systematically.\nT=Float64:                  The float precision on which the model will be trained. One of Float32 or Float64.\nrng=123:                    Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=\"cpu\":               Hardware device to use for computations. Can be either \"cpu\" or \"gpu\".\n\nInternal API\n\nDo config = EvoTreeGaussian() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeGaussian(max_depth=...).\n\nTraining model\n\nA model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n\nInference\n\nPredictions are obtained using predict which returns a Matrix of size [nobs, 2] where the second dimensions refer to μ and σ respectively:\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)\n\nMLJ\n\nFrom MLJ, the type can be imported using:\n\nEvoTreeGaussian = @load EvoTreeGaussian pkg=EvoTrees\n\nDo model = EvoTreeGaussian() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeGaussian(loss=...).\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nOperations\n\npredict(mach, Xnew): returns a vector of Gaussian distributions given features Xnew having the same scitype as X above.\n\nPredictions are probabilistic.\n\nSpecific metrics can also be predicted using:\n\npredict_mean(mach, Xnew)\npredict_mode(mach, Xnew)\npredict_median(mach, Xnew)\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.\n\nReport\n\nThe fields of report(mach) are:\n\n:features: The names of the features encountered in training.\n\nExamples\n\n# Internal API\nusing EvoTrees\nparams = EvoTreeGaussian(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(nobs)\nmodel = fit_evotree(params; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\n# MLJ Interface\nusing MLJ\nEvoTreeGaussian = @load EvoTreeGaussian pkg=EvoTrees\nmodel = EvoTreeGaussian(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_boston\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mean(mach, X)\npreds = predict_mode(mach, X)\npreds = predict_median(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"tutorials/classification-iris/#Classication-on-Iris-dataset","page":"Classification - IRIS","title":"Classication on Iris dataset","text":"","category":"section"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"We will use the iris dataset, which is included in the MLDatasets package. This dataset consists of measurements of the sepal length, sepal width, petal length, and petal width for three different types of iris flowers: Setosa, Versicolor, and Virginica.","category":"page"},{"location":"tutorials/classification-iris/#Getting-started","page":"Classification - IRIS","title":"Getting started","text":"","category":"section"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"To begin, we will load the required packages and the dataset:","category":"page"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"using EvoTrees\nusing MLDatasets\nusing DataFrames\nusing Statistics: mean\nusing CategoricalArrays\nusing Random\n\ndf = MLDatasets.Iris().dataframe","category":"page"},{"location":"tutorials/classification-iris/#Preprocessing","page":"Classification - IRIS","title":"Preprocessing","text":"","category":"section"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"Before we can train our model, we need to preprocess the dataset. We will convert the class variable, which specifies the type of iris flower, into a categorical variable.","category":"page"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"Random.seed!(123)\n\ndf[!, :class] = categorical(df[!, :class])\n\ntrain_ratio = 0.8\ntrain_indices = randperm(nrow(df))[1:Int(train_ratio * nrow(df))]\n\ntrain_data = df[train_indices, :]\neval_data = df[setdiff(1:nrow(df), train_indices), :]\n\nx_train, y_train = Matrix(train_data[:, 1:4]), train_data[:, :class]\nx_eval, y_eval = Matrix(eval_data[:, 1:4]), eval_data[:, :class]","category":"page"},{"location":"tutorials/classification-iris/#Training","page":"Classification - IRIS","title":"Training","text":"","category":"section"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"Now we are ready to train our model. We will first define a model configuration using the EvoTreeClassifier model constructor.  Then, we'll use fit_evotree to train a boosted tree model. We'll pass optional x_eval and y_eval arguments, which enable the usage of early stopping. ","category":"page"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"config = EvoTreeClassifier(nrounds=200, eta=0.1, max_depth=5, lambda=0.01, rowsample = 0.8)\nmodel = fit_evotree(config;\n    x_train, y_train,\n    x_eval, y_eval,\n    metric = :mlogloss,\n    early_stopping_rounds=10,\n    print_every_n=10)","category":"page"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"Finally, we can get predictions by passing training and testing data to our model. We can then evaluate the accuracy of our model, which should be near 100% for this simple classification problem. ","category":"page"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"pred_train = model(x_train)\nidx_train = [findmax(row)[2] for row in eachrow(pred_train)]\n\npred_eval = model(x_eval)\nidx_eval = [findmax(row)[2] for row in eachrow(pred_eval)]","category":"page"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"julia> mean(idx_eval .== levelcode.(y_eval))\n1.0\n\njulia> mean(idx_eval .== levelcode.(y_eval))\n1.0","category":"page"},{"location":"tutorials/examples-MLJ/#MLJ-Integration","page":"MLJ API","title":"MLJ Integration","text":"","category":"section"},{"location":"tutorials/examples-MLJ/","page":"MLJ API","title":"MLJ API","text":"EvoTrees.jl provides a first-class integration with the MLJ ecosystem. ","category":"page"},{"location":"tutorials/examples-MLJ/","page":"MLJ API","title":"MLJ API","text":"See official project page for more info.","category":"page"},{"location":"tutorials/examples-MLJ/","page":"MLJ API","title":"MLJ API","text":"To use with MLJ, an EvoTrees model must first be initialized using either EvoTreeRegressor, EvoTreeClassifier, EvoTreeCount or EvoTreeGaussian. The model is then passed to MLJ's machine, opening access to the rest of the MLJ modeling ecosystem. ","category":"page"},{"location":"tutorials/examples-MLJ/","page":"MLJ API","title":"MLJ API","text":"using StatsBase: sample\nusing EvoTrees\nusing EvoTrees: sigmoid, logit # only needed to create the synthetic data below\nusing MLJBase\n\nfeatures = rand(10_000) .* 5 .- 2\nX = reshape(features, (size(features)[1], 1))\nY = sin.(features) .* 0.5 .+ 0.5\nY = logit(Y) + randn(size(Y))\nY = sigmoid(Y)\ny = Y\nX = MLJBase.table(X)\n\n# linear regression\ntree_model = EvoTreeRegressor(loss=:linear, max_depth=5, eta=0.05, nrounds=10)\n\n# set machine\nmach = machine(tree_model, X, y)\n\n# partition data\ntrain, test = partition(eachindex(y), 0.7, shuffle=true); # 70:30 split\n\n# fit data\nfit!(mach, rows=train, verbosity=1)\n\n# continue training\nmach.model.nrounds += 10\nfit!(mach, rows=train, verbosity=1)\n\n# predict on train data\npred_train = predict(mach, selectrows(X, train))\nmean(abs.(pred_train - selectrows(Y, train)))\n\n# predict on test data\npred_test = predict(mach, selectrows(X, test))\nmean(abs.(pred_test - selectrows(Y, test)))","category":"page"},{"location":"#[EvoTrees.jl](https://github.com/Evovest/EvoTrees.jl)","page":"Introduction","title":"EvoTrees.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"A Julia implementation of boosted trees with CPU and GPU support. Efficient histogram based algorithms with support for multiple loss functions, including various regressions, multi-classification and Gaussian max likelihood. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"See the examples-API section to get started using the internal API, or examples-MLJ to use within the MLJ framework.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Complete details about hyper-parameters are found in the Models section.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"R binding available.","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Latest:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> Pkg.add(url=\"https://github.com/Evovest/EvoTrees.jl\")","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"From General Registry:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> Pkg.add(\"EvoTrees\")","category":"page"},{"location":"#Quick-start-with-internal-API","page":"Introduction","title":"Quick start with internal API","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"A model configuration must first be defined, using one of the model constructor:      - EvoTreeRegressor     - EvoTreeClassifier     - EvoTreeCount     - EvoTreeMLE","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Then fitting can be performed using fit_evotree. This function supports additional arguments to provide eval data in order to track out of sample metrics and perform early stopping. Look at the docs for more details on available hyper-parameters for each of the above constructors and other options for training.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Predictions are obtained by passing features data to the model. Model acts as a functor, ie. it's a struct containing the fitted model as well as a function generating the prediction of that model for the features argument. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using EvoTrees\n\nconfig = EvoTreeRegressor(\n    loss=:linear, \n    nrounds=100, \n    max_depth=6, \n    nbins=32,\n    eta=0.1,\n    lambda=0.1, \n    gamma=0.1, \n    min_weight=1.0,\n    rowsample=0.5, \n    colsample=0.8)\n\nm = fit_evotree(config; x_train, y_train)\npreds = m(x_train)","category":"page"},{"location":"#Save/Load","page":"Introduction","title":"Save/Load","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"EvoTrees.save(m, \"data/model.bson\")\nm = EvoTrees.load(\"data/model.bson\");","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"A GPU model should be converted into a CPU one before saving: m_cpu = convert(EvoTree, m_gpu).","category":"page"},{"location":"tutorials/examples-API/#Internal-API-examples","page":"Internal API","title":"Internal API examples","text":"","category":"section"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"The following provides minimal examples of usage of the various loss functions available in EvoTrees using the internal API.","category":"page"},{"location":"tutorials/examples-API/#Regression","page":"Internal API","title":"Regression","text":"","category":"section"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"Minimal example to fit a noisy sinus wave.","category":"page"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"(Image: )","category":"page"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"using EvoTrees\nusing EvoTrees: sigmoid, logit\n\n# prepare a dataset\nfeatures = rand(10000) .* 20 .- 10\nX = reshape(features, (size(features)[1], 1))\nY = sin.(features) .* 0.5 .+ 0.5\nY = logit(Y) + randn(size(Y))\nY = sigmoid(Y)\n𝑖 = collect(1:size(X, 1))\n\n# train-eval split\n𝑖_sample = sample(𝑖, size(𝑖, 1), replace = false)\ntrain_size = 0.8\n𝑖_train = 𝑖_sample[1:floor(Int, train_size * size(𝑖, 1))]\n𝑖_eval = 𝑖_sample[floor(Int, train_size * size(𝑖, 1))+1:end]\n\nx_train, x_eval = X[𝑖_train, :], X[𝑖_eval, :]\ny_train, y_eval = Y[𝑖_train], Y[𝑖_eval]\n\nparams1 = EvoTreeRegressor(\n    loss=:linear, metric=:mse,\n    nrounds=100, nbins = 100,\n    lambda = 0.5, gamma=0.1, eta=0.1,\n    max_depth = 6, min_weight = 1.0,\n    rowsample=0.5, colsample=1.0)\n\nmodel = fit_evotree(params1; x_train, y_train, x_eval, y_eval, print_every_n = 25)\npred_eval_linear = predict(model, x_eval)\n\n# logistic / cross-entropy\nparams1 = EvoTreeRegressor(\n    loss=:logistic, metric = :logloss,\n    nrounds=100, nbins = 100,\n    lambda = 0.5, gamma=0.1, eta=0.1,\n    max_depth = 6, min_weight = 1.0,\n    rowsample=0.5, colsample=1.0)\n\nmodel = fit_evotree(params1; x_train, y_train, x_eval, y_eval, print_every_n = 25)\npred_eval_logistic = predict(model, x_eval)\n\n# L1\nparams1 = EvoTreeRegressor(\n    loss=:L1, alpha=0.5, metric = :mae,\n    nrounds=100, nbins=100,\n    lambda = 0.5, gamma=0.0, eta=0.1,\n    max_depth = 6, min_weight = 1.0,\n    rowsample=0.5, colsample=1.0)\n\nmodel = fit_evotree(params1; x_train, y_train, x_eval, y_eval, print_every_n = 25)\npred_eval_L1 = predict(model, x_eval)","category":"page"},{"location":"tutorials/examples-API/#Poisson-Count","page":"Internal API","title":"Poisson Count","text":"","category":"section"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"# Poisson\nparams1 = EvoTreeCount(\n    loss=:poisson, metric = :poisson,\n    nrounds=100, nbins = 100,\n    lambda = 0.5, gamma=0.1, eta=0.1,\n    max_depth = 6, min_weight = 1.0,\n    rowsample=0.5, colsample=1.0)\n\nmodel = fit_evotree(params1; x_train, y_train, x_eval, y_eval, print_every_n = 25)\npred_eval_poisson = predict(model, x_eval)","category":"page"},{"location":"tutorials/examples-API/#Quantile-Regression","page":"Internal API","title":"Quantile Regression","text":"","category":"section"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"(Image: )","category":"page"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"# q50\nparams1 = EvoTreeRegressor(\n    loss=:quantile, alpha=0.5, metric = :quantile,\n    nrounds=200, nbins = 100,\n    lambda = 0.1, gamma=0.0, eta=0.05,\n    max_depth = 6, min_weight = 1.0,\n    rowsample=0.5, colsample=1.0)\n\nmodel = fit_evotree(params1; x_train, y_train, x_eval, y_eval, print_every_n = 25)\npred_train_q50 = predict(model, x_train)\n\n# q20\nparams1 = EvoTreeRegressor(\n    loss=:quantile, alpha=0.2, metric = :quantile,\n    nrounds=200, nbins = 100,\n    lambda = 0.1, gamma=0.0, eta=0.05,\n    max_depth = 6, min_weight = 1.0,\n    rowsample=0.5, colsample=1.0)\n\nmodel = fit_evotree(params1; x_train, y_train, x_eval, y_eval, print_every_n = 25)\npred_train_q20 = predict(model, x_train)\n\n# q80\nparams1 = EvoTreeRegressor(\n    loss=:quantile, alpha=0.8,\n    nrounds=200, nbins = 100,\n    lambda = 0.1, gamma=0.0, eta=0.05,\n    max_depth = 6, min_weight = 1.0,\n    rowsample=0.5, colsample=1.0)\n\nmodel = fit_evotree(params1; x_train, y_train, x_eval, y_eval, print_every_n = 25)\npred_train_q80 = predict(model, x_train)","category":"page"},{"location":"tutorials/examples-API/#Gaussian-Max-Likelihood","page":"Internal API","title":"Gaussian Max Likelihood","text":"","category":"section"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"(Image: )","category":"page"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"params1 = EvoTreeMLE(\n    loss=:gaussian_mle, metric=:gaussian_mle,\n    nrounds=100, nbins=100,\n    lambda = 0.0, gamma=0.0, eta=0.1,\n    max_depth = 6, min_weight = 1.0,\n    rowsample=0.5, colsample=1.0, seed=123)","category":"page"}]
}
