var documenterSearchIndex = {"docs":
[{"location":"tutorials/ranking-LTRC/#Ranking-with-Yahoo!-Learning-to-Rank-Challenge.","page":"Ranking - Yahoo! LTRC","title":"Ranking with Yahoo! Learning to Rank Challenge.","text":"","category":"section"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"In this tutorial, we present how a ranking task can be tackled using regular regression techniques without compromising performance compared to specialized ranking learners. The data used is from the C14 - Yahoo! Learning to Rank Challenge, which can be obtained following a request to https://webscope.sandbox.yahoo.com.","category":"page"},{"location":"tutorials/ranking-LTRC/#Getting-started","page":"Ranking - Yahoo! LTRC","title":"Getting started","text":"","category":"section"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"To begin, we load the required packages:","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"using EvoTrees\nusing DataFrames\nusing Statistics: mean\nusing CategoricalArrays\nusing Random","category":"page"},{"location":"tutorials/ranking-LTRC/#Load-LIBSVM-format-data","page":"Ranking - Yahoo! LTRC","title":"Load LIBSVM format data","text":"","category":"section"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"Some datasets come in the so called LIBSVM format, which stores data using a sparse representation: ","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"<label> <query> <feature_id_1>:<feature_value_1> <feature_id_2>:<feature_value_2>","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"We use the ReadLIBSVM.jl package to perform parsing: ","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"using ReadLIBSVM\ndtrain = read_libsvm(\"set1.train.txt\"; has_query=true)\ndeval = read_libsvm(\"set1.valid.txt\"; has_query=true)\ndtest = read_libsvm(\"set1.test.txt\"; has_query=true)","category":"page"},{"location":"tutorials/ranking-LTRC/#Preprocessing","page":"Ranking - Yahoo! LTRC","title":"Preprocessing","text":"","category":"section"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"Preprocessing is minimal since all features are parsed as floats and specific files are provided for each of the train, eval and test splits. ","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"Several features are fully missing (contain only 0s) in the training dataset. They are removed from all datasets since they cannot bring value to the model.","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"Then, the features, targets and query ids are extracted from the parsed LIBSVM format. ","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"colsums_train = map(sum, eachcol(dtrain[:x]))\ndrop_cols = colsums_train .== 0\n\nx_train = dtrain[:x][:, .!drop_cols]\nx_eval = deval[:x][:, .!drop_cols]\nx_test = dtest[:x][:, .!drop_cols]\n\n# assign queries\nq_train = dtrain[:q]\nq_eval = deval[:q]\nq_test = dtest[:q]\n\n# assign targets\ny_train = dtrain[:y]\ny_eval = deval[:y]\ny_test = dtest[:y]","category":"page"},{"location":"tutorials/ranking-LTRC/#Training","page":"Ranking - Yahoo! LTRC","title":"Training","text":"","category":"section"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"Now we are ready to train our model. We first define a model configuration using the EvoTreeRegressor model constructor.  Then, we use fit_evotree to train a boosted tree model. The optional x_eval and y_eval arguments are provided to enable the usage of early stopping. ","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"config = EvoTreeRegressor(\n    nrounds=6000,\n    early_stopping_rounds=200,\n    loss=:mse,\n    eta=0.02,\n    nbins=64,\n    max_depth=11,\n    rowsample=0.9,\n    colsample=0.9,\n)\n\nm_mse, logger_mse = EvoTrees.fit(\n    config;\n    x_train=x_train,\n    y_train=y_train,\n    x_eval=x_eval,\n    y_eval=y_eval,\n    print_every_n=50,\n);\n\np_test = m_mse(x_test);","category":"page"},{"location":"tutorials/ranking-LTRC/#Model-evaluation","page":"Ranking - Yahoo! LTRC","title":"Model evaluation","text":"","category":"section"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"For ranking problems, a commonly used metric is the Normalized Discounted Cumulative Gain. It essentially considers whether the model is good at identifying the top K outcomes within a group. There are various flavors to its implementation, though the most commonly used one is the following:","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"function ndcg(p, y, k=10)\n    k = min(k, length(p))\n    p_order = partialsortperm(p, 1:k, rev=true)\n    y_order = partialsortperm(y, 1:k, rev=true)\n    _y = y[p_order]\n    gains = 2 .^ _y .- 1\n    discounts = log2.((1:k) .+ 1)\n    ndcg = sum(gains ./ discounts)\n\n    y_order = partialsortperm(y, 1:k, rev=true)\n    _y = y[y_order]\n    gains = 2 .^ _y .- 1\n    discounts = log2.((1:k) .+ 1)\n    idcg = sum(gains ./ discounts)\n    return idcg == 0 ? 1.0 : ndcg / idcg\nend","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"To compute the NDCG over a collection of groups, it is handy to leverage DataFrames' combine and groupby functionalities: ","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"test_df = DataFrame(p=p_test, y=y_test, q=q_test)\ntest_df_agg = combine(groupby(test_df, \"q\"), [\"p\", \"y\"] => ndcg => \"ndcg\")\nndcg_test = round(mean(test_df_agg.ndcg), sigdigits=5)\n@info \"ndcg_test MSE\" ndcg_test\n\nâ”Œ Info: ndcg_test MSE\nâ””   ndcg_test = 0.8008","category":"page"},{"location":"tutorials/ranking-LTRC/#Logistic-regression-alternative","page":"Ranking - Yahoo! LTRC","title":"Logistic regression alternative","text":"","category":"section"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"The above regression experiment shows a performance competitive with the results outlined in CatBoost's ranking benchmarks. ","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"Another approach is to use a scaling of the the target ranking scores to perform a logistic regression.","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"max_rank = 4\ny_train = dtrain[:y] ./ max_rank\ny_eval = deval[:y] ./ max_rank\ny_test = dtest[:y] ./ max_rank\n\nconfig = EvoTreeRegressor(\n    nrounds=6000,\n    early_stopping_rounds=200,\n    loss=:logloss,\n    eta=0.01,\n    nbins=64,\n    max_depth=11,\n    rowsample=0.9,\n    colsample=0.9,\n)\n\nm_logloss, logger_logloss = EvoTrees.fit(\n    config;\n    x_train=x_train,\n    y_train=y_train,\n    x_eval=x_eval,\n    y_eval=y_eval,\n    print_every_n=50,\n);","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"To measure the NDCG, the original targets must be used since NDCG is a scale sensitive measure.","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"y_train = dtrain[:y]\ny_eval = deval[:y]\ny_test = dtest[:y]\n\np_test = m_logloss(x_test);\ntest_df = DataFrame(p=p_test, y=y_test, q=q_test)\ntest_df_agg = combine(groupby(test_df, \"q\"), [\"p\", \"y\"] => ndcg => \"ndcg\")\nndcg_test = round(mean(test_df_agg.ndcg), sigdigits=5)\n@info \"ndcg_test LogLoss\" ndcg_test\n\nâ”Œ Info: ndcg_test LogLoss\nâ””   ndcg_test = 0.80267","category":"page"},{"location":"tutorials/ranking-LTRC/#Conclusion","page":"Ranking - Yahoo! LTRC","title":"Conclusion","text":"","category":"section"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"We've seen that a ranking problem can be efficiently handled with generic regression tasks, yet achieve comparable performance to specialized ranking loss functions. Below, we present the NDCG obtained from the above experiments along those published on CatBoost's benchmarks.","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"Model NDCG\nEvoTrees - mse 0.80080\nEvoTrees - logistic 0.80267\ncat-rmse 0.802115\ncat-query-rmse 0.802229\ncat-pair-logit 0.797318\ncat-pair-logit-pairwise 0.790396\ncat-yeti-rank 0.802972\nxgb-rmse 0.798892\nxgb-pairwise 0.800048\nxgb-lambdamart-ndcg 0.800048\nlgb-rmse 0.8013675\nlgb-pairwise 0.801347","category":"page"},{"location":"tutorials/ranking-LTRC/","page":"Ranking - Yahoo! LTRC","title":"Ranking - Yahoo! LTRC","text":"It should be noted that the later results were not reproduced in the scope of current tutorial, so one should be careful about any claim of model superiority. The results from CatBoost's benchmarks were however already indicative of strong performance of non-specialized ranking loss functions, to which this tutorial brings further support. ","category":"page"},{"location":"tutorials/logistic-regression-titanic/#Logistic-Regression-on-Titanic-Dataset","page":"Logistic Regression - Titanic","title":"Logistic Regression on Titanic Dataset","text":"","category":"section"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"We will use the Titanic dataset, which is included in the MLDatasets package. It describes the survival status of individual passengers on the Titanic. The model will be approached as a logistic regression problem, although a Classifier model could also have been used (see the Classification - Iris tutorial). ","category":"page"},{"location":"tutorials/logistic-regression-titanic/#Getting-started","page":"Logistic Regression - Titanic","title":"Getting started","text":"","category":"section"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"To begin, we will load the required packages and the dataset:","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"using EvoTrees\nusing MLDatasets\nusing DataFrames\nusing Statistics: mean, median\nusing CategoricalArrays\nusing Random\n\ndf = MLDatasets.Titanic().dataframe","category":"page"},{"location":"tutorials/logistic-regression-titanic/#Preprocessing","page":"Logistic Regression - Titanic","title":"Preprocessing","text":"","category":"section"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"A first step in data processing is to prepare the input features in a model compatible format. ","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"EvoTrees' Tables API supports input that are either Real (incl. Bool) or Categorical. Bool variables are treated as unordered, 2-levels categorical variables. A recommended approach for String features such as Sex is to convert them into an unordered Categorical. ","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"For dealing with features with missing values such as Age, a common approach is to first create an Bool indicator variable capturing the info on whether a value is missing. Then, the missing values can be imputed (replaced by some default values such as mean or median, or more sophisticated approach such as predictions from another model).","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"# convert string feature to Categorical\ntransform!(df, :Sex => categorical => :Sex)\n\n# treat string feature and missing values\ntransform!(df, :Age => ByRow(ismissing) => :Age_ismissing)\ntransform!(df, :Age => (x -> coalesce.(x, median(skipmissing(x)))) => :Age);\n\n# remove unneeded variables\ndf = df[:, Not([:PassengerId, :Name, :Embarked, :Cabin, :Ticket])]","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"The full data can now be split according to train and eval indices.  Target and feature names are also set.","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"Random.seed!(123)\n\ntrain_ratio = 0.8\ntrain_indices = randperm(nrow(df))[1:Int(round(train_ratio * nrow(df)))]\n\ndtrain = df[train_indices, :]\ndeval = df[setdiff(1:nrow(df), train_indices), :]\n\ntarget_name = \"Survived\"\nfeature_names = setdiff(names(df), [target_name])","category":"page"},{"location":"tutorials/logistic-regression-titanic/#Training","page":"Logistic Regression - Titanic","title":"Training","text":"","category":"section"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"Now we are ready to train our model. We will first define a model configuration using the EvoTreeRegressor model constructor.  Then, we'll use fit_evotree to train a boosted tree model. We'll pass optional deval arguments, which enables the tracking of an evaluation metric and early stopping. ","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"config = EvoTreeRegressor(\n  loss=:logloss, \n  nrounds=200, \n  early_stopping_rounds=10,\n  eta=0.05, \n  nbins=128, \n  max_depth=5, \n  rowsample=0.5, \n  colsample=0.9)\n\nmodel = EvoTrees.fit(\n    config, dtrain; \n    deval,\n    target_name,\n    feature_names,\n    print_every_n=10)","category":"page"},{"location":"tutorials/logistic-regression-titanic/#Diagnosis","page":"Logistic Regression - Titanic","title":"Diagnosis","text":"","category":"section"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"We can get predictions by passing training and testing data to our model. We can then evaluate the accuracy of our model, which should be around 85%. ","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"pred_train = model(dtrain)\npred_eval = model(deval)","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"julia> mean((pred_train .> 0.5) .== dtrain[!, target_name])\n0.8821879382889201\n\njulia> mean((pred_eval .> 0.5) .== deval[!, target_name])\n0.8426966292134831","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"Finally, features importance can be inspected using EvoTrees.importance.","category":"page"},{"location":"tutorials/logistic-regression-titanic/","page":"Logistic Regression - Titanic","title":"Logistic Regression - Titanic","text":"julia> EvoTrees.importance(model)\n7-element Vector{Pair{String, Float64}}:\n           \"Sex\" => 0.29612654189959403\n           \"Age\" => 0.25487324307720827\n          \"Fare\" => 0.2530947969323613\n        \"Pclass\" => 0.11354283043193575\n         \"SibSp\" => 0.05129209383816148\n         \"Parch\" => 0.017385183317069588\n \"Age_ismissing\" => 0.013685310503669728","category":"page"},{"location":"tutorials/examples-API/#Internal-API-examples","page":"Internal API","title":"Internal API examples","text":"","category":"section"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"The following provides minimal examples of usage of the various loss functions available in EvoTrees using the internal API.","category":"page"},{"location":"tutorials/examples-API/#Regression","page":"Internal API","title":"Regression","text":"","category":"section"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"Minimal example to fit a noisy sinus wave.","category":"page"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"(Image: )","category":"page"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"using EvoTrees\nusing EvoTrees: fit\nusing EvoTrees: sigmoid, logit\nusing StatsBase: sample\n\n# prepare a dataset\nfeatures = rand(10000) .* 20 .- 10\nX = reshape(features, (size(features)[1], 1))\nY = sin.(features) .* 0.5 .+ 0.5\nY = logit(Y) + randn(size(Y))\nY = sigmoid(Y)\nð‘– = collect(1:size(X, 1))\n\n# train-eval split\nð‘–_sample = sample(ð‘–, size(ð‘–, 1), replace = false)\ntrain_size = 0.8\nð‘–_train = ð‘–_sample[1:floor(Int, train_size * size(ð‘–, 1))]\nð‘–_eval = ð‘–_sample[floor(Int, train_size * size(ð‘–, 1))+1:end]\n\nx_train, x_eval = X[ð‘–_train, :], X[ð‘–_eval, :]\ny_train, y_eval = Y[ð‘–_train], Y[ð‘–_eval]\n\nconfig = EvoTreeRegressor(\n    loss=:mse,\n    nrounds=100, nbins = 100,\n    lambda = 0.5, gamma=0.1, eta=0.1,\n    max_depth=6, min_weight=1.0,\n    rowsample=0.5, colsample=1.0)\n\nmodel = fit(config; x_train, y_train, x_eval, y_eval, print_every_n=25)\npred_eval_linear = model(x_eval)\n\n# logistic / cross-entropy\nconfig = EvoTreeRegressor(\n    loss=:logloss,\n    nrounds=100, nbins = 100,\n    lambda = 0.5, gamma=0.1, eta=0.1,\n    max_depth=6, min_weight=1.0,\n    rowsample=0.5, colsample=1.0)\n\nmodel = fit(config; x_train, y_train, x_eval, y_eval, print_every_n=25)\npred_eval_logistic = model(x_eval)\n\n# L1\nconfig = EvoTreeRegressor(\n    loss=:mae, alpha=0.5,\n    nrounds=100, nbins=100,\n    lambda = 0.5, gamma=0.0, eta=0.1,\n    max_depth=6, min_weight=1.0,\n    rowsample=0.5, colsample=1.0)\n\nmodel = fit(config; x_train, y_train, x_eval, y_eval, print_every_n=25)\npred_eval_L1 = model(x_eval)","category":"page"},{"location":"tutorials/examples-API/#Poisson-Count","page":"Internal API","title":"Poisson Count","text":"","category":"section"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"# Poisson\nconfig = EvoTreeCount(\n    loss=:poisson,\n    nrounds=100, nbins=100,\n    lambda=0.5, gamma=0.1, eta=0.1,\n    max_depth=6, min_weight=1.0,\n    rowsample=0.5, colsample=1.0)\n\nmodel = fit(config; x_train, y_train, x_eval, y_eval, print_every_n = 25)\npred_eval_poisson = model(x_eval)","category":"page"},{"location":"tutorials/examples-API/#Quantile-Regression","page":"Internal API","title":"Quantile Regression","text":"","category":"section"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"(Image: )","category":"page"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"# q50\nconfig = EvoTreeRegressor(\n    loss=:quantile, alpha=0.5,\n    nrounds=200, nbins=100,\n    lambda=0.1, gamma=0.0, eta=0.05,\n    max_depth=6, min_weight=1.0,\n    rowsample=0.5, colsample=1.0)\n\nmodel = fit(config; x_train, y_train, x_eval, y_eval, print_every_n = 25)\npred_train_q50 = model(x_train)\n\n# q20\nconfig = EvoTreeRegressor(\n    loss=:quantile, alpha=0.2,\n    nrounds=200, nbins=100,\n    lambda=0.1, gamma=0.0, eta=0.05,\n    max_depth=6, min_weight=1.0,\n    rowsample=0.5, colsample=1.0)\n\nmodel = fit(config; x_train, y_train, x_eval, y_eval, print_every_n = 25)\npred_train_q20 = model(x_train)\n\n# q80\nconfig = EvoTreeRegressor(\n    loss=:quantile, alpha=0.8,\n    nrounds=200, nbins=100,\n    lambda=0.1, gamma=0.0, eta=0.05,\n    max_depth=6, min_weight=1.0,\n    rowsample=0.5, colsample=1.0)\n\nmodel = fit(config; x_train, y_train, x_eval, y_eval, print_every_n = 25)\npred_train_q80 = model(x_train)","category":"page"},{"location":"tutorials/examples-API/#Gaussian-Max-Likelihood","page":"Internal API","title":"Gaussian Max Likelihood","text":"","category":"section"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"(Image: )","category":"page"},{"location":"tutorials/examples-API/","page":"Internal API","title":"Internal API","text":"config = EvoTreeMLE(\n    loss=:gaussian_mle,\n    nrounds=100, nbins=100,\n    lambda=0.0, gamma=0.0, eta=0.1,\n    max_depth=6, rowsample=0.5)","category":"page"},{"location":"models/#Models","page":"Models","title":"Models","text":"","category":"section"},{"location":"models/#EvoTreeRegressor","page":"Models","title":"EvoTreeRegressor","text":"","category":"section"},{"location":"models/#EvoTrees.EvoTreeRegressor","page":"Models","title":"EvoTrees.EvoTreeRegressor","text":"EvoTreeRegressor(;kwargs...)\n\nA model type for constructing a EvoTreeRegressor, based on EvoTrees.jl, and implementing both an internal API and the MLJ model interface.\n\nHyper-parameters\n\nloss=:mse:         Loss to be be minimized during training. One of:\n:mse\n:mae\n:logloss\n:gamma\n:tweedie\n:quantile\n:cred_var: experimental credibility-based gains, derived from ratio of spread to process variance.\n:cred_std: experimental credibility-based gains, derived from ratio of spread to process std deviation.\nmetric:     The evaluation metric used to track evaluation data and serves as a basis for early stopping. Supported metrics are: \n:mse:     Mean-squared error. Adapted for general regression models.\n:rmse:    Root-mean-squared error. Adapted for general regression models.\n:mae:     Mean absolute error. Adapted for general regression models.\n:logloss: Adapted for :logistic regression models.\n:poisson: Poisson deviance. Adapted to EvoTreeCount count models.\n:gamma:   Gamma deviance. Adapted to regression problem on Gamma like, positively distributed targets.\n:tweedie: Tweedie deviance. Adapted to regression problem on Tweedie like, positively distributed targets with probability mass at y == 0.\n:quantile: The corresponds to an assymetric absolute error, where residuals are penalized according to alpha / (1-alpha) according to their sign.\n:gini: The normalized Gini between pred and target\nearly_stopping_rounds::Integer: number of consecutive rounds without metric improvement after which fitting in stopped. \nnrounds=100:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be >= 1.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions. Must be > 0. A lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.   \nL2::T=0.0:            L2 regularization factor on aggregate gain. Must be >= 0. Higher L2 can result in a more robust model.\nlambda::T=0.0:        L2 regularization factor on individual gain. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:         Minimum gain improvement needed to perform a node split. Higher gamma can result in a more robust model. Must be >= 0.\nalpha::T=0.5:         Loss specific parameter in the [0, 1] range:                           - :quantile: target quantile for the regression.\nmax_depth=6:          Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=1.0:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector. Must be > 0.\nrowsample=1.0:        Proportion of rows that are sampled at each iteration to build the tree. Should be in ]0, 1].\ncolsample=1.0:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be in ]0, 1].\nnbins=64:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.\nmonotone_constraints=Dict{Int, Int}(): Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  Only :linear, :logistic, :gamma and tweedie losses are supported at the moment.\ntree_type=:binary    Tree structure to be used. One of:\n:binary:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see gamma) stops further node splits.  \n:oblivious:    A common splitting condition is imposed to all nodes of a given depth. \nrng=123:              Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=:cpu: Hardware device to use for computations. Can be either :cpu or gpu.\n\nInternal API\n\nDo config = EvoTreeRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeRegressor(loss=...).\n\nTraining model\n\nA model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n\nInference\n\nPredictions are obtained using predict which returns a Vector of length nobs:\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)\n\nMLJ Interface\n\nFrom MLJ, the type can be imported using:\n\nEvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\n\nDo model = EvoTreeRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeRegressor(loss=...).\n\nTraining model\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y) where\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are deterministic.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.\n\nReport\n\nThe fields of report(mach) are:\n\n:features: The names of the features encountered in training.\n\nExamples\n\n# Internal API\nusing EvoTrees\nconfig = EvoTreeRegressor(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\n# MLJ Interface\nusing MLJ\nEvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\nmodel = EvoTreeRegressor(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_boston\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"models/#EvoTreeClassifier","page":"Models","title":"EvoTreeClassifier","text":"","category":"section"},{"location":"models/#EvoTrees.EvoTreeClassifier","page":"Models","title":"EvoTrees.EvoTreeClassifier","text":"EvoTreeClassifier(;kwargs...)\n\nA model type for constructing a EvoTreeClassifier, based on EvoTrees.jl, and implementing both an internal API and the MLJ model interface. EvoTreeClassifier is used to perform multi-class classification, using cross-entropy loss.\n\nHyper-parameters\n\nearly_stopping_rounds::Integer: number of consecutive rounds without metric improvement after which fitting in stopped. \nnrounds=100:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be >= 1.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions. Must be > 0. A lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.  \nL2::T=0.0:            L2 regularization factor on aggregate gain. Must be >= 0. Higher L2 can result in a more robust model.\nlambda::T=0.0:        L2 regularization factor on individual gain. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:         Minimum gain improvement needed to perform a node split. Higher gamma can result in a more robust model. Must be >= 0.\nmax_depth=6:          Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=1.0:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector. Must be > 0.\nrowsample=1.0:        Proportion of rows that are sampled at each iteration to build the tree. Should be in ]0, 1].\ncolsample=1.0:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be in ]0, 1].\nnbins=64:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.\ntree_type=:binary    Tree structure to be used. One of:\n:binary:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see gamma) stops further node splits.  \n:oblivious:    A common splitting condition is imposed to all nodes of a given depth. \nrng=123:              Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=:cpu: Hardware device to use for computations. Can be either :cpu or :gpu.\n\nInternal API\n\nDo config = EvoTreeClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeClassifier(max_depth=...).\n\nTraining model\n\nA model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n\nInference\n\nPredictions are obtained using predict which returns a Matrix of size [nobs, K] where K is the number of classes:\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)\n\nMLJ\n\nFrom MLJ, the type can be imported using:\n\nEvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees\n\nDo model = EvoTreeClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeClassifier(loss=...).\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Multiclas or <:OrderedFactor; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic.\npredict_mode(mach, Xnew): returns the mode of each of the prediction above.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.\n\nReport\n\nThe fields of report(mach) are:\n\n:features: The names of the features encountered in training.\n\nExamples\n\n# Internal API\nusing EvoTrees\nconfig = EvoTreeClassifier(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(1:3, nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\n# MLJ Interface\nusing MLJ\nEvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees\nmodel = EvoTreeClassifier(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_iris\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mode(mach, X)\n\nSee also EvoTrees.jl.\n\n\n\n\n\n","category":"type"},{"location":"models/#EvoTreeCount","page":"Models","title":"EvoTreeCount","text":"","category":"section"},{"location":"models/#EvoTrees.EvoTreeCount","page":"Models","title":"EvoTrees.EvoTreeCount","text":"EvoTreeCount(;kwargs...)\n\nA model type for constructing a EvoTreeCount, based on EvoTrees.jl, and implementing both an internal API the MLJ model interface. EvoTreeCount is used to perform Poisson probabilistic regression on count target.\n\nHyper-parameters\n\nearly_stopping_rounds::Integer: number of consecutive rounds without metric improvement after which fitting in stopped. \nnrounds=100:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be >= 1.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions. Must be > 0. A lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.  \nL2::T=0.0:            L2 regularization factor on aggregate gain. Must be >= 0. Higher L2 can result in a more robust model.\nlambda::T=0.0:        L2 regularization factor on individual gain. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:         Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model.\nmax_depth=6:          Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=1.0:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector. Must be > 0.\nrowsample=1.0:        Proportion of rows that are sampled at each iteration to build the tree. Should be ]0, 1].\ncolsample=1.0:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be ]0, 1].\nnbins=64:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.\nmonotone_constraints=Dict{Int, Int}(): Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).\ntree_type=:binary    Tree structure to be used. One of:\n:binary:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see gamma) stops further node splits.  \n:oblivious:    A common splitting condition is imposed to all nodes of a given depth. \nrng=123:              Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=:cpu: Hardware device to use for computations. Can be either :cpu or :gpu.\n\nInternal API\n\nDo config = EvoTreeCount() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeCount(max_depth=...).\n\nTraining model\n\nA model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n\nInference\n\nPredictions are obtained using predict which returns a Vector of length nobs:\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)\n\nMLJ\n\nFrom MLJ, the type can be imported using:\n\nEvoTreeCount = @load EvoTreeCount pkg=EvoTrees\n\nDo model = EvoTreeCount() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeCount(loss=...).\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with     mach = machine(model, X, y) where\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Count; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nOperations\n\npredict(mach, Xnew): returns a vector of Poisson distributions given features Xnew having the same scitype as X above. Predictions are probabilistic.\n\nSpecific metrics can also be predicted using:\n\npredict_mean(mach, Xnew)\npredict_mode(mach, Xnew)\npredict_median(mach, Xnew)\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.\n\nReport\n\nThe fields of report(mach) are:\n\n:features: The names of the features encountered in training.\n\nExamples\n\n# Internal API\nusing EvoTrees\nconfig = EvoTreeCount(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(0:2, nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\nusing MLJ\nEvoTreeCount = @load EvoTreeCount pkg=EvoTrees\nmodel = EvoTreeCount(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nX, y = randn(nobs, nfeats), rand(0:2, nobs)\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mean(mach, X)\npreds = predict_mode(mach, X)\npreds = predict_median(mach, X)\n\n\nSee also EvoTrees.jl.\n\n\n\n\n\n","category":"type"},{"location":"models/#EvoTreeMLE","page":"Models","title":"EvoTreeMLE","text":"","category":"section"},{"location":"models/#EvoTrees.EvoTreeMLE","page":"Models","title":"EvoTrees.EvoTreeMLE","text":"EvoTreeMLE(;kwargs...)\n\nA model type for constructing a EvoTreeMLE, based on EvoTrees.jl, and implementing both an internal API the MLJ model interface. EvoTreeMLE performs maximum likelihood estimation. Assumed distribution is specified through loss kwargs. Both Gaussian and Logistic distributions are supported.\n\nHyper-parameters\n\nearly_stopping_rounds::Integer: number of consecutive rounds without metric improvement after which fitting in stopped. \n\nloss=:gaussian:         Loss to be be minimized during training. One of:\n\n:gaussian_mle\n:logistic_mle\nnrounds=100:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be >= 1.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions. Must be > 0.\n\nA lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.  \n\nL2::T=0.0:            L2 regularization factor on aggregate gain. Must be >= 0. Higher L2 can result in a more robust model.\nlambda::T=0.0:        L2 regularization factor on individual gain. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:         Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model. Must be >= 0.\nmax_depth=6:          Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=8.0:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector. Must be > 0.\nrowsample=1.0:        Proportion of rows that are sampled at each iteration to build the tree. Should be in ]0, 1].\ncolsample=1.0:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be in ]0, 1].\nnbins=64:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.\nmonotone_constraints=Dict{Int, Int}(): Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  !Experimental feature: note that for MLE regression, constraints may not be enforced systematically.\ntree_type=:binary    Tree structure to be used. One of:\n:binary:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see gamma) stops further node splits.  \n:oblivious:    A common splitting condition is imposed to all nodes of a given depth. \nrng=123:              Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=:cpu: Hardware device to use for computations. Can be either :cpu or gpu. Following losses are not GPU supported at the moment: :logistic_mle.\n\nInternal API\n\nDo config = EvoTreeMLE() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeMLE(max_depth=...).\n\nTraining model\n\nA model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n\nInference\n\nPredictions are obtained using predict which returns a Matrix of size [nobs, nparams] where the second dimensions refer to Î¼ & Ïƒ for Normal/Gaussian and Î¼ & s for Logistic.\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)\n\nMLJ\n\nFrom MLJ, the type can be imported using:\n\nEvoTreeMLE = @load EvoTreeMLE pkg=EvoTrees\n\nDo model = EvoTreeMLE() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeMLE(loss=...).\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nOperations\n\npredict(mach, Xnew): returns a vector of Gaussian or Logistic distributions (according to provided loss) given features Xnew having the same scitype as X above.\n\nPredictions are probabilistic.\n\nSpecific metrics can also be predicted using:\n\npredict_mean(mach, Xnew)\npredict_mode(mach, Xnew)\npredict_median(mach, Xnew)\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.\n\nReport\n\nThe fields of report(mach) are:\n\n:features: The names of the features encountered in training.\n\nExamples\n\n# Internal API\nusing EvoTrees\nconfig = EvoTreeMLE(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\n# MLJ Interface\nusing MLJ\nEvoTreeMLE = @load EvoTreeMLE pkg=EvoTrees\nmodel = EvoTreeMLE(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_boston\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mean(mach, X)\npreds = predict_mode(mach, X)\npreds = predict_median(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"models/#EvoTreeGaussian","page":"Models","title":"EvoTreeGaussian","text":"","category":"section"},{"location":"models/","page":"Models","title":"Models","text":"EvoTreeGaussian is to be deprecated. Use EvoTreeMLE with loss = :gaussian_mle. ","category":"page"},{"location":"models/#EvoTrees.EvoTreeGaussian","page":"Models","title":"EvoTrees.EvoTreeGaussian","text":"EvoTreeGaussian(;kwargs...)\n\nA model type for constructing a EvoTreeGaussian, based on EvoTrees.jl, and implementing both an internal API the MLJ model interface. EvoTreeGaussian is used to perform Gaussian probabilistic regression, fitting Î¼ and Ïƒ parameters to maximize likelihood.\n\nHyper-parameters\n\nearly_stopping_rounds::Integer: number of consecutive rounds without metric improvement after which fitting in stopped. \nnrounds=100:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be >= 1.\neta=0.1:              Learning rate. Each tree raw predictions are scaled by eta prior to be added to the stack of predictions. Must be > 0. A lower eta results in slower learning, requiring a higher nrounds but typically improves model performance.  \nL2::T=0.0:            L2 regularization factor on aggregate gain. Must be >= 0. Higher L2 can result in a more robust model.\nlambda::T=0.0:        L2 regularization factor on individual gain. Must be >= 0. Higher lambda can result in a more robust model.\ngamma::T=0.0:         Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model. Must be >= 0.\nmax_depth=6:          Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains 2^(N - 1) terminal leaves and 2^(N - 1) - 1 split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\nmin_weight=8.0:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the weights vector. Must be > 0.\nrowsample=1.0:        Proportion of rows that are sampled at each iteration to build the tree. Should be in ]0, 1].\ncolsample=1.0:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be in ]0, 1].\nnbins=64:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.\nmonotone_constraints=Dict{Int, Int}(): Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  !Experimental feature: note that for Gaussian regression, constraints may not be enforce systematically.\ntree_type=:binary    Tree structure to be used. One of:\n:binary:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see gamma) stops further node splits.  \n:oblivious:    A common splitting condition is imposed to all nodes of a given depth. \nrng=123:              Either an integer used as a seed to the random number generator or an actual random number generator (::Random.AbstractRNG).\ndevice=:cpu: Hardware device to use for computations. Can be either :cpu or gpu.\n\nInternal API\n\nDo config = EvoTreeGaussian() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeGaussian(max_depth=...).\n\nTraining model\n\nA model is built using fit_evotree:\n\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n\nInference\n\nPredictions are obtained using predict which returns a Matrix of size [nobs, 2] where the second dimensions refer to Î¼ and Ïƒ respectively:\n\nEvoTrees.predict(model, X)\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\nmodel(X)\n\nMLJ\n\nFrom MLJ, the type can be imported using:\n\nEvoTreeGaussian = @load EvoTreeGaussian pkg=EvoTrees\n\nDo model = EvoTreeGaussian() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeGaussian(loss=...).\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or <:OrderedFactor; check column scitypes with schema(X)\ny: is the target, which can be any AbstractVector whose element scitype is <:Continuous; check the scitype with scitype(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nOperations\n\npredict(mach, Xnew): returns a vector of Gaussian distributions given features Xnew having the same scitype as X above.\n\nPredictions are probabilistic.\n\nSpecific metrics can also be predicted using:\n\npredict_mean(mach, Xnew)\npredict_mode(mach, Xnew)\npredict_median(mach, Xnew)\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\n:fitresult: The GBTree object returned by EvoTrees.jl fitting algorithm.\n\nReport\n\nThe fields of report(mach) are:\n\n:features: The names of the features encountered in training.\n\nExamples\n\n# Internal API\nusing EvoTrees\nparams = EvoTreeGaussian(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(nobs)\nmodel = fit_evotree(params; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n\n# MLJ Interface\nusing MLJ\nEvoTreeGaussian = @load EvoTreeGaussian pkg=EvoTrees\nmodel = EvoTreeGaussian(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_boston\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mean(mach, X)\npreds = predict_mode(mach, X)\npreds = predict_median(mach, X)\n\n\n\n\n\n","category":"type"},{"location":"tutorials/examples-MLJ/#MLJ-Integration","page":"MLJ API","title":"MLJ Integration","text":"","category":"section"},{"location":"tutorials/examples-MLJ/","page":"MLJ API","title":"MLJ API","text":"EvoTrees.jl provides a first-class integration with the MLJ ecosystem. ","category":"page"},{"location":"tutorials/examples-MLJ/","page":"MLJ API","title":"MLJ API","text":"See official project page for more info.","category":"page"},{"location":"tutorials/examples-MLJ/","page":"MLJ API","title":"MLJ API","text":"To use with MLJ, an EvoTrees model configuration must first be initialized using either: ","category":"page"},{"location":"tutorials/examples-MLJ/","page":"MLJ API","title":"MLJ API","text":"EvoTreeRegressor\nEvoTreeClassifier\nEvoTreeCount\nEvoTreeMLE","category":"page"},{"location":"tutorials/examples-MLJ/","page":"MLJ API","title":"MLJ API","text":"The model is then passed to MLJ's machine, opening access to the rest of the MLJ modeling ecosystem. ","category":"page"},{"location":"tutorials/examples-MLJ/","page":"MLJ API","title":"MLJ API","text":"using StatsBase: sample\nusing EvoTrees\nusing EvoTrees: sigmoid, logit # only needed to create the synthetic data below\nusing MLJBase\n\nfeatures = rand(10_000) .* 5 .- 2\nX = reshape(features, (size(features)[1], 1))\nY = sin.(features) .* 0.5 .+ 0.5\nY = logit(Y) + randn(size(Y))\nY = sigmoid(Y)\ny = Y\nX = MLJBase.table(X)\n\n# mse regression\ntree_model = EvoTreeRegressor(loss=:mse, max_depth=5, eta=0.05, nrounds=10)\n\n# set machine\nmach = machine(tree_model, X, y)\n\n# partition data\ntrain, test = partition(eachindex(y), 0.7, shuffle=true); # 70:30 split\n\n# fit data\nfit!(mach, rows=train, verbosity=1)\n\n# continue training\nmach.model.nrounds += 10\nfit!(mach, rows=train, verbosity=1)\n\n# predict on train data\npred_train = predict(mach, selectrows(X, train))\nmean(abs.(pred_train - selectrows(Y, train)))\n\n# predict on test data\npred_test = predict(mach, selectrows(X, test))\nmean(abs.(pred_test - selectrows(Y, test)))","category":"page"},{"location":"tutorials/cred-loss/#Exploring-a-credibility-based-approach-for-tree-gain-estimation","page":"Credibility-based loss","title":"Exploring a credibility-based approach for tree-gain estimation","text":"","category":"section"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"The motivation for this experiment was to explore an alternative to gradient-based gain measure by integrating the volatility of split candidates to identify the best node split.","category":"page"},{"location":"tutorials/cred-loss/#Review-of-key-gradient-based-MSE-characteristics","page":"Credibility-based loss","title":"Review of key gradient-based MSE characteristics","text":"","category":"section"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"The figures below illustrate the behavior of vanilla gradient-based approach using a mean-squared error (MSE) loss. The 2 colors represent the observations belonging to the left and right children.","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"Key observations:","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"the gain is invariant to the volatility: the top vs bottom figures differ only by the std dev of the observations.   The associated gain is identical, which is aligned with the gradient-based approach to gain: the gain matches the reduction in the MSE, which is identical regardless of the dispersion. It's strictly driven by their mean.\nthe gain scales linearly with the number of observations: the right vs left figures contrast different number of observations (100 vs 10k), and show that gain is directly proportional.\nthe gain scales quadratically with the spread: moving from a spread of 1.0 to 0.1 between the 2nd and 3rd row results in a drop by 100x of the gain: from 50.0 to 0.5.","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"(Image: ) (Image: )\n(Image: ) (Image: )","category":"page"},{"location":"tutorials/cred-loss/#Credibility-based-gains","page":"Credibility-based loss","title":"Credibility-based gains","text":"","category":"section"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"The idea is for gain to reflect varying uncertainty levels for observations associated to each of the tree-split candidates. For tree-split candidates with an identical spread, the intuition is that candidates with a lower volatility, all other things being equal, should be preferred. The original inspiration comes from credibility theory, a foundational notion in actuarial science with direct connection mixed effect models and bayesian theory. Key concept is that the credibility associated with a set of observations is driven by the relative effect of 2 components:","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"Variance of the Hypothetical Means (VHM): if large differences between candidates means are expected, a greater credibility is assigned.\nExpected Value of the Process Variance (EVPV): if the data generation process of a given candidate has a large volatility, a smaller credibility is assigned.","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"The Buhlmann credibility states that the optimal linear posterior estimator of a group mean is:","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"Z * XÌ„ + (1 - Z) * Î¼, where XÌ„ is the group mean and Î¼ the population mean.","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"This approach results in a shift of perspective in how the gain is derived. Classical gradient-based is about deriving a second-order approximation of the loss curve for a tree-split candidate. The gain corresponds to the reduction in this approximated loss by taking the prediction that minimises the quadratic loss curve. The credibility-based takes a loss function agnostic approach, and view the gain as the total absolute change in the credibility-adjusted predicted value. Example, if a child has a mean residual of 2.0, credibility of 0.5 and 100 observations, the resulting gain is: 2.0 * 0.5 * 100 = 100.0, where 2.0 * 0.5 corresponds to the credibility adjusted prediction.","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"VHM is estimated as the square of the mean of the spread between observed values and predictions:","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"VHM = E[X] = mean(y - p)","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"EVPV is estimated as the variance of the observations. This value can be derived from the aggregation of the first and second moment of the individual observations:","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"EVPV = E[(x - Î¼)Â²] = E[XÂ²] - EÂ²[X]","category":"page"},{"location":"tutorials/cred-loss/#Credibility-based-losses-in-EvoTrees","page":"Credibility-based loss","title":"Credibility-based losses in EvoTrees","text":"","category":"section"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"Two credibility-based losses are supported in EvoTreeRegressor:","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"cred_var: VHM / (VHM + EVPV)\ncred_std: sqrt(VHM) / (sqrt(VHM) + sqrt(EVPV))","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"Just like the gradient-based MSE error, the gain grows linearly with the number of observations, all other things being equal. However, a smaller volatility results in an increased gain, as shown in 2nd vs 1st row.","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"(Image: ) (Image: )\n(Image: ) (Image: )","category":"page"},{"location":"tutorials/cred-loss/#Simulation-grid","page":"Credibility-based loss","title":"Simulation grid","text":"","category":"section"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"The chart below show the associated credibility and gain for a given node split candidate for various spreads and standards deviations.","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"nobs = 1000\nsd_list = [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 5]\nspread_list = [0.01, 0.05, 0.1, 0.2, 0.5, 1]","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"(Image: ) (Image: )\n ","category":"page"},{"location":"tutorials/cred-loss/#Illustration-of-different-cred-based-decision-between-cred_std-to-MSE","page":"Credibility-based loss","title":"Illustration of different cred-based decision between cred_std to MSE","text":"","category":"section"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"Despite both mse and cred_std resulting in the same prediction, which matches the mean of the observations, the associated gain differs due to the volatility penalty.","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"The following illustrates a minimal scenario of 2 features, each with only 2 levels.","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"(Image: ) (Image: )\n ","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"config = EvoTreeRegressor(loss=:mse, nrounds=1, max_depth=2)\nmodel_mse = EvoTrees.fit(config, dtrain; target_name=\"y\")\n\nEvoTrees.Tree{EvoTrees.MSE, 1}\n - feat: [2, 0, 0]\n - cond_bin: UInt8[0x01, 0x00, 0x00]\n - gain: Float32[12113.845, 0.0, 0.0]\n - pred: Float32[0.0 -0.017858343 0.3391479]\n - split: Bool[1, 0, 0]","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"config = EvoTreeRegressor(loss=:cred_std, nrounds=1, max_depth=2)\nmodel_std = EvoTrees.fit(config, dtrain; target_name=\"y\")\n\nEvoTrees.Tree{EvoTrees.CredStd, 1}\n - feat: [1, 0, 0]\n - cond_bin: UInt8[0x02, 0x00, 0x00]\n - gain: Float32[8859.706, 0.0, 0.0]\n - pred: Float32[0.0 0.07375729 -0.07375729]\n - split: Bool[1, 0, 0]","category":"page"},{"location":"tutorials/cred-loss/#Benchmarks","page":"Credibility-based loss","title":"Benchmarks","text":"","category":"section"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"From MLBenchmarks.jl.","category":"page"},{"location":"tutorials/cred-loss/","page":"Credibility-based loss","title":"Credibility-based loss","text":"model metric mse cred_var cred_std\nboston mse 6.3 5.95 5.43\nboston gini 0.945 0.947 0.952\nyear mse 74.9 74.6 74.2\nyear gini 0.662 0.664 0.661\nmsrank mse 0.55 0.551 0.549\nmsrank ndcg 0.511 0.509 0.51\nyahoo mse 0.565 0.589 0.568\nyahoo ndcg 0.795 0.787 0.794","category":"page"},{"location":"internals/#Internal-API","page":"Internals","title":"Internal API","text":"","category":"section"},{"location":"internals/#General","page":"Internals","title":"General","text":"","category":"section"},{"location":"internals/#EvoTrees.TrainNode","page":"Internals","title":"EvoTrees.TrainNode","text":"TrainNode{S,V,M}\n\nCarries training information for a given tree node\n\n\n\n\n\n","category":"type"},{"location":"internals/#EvoTrees.EvoTree","page":"Internals","title":"EvoTrees.EvoTree","text":"EvoTree{L,K}\n\nAn EvoTree holds the structure of a fitted gradient-boosted tree.\n\nFields\n\ntrees::Vector{Tree{L,K}}\ninfo::Dict\n\nEvoTree acts as a functor to perform inference on input data: \n\npred = (m::EvoTree; ntree_limit=length(m.trees))(x)\n\n\n\n\n\n","category":"type"},{"location":"internals/#EvoTrees.check_parameter","page":"Internals","title":"EvoTrees.check_parameter","text":"check_parameter(::Type{<:T}, value, min_value::Real, max_value::Real, label::Symbol) where {T<:Number}\n\nCheck model parameter if it's valid\n\n\n\n\n\n","category":"function"},{"location":"internals/#EvoTrees.check_args","page":"Internals","title":"EvoTrees.check_args","text":"check_args(args::Dict{Symbol,Any})\n\nCheck model arguments if they are valid\n\n\n\n\n\ncheck_args(model::EvoTypes)\n\nCheck model arguments if they are valid (eg, after mutation when tuning hyperparams) Note: does not check consistency of model type and loss selected\n\n\n\n\n\n","category":"function"},{"location":"internals/#Training-utils","page":"Internals","title":"Training utils","text":"","category":"section"},{"location":"internals/#EvoTrees.init","page":"Internals","title":"EvoTrees.init","text":"init(\n    params::EvoTypes,\n    dtrain,\n    device::Type{<:Device}=CPU;\n    target_name,\n    feature_names=nothing,\n    weight_name=nothing,\n    offset_name=nothing\n)\n\nInitialise EvoTree\n\n\n\n\n\ninit(\n    params::EvoTypes,\n    x_train::AbstractMatrix,\n    y_train::AbstractVector,\n    device::Type{<:Device}=CPU;\n    feature_names=nothing,\n    w_train=nothing,\n    offset_train=nothing\n)\n\nInitialise EvoTree\n\n\n\n\n\n","category":"function"},{"location":"internals/#EvoTrees.grow_evotree!","page":"Internals","title":"EvoTrees.grow_evotree!","text":"grow_evotree!(evotree::EvoTree{L,K}, cache, params::EvoTypes) where {L,K}\n\nGiven a instantiate\n\n\n\n\n\n","category":"function"},{"location":"internals/#EvoTrees.get_best_split","page":"Internals","title":"EvoTrees.get_best_split","text":"get_best_split(\n    ::Type{L},\n    node::TrainNode,\n    js,\n    params::EvoTypes,\n    feattypes::Vector{Bool},\n    monotone_constraints,\n)\n\nGeneric fallback\n\n\n\n\n\n","category":"function"},{"location":"internals/#EvoTrees.update_gains!","page":"Internals","title":"EvoTrees.update_gains!","text":"update_gains!(\n    ::Type{L},\n    node::TrainNode,\n    js,\n    params::EvoTypes,\n    feattypes::Vector{Bool},\n    monotone_constraints,\n)\n\n\n\n\n\n","category":"function"},{"location":"internals/#EvoTrees.predict!","page":"Internals","title":"EvoTrees.predict!","text":"predict!(pred::Matrix, tree::Tree, X)\n\nGeneric fallback to add predictions of tree to existing pred matrix.\n\n\n\n\n\n","category":"function"},{"location":"internals/#EvoTrees.subsample","page":"Internals","title":"EvoTrees.subsample","text":"subsample(left::AbstractVector, is::AbstractVector, mask_cond::AbstractVector{UInt8}, rowsample::AbstractFloat, rng)\n\nReturns a view of selected rows ids.\n\n\n\n\n\n","category":"function"},{"location":"internals/#EvoTrees.split_set_chunk!","page":"Internals","title":"EvoTrees.split_set_chunk!","text":"Multi-threaded split_set!\n    Take a view into left and right placeholders. Right ids are assigned at the end of the length of the current node set.\n\n\n\n\n\n","category":"function"},{"location":"internals/#Histogram","page":"Internals","title":"Histogram","text":"","category":"section"},{"location":"internals/#EvoTrees.get_edges","page":"Internals","title":"EvoTrees.get_edges","text":"get_edges(X::AbstractMatrix{T}; feature_names, nbins, rng=Random.TaskLocalRNG()) where {T}\nget_edges(df; feature_names, nbins, rng=Random.TaskLocalRNG())\n\nGet the histogram breaking points of the feature data.\n\n\n\n\n\n","category":"function"},{"location":"internals/#EvoTrees.binarize","page":"Internals","title":"EvoTrees.binarize","text":"binarize(X::AbstractMatrix; feature_names, edges)\nbinarize(df; feature_names, edges)\n\nTransform feature data into a UInt8 binarized matrix.\n\n\n\n\n\n","category":"function"},{"location":"internals/#EvoTrees.update_hist!","page":"Internals","title":"EvoTrees.update_hist!","text":"update_hist!\n    GradientRegression\n\n\n\n\n\nupdate_hist!\n    MLE2P\n\n\n\n\n\nupdate_hist!\n\nGeneric fallback - Softmax\n\n\n\n\n\n","category":"function"},{"location":"api/#Public-API","page":"Public","title":"Public API","text":"","category":"section"},{"location":"api/#fit_evotree","page":"Public","title":"fit_evotree","text":"","category":"section"},{"location":"api/#MLJModelInterface.fit","page":"Public","title":"MLJModelInterface.fit","text":"fit(\n    params::EvoTypes, \n    dtrain;\n    target_name,\n    feature_names=nothing,\n    weight_name=nothing,\n    offset_name=nothing,\n    deval=nothing,\n    print_every_n=9999,\n    verbosity=1\n\n)\n\nMain training function. Performs model fitting given configuration params, dtrain, target_name and other optional kwargs. \n\nArguments\n\nparams::EvoTypes: configuration info providing hyper-paramters. EvoTypes can be one of: \nEvoTreeRegressor\nEvoTreeClassifier\nEvoTreeCount\nEvoTreeMLE\ndtrain: A Tables compatible training data (named tuples, DataFrame...) containing features and target variables. \n\nKeyword arguments\n\ntarget_name: name of the target variable. \nfeature_names = nothing: the names dtrain variables to use as features. If not provided, it deafults to all variables that aren't one of target, weight or offset`.\nweight_name = nothing: name of the variable containing weights. If nothing, common weights on one will be used.\noffset_name = nothing: name of the offset variable.\ndeval: A Tables compatible evaluation data containing features and target variables. \nprint_every_n: sets at which frequency logging info should be printed. \nverbosity: set to 1 to print logging info during training.\n\n\n\n\n\nfit(\n    params::EvoTypes{L};\n    x_train::AbstractMatrix, \n    y_train::AbstractVector, \n    w_train=nothing, \n    offset_train=nothing,\n    x_eval=nothing, \n    y_eval=nothing, \n    w_eval=nothing, \n    offset_eval=nothing,\n    feature_names=nothing,\n    early_stopping_rounds=9999,\n    print_every_n=9999,\n    verbosity=1)\n\nMain training function. Performs model fitting given configuration params, x_train, y_train and other optional kwargs. \n\nArguments\n\nparams::EvoTypes: configuration info providing hyper-paramters. EvoTypes can be one of: \nEvoTreeRegressor\nEvoTreeClassifier\nEvoTreeCount\nEvoTreeMLE\n\nKeyword arguments\n\nx_train::Matrix: training data of size [#observations, #features]. \ny_train::Vector: vector of train targets of length #observations.\nw_train::Vector: vector of train weights of length #observations. If nothing, a vector of ones is assumed.\noffset_train::VecOrMat: offset for the training data. Should match the size of the predictions.\nx_eval::Matrix: evaluation data of size [#observations, #features]. \ny_eval::Vector: vector of evaluation targets of length #observations.\nw_eval::Vector: vector of evaluation weights of length #observations. Defaults to nothing (assumes a vector of 1s).\noffset_eval::VecOrMat: evaluation data offset. Should match the size of the predictions.\nfeature_names = nothing: the names of the x_train features. If provided, should be a vector of string with length(feature_names) = size(x_train, 2).\nprint_every_n: sets at which frequency logging info should be printed. \nverbosity: set to 1 to print logging info during training.\n\n\n\n\n\n","category":"function"},{"location":"api/#predict","page":"Public","title":"predict","text":"","category":"section"},{"location":"api/#MLJModelInterface.predict","page":"Public","title":"MLJModelInterface.predict","text":"predict(m::EvoTree, data; ntree_limit=length(m.trees), device=:cpu)\n\nPredictions from an EvoTree model - sums the predictions from all trees composing the model. Use ntree_limit=N to only predict with the first N trees.\n\n\n\n\n\n","category":"function"},{"location":"api/#importance","page":"Public","title":"importance","text":"","category":"section"},{"location":"api/#EvoTrees.importance","page":"Public","title":"EvoTrees.importance","text":"importance(model::EvoTree; feature_names=model.info[:feature_names])\n\nSorted normalized feature importance based on loss function gain. Feature names associated to the model are stored in model.info[:feature_names] as a string Vector and can be updated at any time. Eg: model.info[:feature_names] = new_feature_names_vec.\n\n\n\n\n\n","category":"function"},{"location":"api/#Deprecated","page":"Public","title":"Deprecated","text":"","category":"section"},{"location":"api/#EvoTrees.fit_evotree","page":"Public","title":"EvoTrees.fit_evotree","text":"fit_evotree(\n    params::EvoTypes, \n    dtrain;\n    target_name,\n    feature_names=nothing,\n    weight_name=nothing,\n    offset_name=nothing,\n    deval=nothing,\n    print_every_n=9999,\n    verbosity=1\n\n)\n\nMain training function. Performs model fitting given configuration params, dtrain, target_name and other optional kwargs. \n\nArguments\n\nparams::EvoTypes: configuration info providing hyper-paramters. EvoTypes can be one of: \nEvoTreeRegressor\nEvoTreeClassifier\nEvoTreeCount\nEvoTreeMLE\ndtrain: A Tables compatible training data (named tuples, DataFrame...) containing features and target variables. \n\nKeyword arguments\n\ntarget_name: name of target variable. \nfeature_names = nothing: the names dtrain variables to use as features. If not provided, it deafults to all variables that aren't one of target, weight or offset`.\nweight_name = nothing: name of the variable containing weights. If nothing, common weights on one will be used.\noffset_name = nothing: name of the offset variable.\ndeval: A Tables compatible evaluation data containing features and target variables. \nprint_every_n: sets at which frequency logging info should be printed. \nverbosity: set to 1 to print logging info during training.\n\n\n\n\n\nfit_evotree(\n    params::EvoTypes{L};\n    x_train::AbstractMatrix, \n    y_train::AbstractVector, \n    w_train=nothing, \n    offset_train=nothing,\n    x_eval=nothing, \n    y_eval=nothing, \n    w_eval=nothing, \n    offset_eval=nothing,\n    feature_names=nothing,\n    early_stopping_rounds=9999,\n    print_every_n=9999,\n    verbosity=1)\n\nMain training function. Performs model fitting given configuration params, x_train, y_train and other optional kwargs. \n\nArguments\n\nparams::EvoTypes: configuration info providing hyper-paramters. EvoTypes can be one of: \nEvoTreeRegressor\nEvoTreeClassifier\nEvoTreeCount\nEvoTreeMLE\n\nKeyword arguments\n\nx_train::Matrix: training data of size [#observations, #features]. \ny_train::Vector: vector of train targets of length #observations.\nw_train::Vector: vector of train weights of length #observations. If nothing, a vector of ones is assumed.\noffset_train::VecOrMat: offset for the training data. Should match the size of the predictions.\nx_eval::Matrix: evaluation data of size [#observations, #features]. \ny_eval::Vector: vector of evaluation targets of length #observations.\nw_eval::Vector: vector of evaluation weights of length #observations. Defaults to nothing (assumes a vector of 1s).\noffset_eval::VecOrMat: evaluation data offset. Should match the size of the predictions.\nfeature_names = nothing: the names of the x_train features. If provided, should be a vector of string with length(feature_names) = size(x_train, 2).\nprint_every_n: sets at which frequency logging info should be printed. \nverbosity: set to 1 to print logging info during training.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/classification-iris/#Classification-on-Iris-dataset","page":"Classification - IRIS","title":"Classification on Iris dataset","text":"","category":"section"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"We will use the iris dataset, which is included in the MLDatasets package. This dataset consists of measurements of the sepal length, sepal width, petal length, and petal width for three different types of iris flowers: Setosa, Versicolor, and Virginica.","category":"page"},{"location":"tutorials/classification-iris/#Getting-started","page":"Classification - IRIS","title":"Getting started","text":"","category":"section"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"To begin, we will load the required packages and the dataset:","category":"page"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"using EvoTrees\nusing MLDatasets\nusing DataFrames\nusing Statistics: mean\nusing CategoricalArrays\nusing Random\n\ndf = MLDatasets.Iris().dataframe","category":"page"},{"location":"tutorials/classification-iris/#Preprocessing","page":"Classification - IRIS","title":"Preprocessing","text":"","category":"section"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"Before we can train our model, we need to preprocess the dataset. We will convert the class variable, which specifies the type of iris flower, into a categorical variable.","category":"page"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"Random.seed!(123)\n\ndf[!, :class] = categorical(df[!, :class])\ntarget_name = \"class\"\nfeature_names = setdiff(names(df), [target_name])\n\ntrain_ratio = 0.8\ntrain_indices = randperm(nrow(df))[1:Int(train_ratio * nrow(df))]\n\ndtrain = df[train_indices, :]\ndeval = df[setdiff(1:nrow(df), train_indices), :]","category":"page"},{"location":"tutorials/classification-iris/#Training","page":"Classification - IRIS","title":"Training","text":"","category":"section"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"Now we are ready to train our model. We will first define a model configuration using the EvoTreeClassifier model constructor.  Then, we'll use fit_evotree to train a boosted tree model. We'll pass optional x_eval and y_eval arguments, which enable the usage of early stopping. ","category":"page"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"config = EvoTreeClassifier(\n    nrounds=200, \n    early_stopping_rounds=10,\n    eta=0.05, \n    max_depth=5, \n    lambda=0.1, \n    rowsample=0.8, \n    colsample=0.8)\n\nmodel = EvoTrees.fit(config, dtrain;\n    target_name,\n    feature_names,\n    deval,\n    print_every_n=10)","category":"page"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"Finally, we can get predictions by passing training and testing data to our model. We can then evaluate the accuracy of our model, which should be near 100% for this simple classification problem. ","category":"page"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"pred_train = model(dtrain)\nidx_train = [findmax(row)[2] for row in eachrow(pred_train)]\n\npred_eval = model(deval)\nidx_eval = [findmax(row)[2] for row in eachrow(pred_eval)]","category":"page"},{"location":"tutorials/classification-iris/","page":"Classification - IRIS","title":"Classification - IRIS","text":"julia> mean(idx_train .== levelcode.(dtrain[:, target_name]))\n1.0\n\njulia> mean(idx_eval .== levelcode.(deval[:, target_name]))\n0.9333333333333333","category":"page"},{"location":"tutorials/regression-boston/#Regression-on-Boston-Housing-Dataset","page":"Regression - Boston","title":"Regression on Boston Housing Dataset","text":"","category":"section"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"We will use the Boston Housing dataset, which is included in the MLDatasets package. It's derived from information collected by the U.S. Census Service concerning housing in the area of Boston. Target variable represents the median housing value.","category":"page"},{"location":"tutorials/regression-boston/#Getting-started","page":"Regression - Boston","title":"Getting started","text":"","category":"section"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"To begin, we will load the required packages and the dataset:","category":"page"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"using EvoTrees\nusing MLDatasets\nusing DataFrames\nusing Statistics: mean\nusing CategoricalArrays\nusing Random\n\ndf = MLDatasets.BostonHousing().dataframe","category":"page"},{"location":"tutorials/regression-boston/#Preprocessing","page":"Regression - Boston","title":"Preprocessing","text":"","category":"section"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"Before we can train our model, we need to preprocess the dataset. We will split our data according to train and eval indices, and separate features from the target variable.","category":"page"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"Random.seed!(123)\n\ntrain_ratio = 0.8\ntrain_indices = randperm(nrow(df))[1:Int(round(train_ratio * nrow(df)))]\n\ntrain_data = df[train_indices, :]\neval_data = df[setdiff(1:nrow(df), train_indices), :]\n\nx_train, y_train = Matrix(train_data[:, Not(:MEDV)]), train_data[:, :MEDV]\nx_eval, y_eval = Matrix(eval_data[:, Not(:MEDV)]), eval_data[:, :MEDV]","category":"page"},{"location":"tutorials/regression-boston/#Training","page":"Regression - Boston","title":"Training","text":"","category":"section"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"Now we are ready to train our model. We will first define a model configuration using the EvoTreeRegressor model constructor.  Then, we'll use fit_evotree to train a boosted tree model. We'll pass optional x_eval and y_eval arguments, which enable the usage of early stopping. ","category":"page"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"config = EvoTreeRegressor(\n    nrounds=200, \n    early_stopping_rounds=10,\n    eta=0.1, \n    max_depth=4, \n    lambda=0.1, \n    rowsample=0.9, \n    colsample=0.9)\n\nmodel = EvoTrees.fit(config;\n    x_train, y_train,\n    x_eval, y_eval,\n    print_every_n=10)","category":"page"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"Finally, we can get predictions by passing training and testing data to our model. We can then apply various evaluation metric, such as the MAE (mean absolute error):  ","category":"page"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"pred_train = model(x_train)\npred_eval = model(x_eval)","category":"page"},{"location":"tutorials/regression-boston/","page":"Regression - Boston","title":"Regression - Boston","text":"julia> mean(abs.(pred_train .- y_train))\n1.056997874224627\n\njulia> mean(abs.(pred_eval .- y_eval))\n2.3298767665825264","category":"page"},{"location":"#EvoTrees.jl","page":"Introduction","title":"EvoTrees.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"A Julia implementation of boosted trees with CPU and GPU support. Efficient histogram based algorithms with support for multiple loss functions, including various regressions, multi-classification and Gaussian max likelihood. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"See the examples-API section to get started using the internal API, or examples-MLJ to use within the MLJ framework.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Complete details about hyper-parameters are found in the Models section.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"R binding available.","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Latest:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> Pkg.add(url=\"https://github.com/Evovest/EvoTrees.jl\")","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"From General Registry:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> Pkg.add(\"EvoTrees\")","category":"page"},{"location":"#Quick-start","page":"Introduction","title":"Quick start","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"A model configuration must first be defined, using one of the model constructor: ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"EvoTreeRegressor\nEvoTreeClassifier\nEvoTreeCount\nEvoTreeMLE","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Then fitting can be performed using fit_evotree. 2 broad methods are supported: Matrix and Tables based inputs. Optional kwargs can be used to specify eval data on which to track eval metric and perform early stopping. Look at the docs for more details on available hyper-parameters for each of the above constructors and other options for training.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Predictions are obtained by passing features data to the model. Model acts as a functor, ie. it's a struct containing the fitted model as well as a function generating the prediction of that model for the features argument. ","category":"page"},{"location":"#Tables-and-DataFrames-input","page":"Introduction","title":"Tables and DataFrames input","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"When using a Tables compatible input such as DataFrames, features with element type Real (incl. Bool) and Categorical are automatically recognized as input features. Alternatively, feature_names kwarg can be used. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Categorical features are treated accordingly by the algorithm. Ordered variables will be treated as numerical features, using â‰¤ split rule, while unordered variables are using ==. Support is currently limited to a maximum of 255 levels. Bool variables are treated as unordered, 2-levels cat variables.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using EvoTrees\nusing DataFrames\n\nconfig = EvoTreeRegressor(\n    loss=:mse, \n    nrounds=100, \n    max_depth=6,\n    nbins=32,\n    eta=0.1)\n\nx_train, y_train = rand(1_000, 10), rand(1_000)\ndtrain = DataFrame(x_train, :auto)\ndtrain.y .= y_train\nm = fit_evotree(config, dtrain; target_name=\"y\");\nm = fit_evotree(config, dtrain; target_name=\"y\", feature_names=[\"x1\", \"x3\"]); # to only use specified features\npreds = m(dtrain)","category":"page"},{"location":"#Matrix-features-input","page":"Introduction","title":"Matrix features input","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"using EvoTrees\n\nconfig = EvoTreeRegressor(\n    loss=:mse, \n    nrounds=100, \n    max_depth=6,\n    nbins=32,\n    eta=0.1)\n\nx_train, y_train = rand(1_000, 10), rand(1_000)\nm = fit_evotree(config; x_train, y_train)\npreds = m(x_train)","category":"page"},{"location":"#GPU-Acceleration","page":"Introduction","title":"GPU Acceleration","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"EvoTrees supports training and inference on Nvidia GPU's with CUDA.jl. Note that on Julia â‰¥ 1.9 CUDA support is only enabled when CUDA.jl is installed and loaded, by another package or explicitly with e.g.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using CUDA","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"If running on a CUDA enabled machine, training and inference on GPU can be triggered through the device kwarg passed to the learner's constructor: ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"config = EvoTreeRegressor(\n    loss=:mse, \n    device=:gpu\n)\n\nm = fit_evotree(config, dtrain; target_name=\"y\");\np = m(dtrain; device=:gpu)","category":"page"},{"location":"#Reproducibility","page":"Introduction","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"EvoTrees models trained on cpu can be fully reproducible.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Models of the gradient boosting family typically involve some stochasticity.  In EvoTrees, this primarily concern the the 2 subsampling parameters rowsample and colsample. The other stochastic operation happens at model initialisation when the features are binarized to allow for fast histogram construction: a random subsample of 1_000 * nbins is used to compute the breaking points. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"These random parts of the algorithm can be deterministically reproduced on cpu by specifying an rng to the model constructor. rng can be an integer (ex: 123) or a random generator (ex: Random.Xoshiro(123)).  If no rng is specified, 123 is used by default. When an integer rng is used, a Random.MersenneTwister generator will be created by the EvoTrees's constructor. Otherwise, the provided random generator will be used.  ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Consequently, the following m1 and m2 models will be identical:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"config = EvoTreeRegressor(rowsample=0.5, rng=123)\nm1 = fit_evotree(config, dtrain; target_name=\"y\");\nconfig = EvoTreeRegressor(rowsample=0.5, rng=123)\nm2 = fit_evotree(config, dtrain; target_name=\"y\");","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"However, the following m1 and m2 models won't be because the there's stochasticity involved in the model from rowsample and the random generator in the config isn't reset between the fits:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"config = EvoTreeRegressor(rowsample=0.5, rng=123)\nm1 = fit_evotree(config, dtrain; target_name=\"y\");\nm2 = fit_evotree(config, dtrain; target_name=\"y\");","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Note that in presence of multiple identical or very highly correlated features, model may not be reproducible if features are permuted since in situation where 2 features provide identical gains, the first one will be selected. Therefore, if the identity relationship doesn't hold on new data, different predictions will be returned from models trained on different features order. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"At the moment, there's no reproducibility guarantee on GPU, although this may change in the future. ","category":"page"},{"location":"#Missing-values","page":"Introduction","title":"Missing values","text":"","category":"section"},{"location":"#Features","page":"Introduction","title":"Features","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"EvoTrees does not handle features having missing values. Proper preprocessing of the data is therefore needed (and a general good practice regardless of the ML model used).","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This includes situations where values may be all non-missing, but where the eltype is Union{Missing,Float64} or Any for example. A conversion using identity is then recommended: ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> x = Vector{Union{Missing, Float64}}([1, 2])\n2-element Vector{Union{Missing, Float64}}:\n 1.0\n 2.0\n\njulia> identity.(x)\n2-element Vector{Float64}:\n 1.0\n 2.0","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"For dealing with numerical or ordered categorical features containing missing values, a common approach is to first create an Bool variable capturing the info on whether a value is missing:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using DataFrames\ntransform!(df, :my_feat => ByRow(ismissing) => :my_feat_ismissing)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Then, the missing values can be imputed (replaced by some default values such as mean or median, or using a more sophisticated approach such as predictions from another model):","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"transform!(df, :my_feat => (x -> coalesce.(x, median(skipmissing(x)))) => :my_feat)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"For unordered categorical variables, a recode of the missing into a non missing level is sufficient:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using CategoricalArrays\njulia> x = categorical([\"a\", \"b\", missing])\n3-element CategoricalArray{Union{Missing, String},1,UInt32}:\n \"a\"\n \"b\"\n missing\n\njulia> x = recode(x, missing => \"missing value\")\n3-element CategoricalArray{String,1,UInt32}:\n \"a\"\n \"b\"\n \"missing value\"","category":"page"},{"location":"#Target","page":"Introduction","title":"Target","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Target variable must have its element type <:Real. Only exception is for EvoTreeClassifier for which CategoricalValue, Integer, String and Char are supported.","category":"page"},{"location":"#Save/Load","page":"Introduction","title":"Save/Load","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"EvoTrees.save(m, \"data/model.bson\")\nm = EvoTrees.load(\"data/model.bson\");","category":"page"}]
}
