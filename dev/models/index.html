<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Models · EvoTrees.jl</title><meta name="title" content="Models · EvoTrees.jl"/><meta property="og:title" content="Models · EvoTrees.jl"/><meta property="twitter:title" content="Models · EvoTrees.jl"/><meta name="description" content="Documentation for EvoTrees.jl."/><meta property="og:description" content="Documentation for EvoTrees.jl."/><meta property="twitter:description" content="Documentation for EvoTrees.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="EvoTrees.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li class="is-active"><a class="tocitem" href>Models</a><ul class="internal"><li><a class="tocitem" href="#EvoTreeRegressor"><span>EvoTreeRegressor</span></a></li><li><a class="tocitem" href="#EvoTreeClassifier"><span>EvoTreeClassifier</span></a></li><li><a class="tocitem" href="#EvoTreeCount"><span>EvoTreeCount</span></a></li><li><a class="tocitem" href="#EvoTreeMLE"><span>EvoTreeMLE</span></a></li><li><a class="tocitem" href="#EvoTreeGaussian"><span>EvoTreeGaussian</span></a></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../api/">Public</a></li><li><a class="tocitem" href="../internals/">Internals</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../tutorials/regression-boston/">Regression - Boston</a></li><li><a class="tocitem" href="../tutorials/logistic-regression-titanic/">Logistic Regression - Titanic</a></li><li><a class="tocitem" href="../tutorials/classification-iris/">Classification - IRIS</a></li><li><a class="tocitem" href="../tutorials/ranking-LTRC/">Ranking - Yahoo! LTRC</a></li><li><a class="tocitem" href="../tutorials/cred-loss/">Credibility-based loss</a></li><li><a class="tocitem" href="../tutorials/examples-API/">Internal API</a></li><li><a class="tocitem" href="../tutorials/examples-MLJ/">MLJ API</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Models</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/Evovest/EvoTrees.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/Evovest/EvoTrees.jl/blob/main/docs/src/models.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Models"><a class="docs-heading-anchor" href="#Models">Models</a><a id="Models-1"></a><a class="docs-heading-anchor-permalink" href="#Models" title="Permalink"></a></h1><h2 id="EvoTreeRegressor"><a class="docs-heading-anchor" href="#EvoTreeRegressor">EvoTreeRegressor</a><a id="EvoTreeRegressor-1"></a><a class="docs-heading-anchor-permalink" href="#EvoTreeRegressor" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="EvoTrees.EvoTreeRegressor"><a class="docstring-binding" href="#EvoTrees.EvoTreeRegressor"><code>EvoTrees.EvoTreeRegressor</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">EvoTreeRegressor(;kwargs...)</code></pre><p>A model type for constructing a EvoTreeRegressor, based on <a href="https://github.com/Evovest/EvoTrees.jl">EvoTrees.jl</a>, and implementing both an internal API and the MLJ model interface.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>loss=:mse</code>:         Loss to be be minimized during training. One of:<ul><li><code>:mse</code></li><li><code>:mae</code></li><li><code>:logloss</code></li><li><code>:gamma</code></li><li><code>:tweedie</code></li><li><code>:quantile</code></li><li><code>:cred_var</code>: <strong>experimental</strong> credibility-based gains, derived from ratio of spread to process variance.</li><li><code>:cred_std</code>: <strong>experimental</strong> credibility-based gains, derived from ratio of spread to process std deviation.</li></ul></li><li><code>metric</code>:     The evaluation metric used to track evaluation data and serves as a basis for early stopping. Supported metrics are: <ul><li><code>:mse</code>:     Mean-squared error. Adapted for general regression models.</li><li><code>:rmse</code>:    Root-mean-squared error. Adapted for general regression models.</li><li><code>:mae</code>:     Mean absolute error. Adapted for general regression models.</li><li><code>:logloss</code>: Adapted for <code>:logistic</code> regression models.</li><li><code>:poisson</code>: Poisson deviance. Adapted to <code>EvoTreeCount</code> count models.</li><li><code>:gamma</code>:   Gamma deviance. Adapted to regression problem on Gamma like, positively distributed targets.</li><li><code>:tweedie</code>: Tweedie deviance. Adapted to regression problem on Tweedie like, positively distributed targets with probability mass at <code>y == 0</code>.</li><li><code>:quantile</code>: Loss is an assymetric absolute error, where residuals are penalized as <code>alpha</code> or <code>(1-alpha)</code> according to their sign.</li><li><code>:gini</code>: The normalized Gini between pred and target</li></ul></li><li><code>early_stopping_rounds::Integer</code>: number of consecutive rounds without metric improvement after which fitting in stopped. </li><li><code>nrounds=100</code>:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be &gt;= 1.</li><li><code>eta=0.1</code>:               Learning rate. Each tree raw predictions are scaled by <code>eta</code> prior to be added to the stack of predictions. Must be &gt; 0. A lower <code>eta</code> results in slower learning, requiring a higher <code>nrounds</code> but typically improves model performance.   </li><li><code>L2=1.0</code>:               L2 regularization factor on aggregate gain. Must be &gt;= 0. Higher L2 can result in a more robust model.</li><li><code>lambda=0.0</code>:           L2 regularization factor on individual gain. Must be &gt;= 0. Higher lambda can result in a more robust model.</li><li><code>gamma=0.0</code>:            Minimum gain improvement needed to perform a node split. Higher gamma can result in a more robust model. Must be &gt;= 0.</li><li><code>alpha=0.5</code>:            Loss specific parameter in the [0, 1] range:                           - <code>:quantile</code>: target quantile for the regression.</li><li><code>max_depth=6</code>:          Maximum depth of a tree. Must be &gt;= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains <code>2^(N - 1)</code> terminal leaves and <code>2^(N - 1) - 1</code> split nodes. Compute cost is proportional to <code>2^max_depth</code>. Typical optimal values are in the 3 to 9 range.</li><li><code>min_weight=1.0</code>:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the <code>weights</code> vector. Must be &gt; 0.</li><li><code>rowsample=1.0</code>:        Proportion of rows that are sampled at each iteration to build the tree. Should be in <code>]0, 1]</code>.</li><li><code>colsample=1.0</code>:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be in <code>]0, 1]</code>.</li><li><code>nbins=64</code>:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.</li><li><code>monotone_constraints=Dict{Int, Int}()</code>: Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  Only <code>:linear</code>, <code>:logistic</code>, <code>:gamma</code> and <code>tweedie</code> losses are supported at the moment.</li><li><code>tree_type=:binary</code>    Tree structure to be used. One of:<ul><li><code>:binary</code>:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see <code>gamma</code>) stops further node splits.  </li><li><code>:oblivious</code>:    A common splitting condition is imposed to all nodes of a given depth. </li></ul></li><li><code>seed=123</code>:             An integer used as a seed to the random number generator.</li><li><code>device=:cpu</code>: Hardware device to use for computations. Can be either <code>:cpu</code> or <code>gpu</code>.</li></ul><p><strong>Internal API</strong></p><p>Do <code>config = EvoTreeRegressor()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeRegressor(loss=...).</p><p><strong>Training model</strong></p><p>A model is built using <a href="../api/#fit_evotree"><code>fit_evotree</code></a>:</p><pre><code class="language-julia hljs">model = fit_evotree(config; x_train, y_train, kwargs...)</code></pre><p><strong>Inference</strong></p><p>Predictions are obtained using <a href="../api/#predict"><code>predict</code></a> which returns a <code>Vector</code> of length <code>nobs</code>:</p><pre><code class="language-julia hljs">EvoTrees.predict(model, X)</code></pre><p>Alternatively, models act as a functor, returning predictions when called as a function with features as argument:</p><pre><code class="language-julia hljs">model(X)</code></pre><p><strong>MLJ Interface</strong></p><p>From MLJ, the type can be imported using:</p><pre><code class="language-julia hljs">EvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees</code></pre><p>Do <code>model = EvoTreeRegressor()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>EvoTreeRegressor(loss=...)</code>.</p><p><strong>Training model</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     <code>mach = machine(model, X, y)</code> where</p><ul><li><code>X</code>: any table of input features (eg, a <code>DataFrame</code>) whose columns each have one of the following element scitypes: <code>Continuous</code>, <code>Count</code>, or <code>&lt;:OrderedFactor</code>; check column scitypes with <code>schema(X)</code></li><li><code>y</code>: is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>&lt;:Continuous</code>; check the scitype with <code>scitype(y)</code></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions are deterministic.</li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>:fitresult</code>: The <code>GBTree</code> object returned by EvoTrees.jl fitting algorithm.</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>:features</code>: The names of the features encountered in training.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Internal API
using EvoTrees
config = EvoTreeRegressor(max_depth=5, nbins=32, nrounds=100)
nobs, nfeats = 1_000, 5
x_train, y_train = randn(nobs, nfeats), rand(nobs)
model = fit_evotree(config; x_train, y_train)
preds = EvoTrees.predict(model, x_train)</code></pre><pre><code class="language-julia hljs"># MLJ Interface
using MLJ
EvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees
model = EvoTreeRegressor(max_depth=5, nbins=32, nrounds=100)
X, y = @load_boston
mach = machine(model, X, y) |&gt; fit!
preds = predict(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Evovest/EvoTrees.jl/blob/63b0a6574da444fcc0e51443809a2a88869bb573/src/MLJ.jl#L156-L294">source</a></section></details></article><h2 id="EvoTreeClassifier"><a class="docs-heading-anchor" href="#EvoTreeClassifier">EvoTreeClassifier</a><a id="EvoTreeClassifier-1"></a><a class="docs-heading-anchor-permalink" href="#EvoTreeClassifier" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="EvoTrees.EvoTreeClassifier"><a class="docstring-binding" href="#EvoTrees.EvoTreeClassifier"><code>EvoTrees.EvoTreeClassifier</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">EvoTreeClassifier(;kwargs...)</code></pre><p>A model type for constructing a EvoTreeClassifier, based on <a href="https://github.com/Evovest/EvoTrees.jl">EvoTrees.jl</a>, and implementing both an internal API and the MLJ model interface. EvoTreeClassifier is used to perform multi-class classification, using cross-entropy loss.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>early_stopping_rounds::Integer</code>: number of consecutive rounds without metric improvement after which fitting in stopped. </li><li><code>nrounds=100</code>:          Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be &gt;= 1.</li><li><code>eta=0.1</code>:              Learning rate. Each tree raw predictions are scaled by <code>eta</code> prior to be added to the stack of predictions. Must be &gt; 0. A lower <code>eta</code> results in slower learning, requiring a higher <code>nrounds</code> but typically improves model performance.  </li><li><code>L2=1.0</code>:               L2 regularization factor on aggregate gain. Must be &gt;= 0. Higher L2 can result in a more robust model.</li><li><code>lambda=0.0</code>:           L2 regularization factor on individual gain. Must be &gt;= 0. Higher lambda can result in a more robust model.</li><li><code>gamma=0.0</code>:            Minimum gain improvement needed to perform a node split. Higher gamma can result in a more robust model. Must be &gt;= 0.</li><li><code>max_depth=6</code>:          Maximum depth of a tree. Must be &gt;= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains <code>2^(N - 1)</code> terminal leaves and <code>2^(N - 1) - 1</code> split nodes. Compute cost is proportional to <code>2^max_depth</code>. Typical optimal values are in the 3 to 9 range.</li><li><code>min_weight=1.0</code>:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the <code>weights</code> vector. Must be &gt; 0.</li><li><code>rowsample=1.0</code>:        Proportion of rows that are sampled at each iteration to build the tree. Should be in <code>]0, 1]</code>.</li><li><code>colsample=1.0</code>:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be in <code>]0, 1]</code>.</li><li><code>nbins=64</code>:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.</li><li><code>tree_type=:binary</code>     Tree structure to be used. One of:<ul><li><code>:binary</code>:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see <code>gamma</code>) stops further node splits.  </li><li><code>:oblivious</code>:    A common splitting condition is imposed to all nodes of a given depth. </li></ul></li><li><code>seed=123</code>:             An integer used as a seed to the random number generator.</li><li><code>device=:cpu</code>:          Hardware device to use for computations. Can be either <code>:cpu</code> or <code>:gpu</code>.</li></ul><p><strong>Internal API</strong></p><p>Do <code>config = EvoTreeClassifier()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeClassifier(max_depth=...).</p><p><strong>Training model</strong></p><p>A model is built using <a href="../api/#fit_evotree"><code>fit_evotree</code></a>:</p><pre><code class="language-julia hljs">model = fit_evotree(config; x_train, y_train, kwargs...)</code></pre><p><strong>Inference</strong></p><p>Predictions are obtained using <a href="../api/#predict"><code>predict</code></a> which returns a <code>Matrix</code> of size <code>[nobs, K]</code> where <code>K</code> is the number of classes:</p><pre><code class="language-julia hljs">EvoTrees.predict(model, X)</code></pre><p>Alternatively, models act as a functor, returning predictions when called as a function with features as argument:</p><pre><code class="language-julia hljs">model(X)</code></pre><p><strong>MLJ</strong></p><p>From MLJ, the type can be imported using:</p><pre><code class="language-julia hljs">EvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees</code></pre><p>Do <code>model = EvoTreeClassifier()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>EvoTreeClassifier(loss=...)</code>.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-julia hljs">mach = machine(model, X, y)</code></pre><p>where</p><ul><li><code>X</code>: any table of input features (eg, a <code>DataFrame</code>) whose columns each have one of the following element scitypes: <code>Continuous</code>, <code>Count</code>, or <code>&lt;:OrderedFactor</code>; check column scitypes with <code>schema(X)</code></li><li><code>y</code>: is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>&lt;:Multiclas</code> or <code>&lt;:OrderedFactor</code>; check the scitype with <code>scitype(y)</code></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Operations</strong></p><ul><li><p><code>predict(mach, Xnew)</code>: return predictions of the target given features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions are probabilistic.</p></li><li><p><code>predict_mode(mach, Xnew)</code>: returns the mode of each of the prediction above.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>:fitresult</code>: The <code>GBTree</code> object returned by EvoTrees.jl fitting algorithm.</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>:features</code>: The names of the features encountered in training.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Internal API
using EvoTrees
config = EvoTreeClassifier(max_depth=5, nbins=32, nrounds=100)
nobs, nfeats = 1_000, 5
x_train, y_train = randn(nobs, nfeats), rand(1:3, nobs)
model = fit_evotree(config; x_train, y_train)
preds = EvoTrees.predict(model, x_train)</code></pre><pre><code class="language-julia hljs"># MLJ Interface
using MLJ
EvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees
model = EvoTreeClassifier(max_depth=5, nbins=32, nrounds=100)
X, y = @load_iris
mach = machine(model, X, y) |&gt; fit!
preds = predict(mach, X)
preds = predict_mode(mach, X)</code></pre><p>See also <a href="https://github.com/Evovest/EvoTrees.jl">EvoTrees.jl</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Evovest/EvoTrees.jl/blob/63b0a6574da444fcc0e51443809a2a88869bb573/src/MLJ.jl#L298-L422">source</a></section></details></article><h2 id="EvoTreeCount"><a class="docs-heading-anchor" href="#EvoTreeCount">EvoTreeCount</a><a id="EvoTreeCount-1"></a><a class="docs-heading-anchor-permalink" href="#EvoTreeCount" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="EvoTrees.EvoTreeCount"><a class="docstring-binding" href="#EvoTrees.EvoTreeCount"><code>EvoTrees.EvoTreeCount</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">EvoTreeCount(;kwargs...)</code></pre><p>A model type for constructing a EvoTreeCount, based on <a href="https://github.com/Evovest/EvoTrees.jl">EvoTrees.jl</a>, and implementing both an internal API the MLJ model interface. EvoTreeCount is used to perform Poisson probabilistic regression on count target.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>early_stopping_rounds::Integer</code>: number of consecutive rounds without metric improvement after which fitting in stopped. </li><li><code>nrounds=100</code>:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be &gt;= 1.</li><li><code>eta=0.1</code>:              Learning rate. Each tree raw predictions are scaled by <code>eta</code> prior to be added to the stack of predictions. Must be &gt; 0. A lower <code>eta</code> results in slower learning, requiring a higher <code>nrounds</code> but typically improves model performance.  </li><li><code>L2=1.0</code>:               L2 regularization factor on aggregate gain. Must be &gt;= 0. Higher L2 can result in a more robust model.</li><li><code>lambda=0.0</code>:           L2 regularization factor on individual gain. Must be &gt;= 0. Higher lambda can result in a more robust model.</li><li><code>gamma=0.0</code>:            Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model.</li><li><code>max_depth=6</code>:          Maximum depth of a tree. Must be &gt;= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains <code>2^(N - 1)</code> terminal leaves and <code>2^(N - 1) - 1</code> split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.</li><li><code>min_weight=1.0</code>:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the <code>weights</code> vector. Must be &gt; 0.</li><li><code>rowsample=1.0</code>:        Proportion of rows that are sampled at each iteration to build the tree. Should be <code>]0, 1]</code>.</li><li><code>colsample=1.0</code>:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be <code>]0, 1]</code>.</li><li><code>nbins=64</code>:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.</li><li><code>monotone_constraints=Dict{Int, Int}()</code>: Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).</li><li><code>tree_type=:binary</code>     Tree structure to be used. One of:<ul><li><code>:binary</code>:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see <code>gamma</code>) stops further node splits.  </li><li><code>:oblivious</code>:    A common splitting condition is imposed to all nodes of a given depth. </li></ul></li><li><code>seed=123</code>:             An integer used as a seed to the random number generator.</li><li><code>device=:cpu</code>:          Hardware device to use for computations. Can be either <code>:cpu</code> or <code>:gpu</code>.</li></ul><p><strong>Internal API</strong></p><p>Do <code>config = EvoTreeCount()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeCount(max_depth=...).</p><p><strong>Training model</strong></p><p>A model is built using <a href="../api/#fit_evotree"><code>fit_evotree</code></a>:</p><pre><code class="language-julia hljs">model = fit_evotree(config; x_train, y_train, kwargs...)</code></pre><p><strong>Inference</strong></p><p>Predictions are obtained using <a href="../api/#predict"><code>predict</code></a> which returns a <code>Vector</code> of length <code>nobs</code>:</p><pre><code class="language-julia hljs">EvoTrees.predict(model, X)</code></pre><p>Alternatively, models act as a functor, returning predictions when called as a function with features as argument:</p><pre><code class="language-julia hljs">model(X)</code></pre><p><strong>MLJ</strong></p><p>From MLJ, the type can be imported using:</p><pre><code class="language-julia hljs">EvoTreeCount = @load EvoTreeCount pkg=EvoTrees</code></pre><p>Do <code>model = EvoTreeCount()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>EvoTreeCount(loss=...)</code>.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with     mach = machine(model, X, y) where</p><ul><li><code>X</code>: any table of input features (eg, a <code>DataFrame</code>) whose columns each have one of the following element scitypes: <code>Continuous</code>, <code>Count</code>, or <code>&lt;:OrderedFactor</code>; check column scitypes with <code>schema(X)</code></li><li><code>y</code>: is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>&lt;:Count</code>; check the scitype with <code>scitype(y)</code></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: returns a vector of Poisson distributions given features <code>Xnew</code> having the same scitype as <code>X</code> above. Predictions are probabilistic.</li></ul><p>Specific metrics can also be predicted using:</p><ul><li><code>predict_mean(mach, Xnew)</code></li><li><code>predict_mode(mach, Xnew)</code></li><li><code>predict_median(mach, Xnew)</code></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>:fitresult</code>: The <code>GBTree</code> object returned by EvoTrees.jl fitting algorithm.</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>:features</code>: The names of the features encountered in training.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Internal API
using EvoTrees
config = EvoTreeCount(max_depth=5, nbins=32, nrounds=100)
nobs, nfeats = 1_000, 5
x_train, y_train = randn(nobs, nfeats), rand(0:2, nobs)
model = fit_evotree(config; x_train, y_train)
preds = EvoTrees.predict(model, x_train)</code></pre><pre><code class="language-julia hljs">using MLJ
EvoTreeCount = @load EvoTreeCount pkg=EvoTrees
model = EvoTreeCount(max_depth=5, nbins=32, nrounds=100)
nobs, nfeats = 1_000, 5
X, y = randn(nobs, nfeats), rand(0:2, nobs)
mach = machine(model, X, y) |&gt; fit!
preds = predict(mach, X)
preds = predict_mean(mach, X)
preds = predict_mode(mach, X)
preds = predict_median(mach, X)
</code></pre><p>See also <a href="https://github.com/Evovest/EvoTrees.jl">EvoTrees.jl</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Evovest/EvoTrees.jl/blob/63b0a6574da444fcc0e51443809a2a88869bb573/src/MLJ.jl#L425-L554">source</a></section></details></article><h2 id="EvoTreeMLE"><a class="docs-heading-anchor" href="#EvoTreeMLE">EvoTreeMLE</a><a id="EvoTreeMLE-1"></a><a class="docs-heading-anchor-permalink" href="#EvoTreeMLE" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="EvoTrees.EvoTreeMLE"><a class="docstring-binding" href="#EvoTrees.EvoTreeMLE"><code>EvoTrees.EvoTreeMLE</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">EvoTreeMLE(;kwargs...)</code></pre><p>A model type for constructing a EvoTreeMLE, based on <a href="https://github.com/Evovest/EvoTrees.jl">EvoTrees.jl</a>, and implementing both an internal API the MLJ model interface. EvoTreeMLE performs maximum likelihood estimation. Assumed distribution is specified through <code>loss</code> kwargs. Both Gaussian and Logistic distributions are supported.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>early_stopping_rounds::Integer</code>: number of consecutive rounds without metric improvement after which fitting in stopped. </li><li><code>loss=:gaussian</code>:       Loss to be be minimized during training. One of:<ul><li><code>:gaussian_mle</code></li><li><code>:logistic_mle</code></li></ul></li><li><code>nrounds=100</code>:          Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be &gt;= 1.</li><li><code>eta=0.1</code>:              Learning rate. Each tree raw predictions are scaled by <code>eta</code> prior to be added to the stack of predictions. Must be &gt; 0. A lower <code>eta</code> results in slower learning, requiring a higher <code>nrounds</code> but typically improves model performance.  </li><li><code>L2=1.0</code>:               L2 regularization factor on aggregate gain. Must be &gt;= 0. Higher L2 can result in a more robust model.</li><li><code>lambda=0.0</code>:           L2 regularization factor on individual gain. Must be &gt;= 0. Higher lambda can result in a more robust model.</li><li><code>gamma=0.0</code>:            Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model. Must be &gt;= 0.</li><li><code>max_depth=6</code>:          Maximum depth of a tree. Must be &gt;= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains <code>2^(N - 1)</code> terminal leaves and <code>2^(N - 1) - 1</code> split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.</li><li><code>min_weight=8.0</code>:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the <code>weights</code> vector. Must be &gt; 0.</li><li><code>rowsample=1.0</code>:        Proportion of rows that are sampled at each iteration to build the tree. Should be in <code>]0, 1]</code>.</li><li><code>colsample=1.0</code>:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be in <code>]0, 1]</code>.</li><li><code>nbins=64</code>:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.</li><li><code>monotone_constraints=Dict{Int, Int}()</code>: Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  !Experimental feature: note that for MLE regression, constraints may not be enforced systematically.</li><li><code>tree_type=:binary</code>     Tree structure to be used. One of:<ul><li><code>:binary</code>:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see <code>gamma</code>) stops further node splits.  </li><li><code>:oblivious</code>:    A common splitting condition is imposed to all nodes of a given depth. </li></ul></li><li><code>seed=123</code>:             An integer used as a seed to the random number generator.</li><li><code>device=:cpu</code>:          Hardware device to use for computations. Can be either <code>:cpu</code> or <code>gpu</code>. Following losses are not GPU supported at the moment: <code>:logistic_mle</code>.</li></ul><p><strong>Internal API</strong></p><p>Do <code>config = EvoTreeMLE()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeMLE(max_depth=...).</p><p><strong>Training model</strong></p><p>A model is built using <a href="../api/#fit_evotree"><code>fit_evotree</code></a>:</p><pre><code class="language-julia hljs">model = fit_evotree(config; x_train, y_train, kwargs...)</code></pre><p><strong>Inference</strong></p><p>Predictions are obtained using <a href="../api/#predict"><code>predict</code></a> which returns a <code>Matrix</code> of size <code>[nobs, nparams]</code> where the second dimensions refer to <code>μ</code> &amp; <code>σ</code> for Normal/Gaussian and <code>μ</code> &amp; <code>s</code> for Logistic.</p><pre><code class="language-julia hljs">EvoTrees.predict(model, X)</code></pre><p>Alternatively, models act as a functor, returning predictions when called as a function with features as argument:</p><pre><code class="language-julia hljs">model(X)</code></pre><p><strong>MLJ</strong></p><p>From MLJ, the type can be imported using:</p><pre><code class="language-julia hljs">EvoTreeMLE = @load EvoTreeMLE pkg=EvoTrees</code></pre><p>Do <code>model = EvoTreeMLE()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>EvoTreeMLE(loss=...)</code>.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-julia hljs">mach = machine(model, X, y)</code></pre><p>where</p><ul><li><p><code>X</code>: any table of input features (eg, a <code>DataFrame</code>) whose columns each have one of the following element scitypes: <code>Continuous</code>, <code>Count</code>, or <code>&lt;:OrderedFactor</code>; check column scitypes with <code>schema(X)</code></p></li><li><p><code>y</code>: is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>&lt;:Continuous</code>; check the scitype with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: returns a vector of Gaussian or Logistic distributions (according to provided <code>loss</code>) given features <code>Xnew</code> having the same scitype as <code>X</code> above.</li></ul><p>Predictions are probabilistic.</p><p>Specific metrics can also be predicted using:</p><ul><li><code>predict_mean(mach, Xnew)</code></li><li><code>predict_mode(mach, Xnew)</code></li><li><code>predict_median(mach, Xnew)</code></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>:fitresult</code>: The <code>GBTree</code> object returned by EvoTrees.jl fitting algorithm.</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>:features</code>: The names of the features encountered in training.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Internal API
using EvoTrees
config = EvoTreeMLE(max_depth=5, nbins=32, nrounds=100)
nobs, nfeats = 1_000, 5
x_train, y_train = randn(nobs, nfeats), rand(nobs)
model = fit_evotree(config; x_train, y_train)
preds = EvoTrees.predict(model, x_train)</code></pre><pre><code class="language-julia hljs"># MLJ Interface
using MLJ
EvoTreeMLE = @load EvoTreeMLE pkg=EvoTrees
model = EvoTreeMLE(max_depth=5, nbins=32, nrounds=100)
X, y = @load_boston
mach = machine(model, X, y) |&gt; fit!
preds = predict(mach, X)
preds = predict_mean(mach, X)
preds = predict_mode(mach, X)
preds = predict_median(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Evovest/EvoTrees.jl/blob/63b0a6574da444fcc0e51443809a2a88869bb573/src/MLJ.jl#L694-L829">source</a></section></details></article><h2 id="EvoTreeGaussian"><a class="docs-heading-anchor" href="#EvoTreeGaussian">EvoTreeGaussian</a><a id="EvoTreeGaussian-1"></a><a class="docs-heading-anchor-permalink" href="#EvoTreeGaussian" title="Permalink"></a></h2><p><code>EvoTreeGaussian</code> is to be deprecated. Use EvoTreeMLE with <code>loss = :gaussian_mle</code>. </p><article><details class="docstring" open="true"><summary id="EvoTrees.EvoTreeGaussian"><a class="docstring-binding" href="#EvoTrees.EvoTreeGaussian"><code>EvoTrees.EvoTreeGaussian</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">EvoTreeGaussian(;kwargs...)</code></pre><p>A model type for constructing a EvoTreeGaussian, based on <a href="https://github.com/Evovest/EvoTrees.jl">EvoTrees.jl</a>, and implementing both an internal API the MLJ model interface. EvoTreeGaussian is used to perform Gaussian probabilistic regression, fitting μ and σ parameters to maximize likelihood.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>early_stopping_rounds::Integer</code>: number of consecutive rounds without metric improvement after which fitting in stopped. </li><li><code>nrounds=100</code>:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked. Must be &gt;= 1.</li><li><code>eta=0.1</code>:              Learning rate. Each tree raw predictions are scaled by <code>eta</code> prior to be added to the stack of predictions. Must be &gt; 0. A lower <code>eta</code> results in slower learning, requiring a higher <code>nrounds</code> but typically improves model performance.  </li><li><code>L2=1.0</code>:               L2 regularization factor on aggregate gain. Must be &gt;= 0. Higher L2 can result in a more robust model.</li><li><code>lambda=0.0</code>:           L2 regularization factor on individual gain. Must be &gt;= 0. Higher lambda can result in a more robust model.</li><li><code>gamma=0.0</code>:            Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model. Must be &gt;= 0.</li><li><code>max_depth=6</code>:          Maximum depth of a tree. Must be &gt;= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains <code>2^(N - 1)</code> terminal leaves and <code>2^(N - 1) - 1</code> split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.</li><li><code>min_weight=8.0</code>:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the <code>weights</code> vector. Must be &gt; 0.</li><li><code>rowsample=1.0</code>:        Proportion of rows that are sampled at each iteration to build the tree. Should be in <code>]0, 1]</code>.</li><li><code>colsample=1.0</code>:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be in <code>]0, 1]</code>.</li><li><code>nbins=64</code>:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins. Should be between 2 and 255.</li><li><code>monotone_constraints=Dict{Int, Int}()</code>: Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  !Experimental feature: note that for Gaussian regression, constraints may not be enforce systematically.</li><li><code>tree_type=:binary</code>     Tree structure to be used. One of:<ul><li><code>:binary</code>:       Each node of a tree is grown independently. Tree are built depthwise until max depth is reach or if min weight or gain (see <code>gamma</code>) stops further node splits.  </li><li><code>:oblivious</code>:    A common splitting condition is imposed to all nodes of a given depth. </li></ul></li><li><code>seed=123</code>:             An integer used as a seed to the random number generator.</li><li><code>device=:cpu</code>:          Hardware device to use for computations. Can be either <code>:cpu</code> or <code>gpu</code>.</li></ul><p><strong>Internal API</strong></p><p>Do <code>config = EvoTreeGaussian()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeGaussian(max_depth=...).</p><p><strong>Training model</strong></p><p>A model is built using <a href="../api/#fit_evotree"><code>fit_evotree</code></a>:</p><pre><code class="language-julia hljs">model = fit_evotree(config; x_train, y_train, kwargs...)</code></pre><p><strong>Inference</strong></p><p>Predictions are obtained using <a href="../api/#predict"><code>predict</code></a> which returns a <code>Matrix</code> of size <code>[nobs, 2]</code> where the second dimensions refer to <code>μ</code> and <code>σ</code> respectively:</p><pre><code class="language-julia hljs">EvoTrees.predict(model, X)</code></pre><p>Alternatively, models act as a functor, returning predictions when called as a function with features as argument:</p><pre><code class="language-julia hljs">model(X)</code></pre><p><strong>MLJ</strong></p><p>From MLJ, the type can be imported using:</p><pre><code class="language-julia hljs">EvoTreeGaussian = @load EvoTreeGaussian pkg=EvoTrees</code></pre><p>Do <code>model = EvoTreeGaussian()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>EvoTreeGaussian(loss=...)</code>.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-julia hljs">mach = machine(model, X, y)</code></pre><p>where</p><ul><li><p><code>X</code>: any table of input features (eg, a <code>DataFrame</code>) whose columns each have one of the following element scitypes: <code>Continuous</code>, <code>Count</code>, or <code>&lt;:OrderedFactor</code>; check column scitypes with <code>schema(X)</code></p></li><li><p><code>y</code>: is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>&lt;:Continuous</code>; check the scitype with <code>scitype(y)</code></p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: returns a vector of Gaussian distributions given features <code>Xnew</code> having the same scitype as <code>X</code> above.</li></ul><p>Predictions are probabilistic.</p><p>Specific metrics can also be predicted using:</p><ul><li><code>predict_mean(mach, Xnew)</code></li><li><code>predict_mode(mach, Xnew)</code></li><li><code>predict_median(mach, Xnew)</code></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>:fitresult</code>: The <code>GBTree</code> object returned by EvoTrees.jl fitting algorithm.</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>:features</code>: The names of the features encountered in training.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Internal API
using EvoTrees
params = EvoTreeGaussian(max_depth=5, nbins=32, nrounds=100)
nobs, nfeats = 1_000, 5
x_train, y_train = randn(nobs, nfeats), rand(nobs)
model = fit_evotree(params; x_train, y_train)
preds = EvoTrees.predict(model, x_train)</code></pre><pre><code class="language-julia hljs"># MLJ Interface
using MLJ
EvoTreeGaussian = @load EvoTreeGaussian pkg=EvoTrees
model = EvoTreeGaussian(max_depth=5, nbins=32, nrounds=100)
X, y = @load_boston
mach = machine(model, X, y) |&gt; fit!
preds = predict(mach, X)
preds = predict_mean(mach, X)
preds = predict_mode(mach, X)
preds = predict_median(mach, X)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Evovest/EvoTrees.jl/blob/63b0a6574da444fcc0e51443809a2a88869bb573/src/MLJ.jl#L557-L689">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Introduction</a><a class="docs-footer-nextpage" href="../api/">Public »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.17.0 on <span class="colophon-date" title="Tuesday 24 February 2026 06:13">Tuesday 24 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
