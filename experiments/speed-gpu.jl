using Statistics
using StatsBase:sample, sample!
using EvoTrees
using BenchmarkTools
using CUDA


# allocations on transfer
function gpu_to_cpu(dst, src)
    copy!(dst, src)
end
function cpu_to_gpu(dst, src)
    copy!(dst, src)
end

x1c = zeros(1000, 1000)
x1g = CUDA.rand(1000, 1000)
@time gpu_to_cpu(x1c, x1g);
CUDA.@time gpu_to_cpu(x1c, x1g);

x1c = rand(1000, 1000)
x1g = CUDA.zeros(1000, 1000)
@time cpu_to_gpu(x1g, x1c);
CUDA.@time cpu_to_gpu(x1g, x1c);

function reshape_gpu(x)
    reshape(x, 4, size(x, 1) Ã· 4, size(x,2))
end
x2 = CUDA.@time reshape_gpu(x1g);
reshape(x1g, 4, 250, 1000)

# prepare a dataset
features = rand(Int(1.25e6), 100)
# features = rand(100, 10)
X = features
Y = rand(size(X, 1))
ğ‘– = collect(1:size(X, 1))

# train-eval split
ğ‘–_sample = sample(ğ‘–, size(ğ‘–, 1), replace=false)
train_size = 0.8
ğ‘–_train = ğ‘–_sample[1:floor(Int, train_size * size(ğ‘–, 1))]
ğ‘–_eval = ğ‘–_sample[floor(Int, train_size * size(ğ‘–, 1)) + 1:end]

X_train, X_eval = X[ğ‘–_train, :], X[ğ‘–_eval, :]
Y_train, Y_eval = Y[ğ‘–_train], Y[ğ‘–_eval]

###################################################
# GPU
###################################################
params_g = EvoTreeRegressor(T=Float32,
    loss=:linear, metric=:none,
    nrounds=100,
    Î»=1.0, Î³=0.1, Î·=0.1,
    max_depth=2, min_weight=1.0,
    rowsample=0.5, colsample=0.5, nbins=64);

model_g, cache_g = EvoTrees.init_evotree_gpu(params_g, X_train, Y_train);

params_g = model_g.params;
X_size = size(cache_g.X_bin);

# select random rows and cols
ğ‘–c = cache_g.ğ‘–_[sample(params_g.rng, cache_g.ğ‘–_, ceil(Int, params_g.rowsample * X_size[1]), replace=false, ordered=true)]
ğ‘– = CuVector(ğ‘–c)
ğ‘—c = cache_g.ğ‘—_[sample(params_g.rng, cache_g.ğ‘—_, ceil(Int, params_g.colsample * X_size[2]), replace=false, ordered=true)]
ğ‘— = CuVector(ğ‘—c)

cache_g.nodes[1].ğ‘– = ğ‘–
cache_g.ğ‘— .= ğ‘—
# build a new tree
# 144.600 Î¼s (23 allocations: 896 bytes) - 5-6 X time faster on GPU
@time CUDA.@sync EvoTrees.update_grads_gpu!(params_g.loss, cache_g.Î´ğ‘¤, cache_g.pred_gpu, cache_g.Y_gpu)
# @btime CUDA.@sync EvoTrees.update_grads_gpu!($params_g.loss, $cache_g.Î´ğ‘¤, $cache_g.pred_gpu, $cache_g.Y_gpu)
# sum Gradients of each of the K parameters and bring to CPU

# 33.447 ms (6813 allocations: 307.27 KiB)
tree = EvoTrees.TreeGPU(params_g.max_depth, model_g.K, params_g.Î»)
CUDA.@time EvoTrees.grow_tree_gpu!(tree, cache_g.nodes, params_g, cache_g.Î´ğ‘¤, cache_g.edges, cache_g.ğ‘—, cache_g.out, cache_g.left, cache_g.right, cache_g.X_bin, cache_g.K)
CUDA.@time EvoTrees.grow_tree_gpu!(tree, cache_g.nodes, params_g, cache_g.Î´ğ‘¤, cache_g.edges, cache_g.ğ‘—, cache_g.out, cache_g.left, cache_g.right, cache_g.X_bin, cache_g.K)

# CUDA.@time EvoTrees.grow_tree_gpu!(tree, params_g, cache_g.Î´, cache_g.hist, cache_g.histL, cache_g.histR, cache_g.gains, cache_g.edges, ğ‘–, ğ‘—, ğ‘›, cache_g.X_bin);
# @btime EvoTrees.grow_tree_gpu!(EvoTrees.TreeGPU(UInt32($params_g.max_depth), $model_g.K, $params_g.Î»), $params_g, $cache_g.Î´, $cache_g.hist, $cache_g.histL, $cache_g.histR, $cache_g.gains, $cache_g.edges, $ğ‘–, $ğ‘—, $ğ‘›, $cache_g.X_bin);
# @code_warntype EvoTrees.grow_tree_gpu!(EvoTrees.TreeGPU(params_g.max_depth, model_g.K, params_g.Î»), params_g, cache_g.Î´, cache_g.hist, cache_g.histL, cache_g.histR, cache_g.gains, cache_g.edges, ğ‘–, ğ‘—, ğ‘›, cache_g.X_bin);

# push!(model_g.trees, tree);
# # 2.736 ms (93 allocations: 13.98 KiB)
# @time CUDA.@sync EvoTrees.predict_gpu!(cache_g.pred_gpu, tree, cache_g.X_bin)
# @btime CUDA.@sync EvoTrees.predict_gpu!($cache_g.pred_gpu, $tree, $cache_g.X_bin)

###########################
# Tree GPU
###########################
Î´ğ‘¤, K, edges, X_bin, nodes, out, left, right = cache_g.Î´ğ‘¤, cache_g.K, cache_g.edges, cache_g.X_bin, cache_g.nodes, cache_g.out, cache_g.left, cache_g.right;

# 9.613 ms (81 allocations: 13.55 KiB)
# ğ‘—2 = CuArray(sample(UInt32.(1:100), 50, replace=false, ordered=true))
# @time EvoTrees.update_hist_gpu!(params_g.loss, nodes[1].h, Î´ğ‘¤, X_bin, ğ‘–, ğ‘—, K)
# println(nodes[1].h)
CUDA.@time EvoTrees.update_hist_gpu!(params_g.loss, nodes[1].h, Î´ğ‘¤, X_bin, ğ‘–, ğ‘—, K)
CUDA.@time EvoTrees.update_hist_gpu!(params_g.loss, nodes[1].h, Î´ğ‘¤, X_bin, ğ‘–, ğ‘—, K)

# @btime EvoTrees.update_hist_gpu!($params_g.loss, $nodes[1].h, $Î´ğ‘¤, $X_bin, $ğ‘–, $ğ‘—, $K)
# @btime EvoTrees.update_hist_gpu!($nodes[1].h, $Î´ğ‘¤, $X_bin, $nodes[1].ğ‘–, $ğ‘—)
# @code_warntype EvoTrees.update_hist_gpu!(hist, Î´, X_bin, ğ‘–, ğ‘—, ğ‘›)

# depth=1
# nid = 2^(depth - 1):2^(depth) - 1
# # 97.000 Î¼s (159 allocations: 13.09 KiB)
# @time CUDA.@sync EvoTrees.update_gains_gpu!(gains::AbstractArray{T,3}, hist::AbstractArray{T,4}, histL::AbstractArray{T,4}, histR::AbstractArray{T,4}, ğ‘—::AbstractVector{S}, params_g, nid, depth);
# @btime CUDA.@sync EvoTrees.update_gains_gpu!(gains::AbstractArray{T,3}, hist::AbstractArray{T,4}, histL::AbstractArray{T,4}, histR::AbstractArray{T,4}, ğ‘—::AbstractVector{S}, params_g, nid, depth);
# gains[:,:,1]

# tree = EvoTrees.TreeGPU(UInt32(params_g.max_depth), model_g.K, params_g.Î»)
# n = 1
# best = findmax(view(gains, :,:,n))
# if best[2][1] != params_g.nbins && best[1] > -Inf
#     tree.gain[n] = best[1]
#     tree.feat[n] = best[2][2]
#     tree.cond_bin[n] = best[2][1]
#     tree.cond_float[n] = edges[tree.feat[n]][tree.cond_bin[n]]
# end
# tree.split[n] = tree.cond_bin[n] != 0

# # 673.900 Î¼s (600 allocations: 29.39 KiB)
# @time CUDA.@sync EvoTrees.update_set_gpu!(ğ‘›, ğ‘–, X_bin, tree.feat, tree.cond_bin, params_g.nbins)
# @btime CUDA.@sync EvoTrees.update_set_gpu!($ğ‘›, $ğ‘–, $X_bin, $tree.feat, $tree.cond_bin, $params_g.nbins)
