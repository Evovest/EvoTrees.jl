# using CUDA
using CUDA
# using Flux
# using GeometricFlux

nbins = 32
ncol = 100
items = Int(1e6)
hist = zeros(Float32, nbins, ncol)
Î´ = rand(Float32, items)
idx = rand(1:nbins, items, ncol)
ğ‘– = collect(1:items)
ğ‘— = collect(1:ncol)

hist_gpu = CuArray(hist)
Î´_gpu = CuArray(Î´)
idx_gpu = CuArray(idx)
ğ‘–_gpu = CuArray(ğ‘–)
ğ‘—_gpu = CuArray(ğ‘—)

# CPU
function hist_cpu!(hist, Î´, idx, ğ‘–, ğ‘—)
    Threads.@threads for j in ğ‘—
        @inbounds for i in ğ‘–
            hist[idx[i], j] += Î´[i]
        end
    end
    return
end

function kernel_1!(h::CuDeviceMatrix{T}, x::CuDeviceVector{T}, id, ğ‘–, ğ‘—) where {T<:AbstractFloat}
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    j = threadIdx().y + (blockIdx().y - 1) * blockDim().y
    if i <= length(ğ‘–) && j <= length(ğ‘—)
        @inbounds k = Base._to_linear_index(h, id[ğ‘–[i], ğ‘—[j]], ğ‘—[j])
        @inbounds CUDA.atomic_add!(pointer(h, k), x[ğ‘–[i]])
    end
    return
end

# base approach - block built along the cols first, the rows (limit collisions)
function hist_gpu_1!(h::CuMatrix{T}, x::CuVector{T}, id::CuMatrix{Int}, ğ‘–, ğ‘—; MAX_THREADS=1024) where {T<:AbstractFloat}
    thread_j = min(MAX_THREADS, length(ğ‘—))
    thread_i = min(MAX_THREADS Ã· thread_j, length(ğ‘–))
    threads = (thread_i, thread_j)
    blocks = ceil.(Int, (length(ğ‘–), length(ğ‘—)) ./ threads)
    @cuda blocks=blocks threads=threads kernel_1!(h, x, id, ğ‘–, ğ‘—)
    return
end

@time hist_cpu!(hist, Î´, idx)
CUDA.@time hist_gpu_1!(hist_gpu, Î´_gpu, idx_gpu, ğ‘–_gpu, ğ‘—_gpu, MAX_THREADS=1024)




nbins = 32
ncol = 100
items = Int(2e6)
K = 1
hist = zeros(Float32, nbins, 3, ncol)
Î´ = rand(Float32, items, 3)
idx = rand(1:nbins, items, ncol)
ğ‘– = collect(1:items)
ğ‘— = collect(1:ncol)

hist_gpu = CuArray(hist)
Î´_gpu = CuArray(Î´)
idx_gpu = CuArray(idx)
ğ‘–_gpu = CuArray(ğ‘–)
ğ‘—_gpu = CuArray(ğ‘—)

function kernel_2!(h::CuDeviceArray{T,3}, x::CuDeviceMatrix{T}, id, ğ‘–, ğ‘—) where {T<:AbstractFloat}
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    j = threadIdx().y + (blockIdx().y - 1) * blockDim().y
    if i <= length(ğ‘–) && j <= length(ğ‘—)
        @inbounds k1 = Base._to_linear_index(h, id[ğ‘–[i], ğ‘—[j]], 1, ğ‘—[j])
        @inbounds CUDA.atomic_add!(pointer(h, k1), x[ğ‘–[i],1])
        @inbounds k2 = Base._to_linear_index(h, id[ğ‘–[i], ğ‘—[j]], 2, ğ‘—[j])
        @inbounds CUDA.atomic_add!(pointer(h, k2), x[ğ‘–[i],2])
        @inbounds k3 = Base._to_linear_index(h, id[ğ‘–[i], ğ‘—[j]], 3, ğ‘—[j])
        @inbounds CUDA.atomic_add!(pointer(h, k3), x[ğ‘–[i],3])
    end
    return
end

# base approach - block built along the cols first, the rows (limit collisions)
function hist_gpu_2!(h::CuArray{T,3}, x::CuMatrix{T}, id::CuMatrix{Int}, ğ‘–, ğ‘—; MAX_THREADS=1024) where {T<:AbstractFloat}
    thread_j = min(MAX_THREADS, length(ğ‘—))
    thread_i = min(MAX_THREADS Ã· thread_j, length(ğ‘–))
    threads = (thread_i, thread_j)
    blocks = ceil.(Int, (length(ğ‘–), length(ğ‘—)) ./ threads)
    @cuda blocks=blocks threads=threads kernel_2!(h, x, id, ğ‘–, ğ‘—)
    return
end

CUDA.@time hist_gpu_2!(hist_gpu, Î´_gpu, idx_gpu, ğ‘–_gpu, ğ‘—_gpu, MAX_THREADS=1024)

hist_gpu_1 = Array(hist_gpu)
hist_gpu_2 = Array(hist_gpu)
diff1 = hist_gpu_2 - hist_gpu_1

######################################################################################################
# best approach: loop on K indicators
######################################################################################################
function kernel_3!(h::CuDeviceArray{T,3}, x::CuDeviceMatrix{T}, id, ğ‘–, ğ‘—, K) where {T<:AbstractFloat}
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    j = threadIdx().y + (blockIdx().y - 1) * blockDim().y
    if i <= length(ğ‘–) && j <= length(ğ‘—)
        for k in 1:K
            @inbounds pt = Base._to_linear_index(h, id[ğ‘–[i], ğ‘—[j]], k, ğ‘—[j])
            @inbounds CUDA.atomic_add!(pointer(h, pt), x[ğ‘–[i],k])
        end
    end
    return
end

# base approach - block built along the cols first, the rows (limit collisions)
function hist_gpu_3!(h::CuArray{T,3}, x::CuMatrix{T}, id::CuMatrix{Int}, ğ‘–, ğ‘—, K; MAX_THREADS=1024) where {T<:AbstractFloat}
    thread_j = min(MAX_THREADS, length(ğ‘—))
    thread_i = min(MAX_THREADS Ã· thread_j, length(ğ‘–))
    threads = (thread_i, thread_j)
    blocks = ceil.(Int, (length(ğ‘–), length(ğ‘—)) ./ threads)
    @cuda blocks=blocks threads=threads kernel_3!(h, x, id, ğ‘–, ğ‘—, K)
    return
end

hist_gpu_1 = Array(hist_gpu)
hist_gpu_2 = Array(hist_gpu)
diff2 = hist_gpu_2 - hist_gpu_1
diff2 - diff1

CUDA.@time hist_gpu_3!(hist_gpu, Î´_gpu, idx_gpu, ğ‘–_gpu, ğ‘—_gpu, 3, MAX_THREADS=1024)



######################################################################################################
# 3D kernel - instead of iterating on K - Less efficient than the loop on Ks
######################################################################################################
function kernel_3D!(h::CuDeviceArray{T,3}, x::CuDeviceMatrix{T}, id, ğ‘–, ğ‘—, K) where {T<:AbstractFloat}
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    j = threadIdx().y + (blockIdx().y - 1) * blockDim().y
    k = threadIdx().z + (blockIdx().z - 1) * blockDim().z
    if i <= length(ğ‘–) && j <= length(ğ‘—)
        @inbounds pt = Base._to_linear_index(h, id[ğ‘–[i], ğ‘—[j]], k, ğ‘—[j])
        @inbounds CUDA.atomic_add!(pointer(h, pt), x[ğ‘–[i],k])
    end
    return
end

# base approach - block built along the cols first, the rows (limit collisions)
function hist_gpu_3D!(h::CuArray{T,3}, x::CuMatrix{T}, id::CuMatrix{Int}, ğ‘–, ğ‘—, K; MAX_THREADS=1024) where {T<:AbstractFloat}
    thread_k = min(MAX_THREADS, K)
    thread_j = min(MAX_THREADS Ã· thread_k, length(ğ‘—))
    thread_i = min(MAX_THREADS Ã· (thread_k * thread_j), length(ğ‘–))
    threads = (thread_i, thread_j, thread_k)
    blocks = ceil.(Int, (length(ğ‘–), length(ğ‘—), K) ./ threads)
    @cuda blocks=blocks threads=threads kernel_3D!(h, x, id, ğ‘–, ğ‘—, K)
    return
end

CUDA.@time hist_gpu_3D!(hist_gpu, Î´_gpu, idx_gpu, ğ‘–_gpu, ğ‘—_gpu, 3, MAX_THREADS=1024)

hist_gpu_1 = Array(hist_gpu)
hist_gpu_2 = Array(hist_gpu)
diff1 = hist_gpu_2 - hist_gpu_1


######################################################################################################
# 3D kernel - instead of iterating on K - No collision approach - single i thread - bad!
######################################################################################################
function kernel_3D2!(h::CuDeviceArray{T,3}, x::CuDeviceMatrix{T}, id, ğ‘–, ğ‘—, K) where {T<:AbstractFloat}
    i = threadIdx().x + (blockIdx().x - 1) * blockDim().x
    j = threadIdx().y + (blockIdx().y - 1) * blockDim().y
    k = threadIdx().z + (blockIdx().z - 1) * blockDim().z
    if i <= length(ğ‘–) && j <= length(ğ‘—)
        # @inbounds pt = Base._to_linear_index(h, id[ğ‘–[i], ğ‘—[j]], k, ğ‘—[j])
        @inbounds h[id[ğ‘–[i], ğ‘—[j]], k, ğ‘—[j]] += x[ğ‘–[i],k]
    end
    return
end

# base approach - block built along the cols first, the rows (limit collisions)
function hist_gpu_3D2!(h::CuArray{T,3}, x::CuMatrix{T}, id::CuMatrix{Int}, ğ‘–, ğ‘—, K; MAX_THREADS=1024) where {T<:AbstractFloat}
    thread_k = min(MAX_THREADS, K)
    thread_j = min(MAX_THREADS Ã· thread_k, length(ğ‘—))
    thread_i = 1
    threads = (thread_i, thread_j, thread_k)
    blocks = ceil.(Int, (length(ğ‘–), length(ğ‘—), K) ./ threads)
    @cuda blocks=blocks threads=threads kernel_3D2!(h, x, id, ğ‘–, ğ‘—, K)
    return
end

CUDA.@time hist_gpu_3D2!(hist_gpu, Î´_gpu, idx_gpu, ğ‘–_gpu, ğ‘—_gpu, 3, MAX_THREADS=1024)

hist_gpu_1 = Array(hist_gpu)
hist_gpu_2 = Array(hist_gpu)
diff1 = hist_gpu_2 - hist_gpu_1
